# Hydra-Lightning Document

This documentation provides a basic overview of getting up to speed with incorporating hydra for the purposes of tracking various deep learning experiments with pytorch lightning. The documentation is adapted from a lightning-hydra template; Please refer to the git repository [lightning-hydra](https://github.com/ashleve/lightning-hydra-template) for a more comprehensive guide to the whole process of transforming your pytorch setup.

For this documentation, the base assumption would be that there is an existing model written in pytorch. We list down certain steps that would occur during the process of converting to hydra-lightning:

1. Installation of hydra and lightning: \
    `pip install hydra-core lightning`

2. The reorganisation of project folder:
    Adapted from [lightning-hydra](https://github.com/ashleve/lightning-hydra-template), this is an example of how a folder directory should look like. The main difference from before rewriting the project using the hydra-lightning template would be the inclusion of the `configs` folder, which serves to store the various hydra configuration files that would be used during trainings or experiments.
    ```
    ├── configs                   <- Hydra configs
    │   ├── callbacks                <- Callbacks configs
    │   ├── data                     <- Data configs
    │   ├── debug                    <- Debugging configs
    │   ├── experiment               <- Experiment configs
    │   ├── extras                   <- Extra utilities configs
    │   ├── hparams_search           <- Hyperparameter search configs
    │   ├── hydra                    <- Hydra configs
    │   ├── local                    <- Local configs
    │   ├── logger                   <- Logger configs
    │   ├── model                    <- Model configs
    │   ├── paths                    <- Project paths configs
    │   ├── trainer                  <- Trainer configs
    │   │
    │   ├── eval.yaml             <- Main config for evaluation
    │   └── train.yaml            <- Main config for training
    │
    ├── data                   <- Project data
    |
    |── model                  <- Model folder for pretrained models
    │
    ├── logs                   <- Logs generated by hydra and lightning loggers
    │
    ├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                             the creator's initials, and a short `-` delimited description,
    │                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
    │
    ├── lightning_classes (src)      <- Source code
    │   ├── data                     <- Data scripts, LightningDataModules
    │   ├── models                   <- Model scripts, LightningModules
    │   ├── utils                    <- Utility scripts
    │   │
    │   ├── eval.py                  <- Run evaluation
    │   └── train.py                 <- Run training
    │
    ├── requirements.txt          <- File for installing python dependencies
    └── README.md

3. Rewritting pytorch code into lightning code: 
This is the tedious portion of the process. The two main components that needs to be re-expressed in lightning would be data loading and training/evalutation steps. The components would have to be broken down/decomposed and placed into the various methods required by the `LightningDataModule` and `LightningModule` classes respectively. For example, this code snippet in pytorch:
    ```python
    for epoch in range(num_train_epochs):
        # Training
        model.train()
        for batch in train_dataloader:
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)
    ```
    would be rewritten as a collection of methods within a `LightningModule` class:
    ```python
    from lightning import LightningModule

    class SomeNewClass(LightningModule):
        def __init__(self):
            .
            .
            .
    
        def forward(self, batch):
            return self.lm(**batch)
        
        def model_step(self, batch: Any):
            output = self.forward(batch)
            loss = output.loss
            return loss

        def training_step(self, batch: Any, batch_idx: int):
            self.lm.train()
            loss = self.model_step(batch)
            self.train_loss(loss) # tracking of the training loss using train_loss
            return loss
    ```
    The rewriting process would be have to be performed for data loading and training/evaluation steps. For further examples of how certain snippets are written, please look at the `lightning_classes` folder where the data and model codes are stored, or refer to [lightning-hydra](https://github.com/ashleve/lightning-hydra-template) for more details.
4. Setting up the hydra configuration files. (Coming Soon)
5. Writing configuration files for new experiments. (Coming Soon)