{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb458481-b243-450e-a56b-faf9a32e2749",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "In this notebook, we aim to resolve the data quality issue through various methods of augmenting our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5d3db-2324-4b50-b8b2-653cf344db88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A) Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61863244-e9e0-46e3-aeb0-71ebb62a3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97231c13-250c-4070-b4fb-2712a55e9570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12f5c09-43f2-468a-9d91-50cca94c1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fasttext model, only run once\n",
    "#from nlpaug.util.file.download import DownloadUtil\n",
    "#DownloadUtil.download_fasttext(model_name = 'wiki-news-300d-1M', dest_dir = 'Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2105ff68-5db0-4bed-8329-e9bf06f140c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6869ff8-a264-4642-b51e-d1b2a316f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35976d66-fadd-4064-a52a-30099ee75af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train-aws/SSOC_2020.csv')\n",
    "data = pd.read_csv('Data/Processed/Training/train-aws/train_full.csv')\n",
    "extra_info = pd.read_csv('Data/Processed/MCF_Training_Set_Full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e013656f-5a10-412c-a374-646c611bddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ssoc_autocoder/sentaugment/data/sentences.txt', 'w') as f:\n",
    "#     for item in SSOC_2020['Description'][295:296]:\n",
    "#         f.write(\"%s\\n\" % ''.join([i if ord(i) < 128 else ' ' for i in item]))\n",
    "#         f.write(\"%s\\n\" % ''.join([i if ord(i) < 128 else ' ' for i in item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a02bb7-0629-4205-9dc9-c764dffd9b59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B) Testing different types of augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c5cfd8-991b-478d-9068-033c81abd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = SSOC_2020['Description'][SSOC_2020['SSOC 2020'] == 25121].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1dd1d0d-90c9-4b66-9da0-6f32c8bd52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software developer researches, designs and develops computer and network software or specialised utility programs. He/she analyses user needs and develops software solutions, applying principles and techniques of computer science, engineering, and mathematical analysis. He/she also updates software, enhances existing software capabilities, and develops and directs software testing and validation procedures. He/she may work with computer hardware engineers to integrate hardware and software systems, and develop specifications and performance requirements. . researching, analysing and evaluating requirements for software, web and multimedia applications. designing, and developing computer software, web and multimedia systems. designing and developing digital animations, imaging, presentations, games, audio and video clips, and Internet applications using multimedia software, tools and utilities, interactive graphics and programming languages. consulting with engineering staff to evaluate interface between hardware and software. developing and directing software testing and validation procedures. modifying existing software to correct errors, to adapt it to new hardware or to upgrade interfaces and improve performance. directing software programming and development of documentation. assessing, developing, upgrading and documenting maintenance procedures for software, web and multimedia applications. assisting in analysing, specifying and developing Internet strategies, web.based methodologies and development plans. consulting with customers concerning maintenance of software, web and multimedia systems\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b592db7-5867-42fe-9584-97bd33055768",
   "metadata": {},
   "source": [
    "#### 1. Using pretrained word embeddings (`fasttext`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b1283d0-3b97-4841-aa94-538080a89337",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_aug = naw.WordEmbsAug(model_type = 'fasttext', \n",
    "                               model_path = model_dir + 'wiki-news-300d-1M.vec',\n",
    "                               action = \"substitute\",\n",
    "                               top_k = 5,\n",
    "                               aug_p = 0.5,\n",
    "                               aug_min = 10,\n",
    "                               aug_max = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37eadad-aa61-42a7-b1cf-b1aa7cfe9e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software builder researches, prototypes and builds computer which networks software or specialized efficiency programs. .He / her Analysis interface wants in builds software issues, applies tenets, technologies of computing sceince, engineering, and algebraic anaylsis. He / she still updates softare, strengthens existing hardware capabilities, , develops which directs software tests and validation procedures. They / she will research along computing software engineering to combine firmware in software components, but cultivate specification but performance standards. . researching, summarising which examining requirments for software, websites and audio applications. constructing, but establishing computer software, web in multi-media system. designing in creating digital visuals, imaging, lectures, game, recordings and video- excerpts, in Internet applications using interactive sofware, toolkit and utilities, multimedia graphic and programs langauages. consulting with engineering faculty to analyze wildland-urban betweeen software with software. producing and instructing software testing and validating procedures. revising pre-existing hardware and corrrect inaccuracies, from adapt just to current equipment whether and upgrade interfaces and reduce perfomance. directing hardware programs and implementation 's documentation. examining, developing, upgraded with detailing upkeep practices in computer, web which multi-media applications. assisted in analysing, specify and creating Internet approaches, websites. basing methodologies but growth plans. consultants with customers concerning maintenance for software, web and multimedia mechanisms\n"
     ]
    }
   ],
   "source": [
    "fasttext_augmented_text = fasttext_aug.augment(text, num_thread = 4)\n",
    "print(fasttext_augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bbe65-a6e3-4308-8278-aae853ccda50",
   "metadata": {},
   "source": [
    "#### 2. Using back translation\n",
    "Back translation means translating the whole text to another language and back to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01088545-3e4c-4b03-b521-2e827db0a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(from_model_name='facebook/wmt19-en-de', \n",
    "                                              to_model_name='facebook/wmt19-de-en',\n",
    "                                              device = 'cuda',\n",
    "                                              max_length = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ee4f2-a81b-4471-9205-8e77eafb5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtransl_augmented_text = back_translation_aug.augment(text, num_thread = 4)\n",
    "print(backtransl_augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdb789-645c-49c5-a24a-3523db71e97e",
   "metadata": {},
   "source": [
    "#### 3. Using synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cde746-a31e-402f-9f22-d0c96c038236",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_aug = naw.SynonymAug(aug_src = 'ppdb', \n",
    "                             model_path = model_dir + 'ppdb-2.0-tldr',\n",
    "                             aug_p = 0.5,\n",
    "                             aug_min = 10,\n",
    "                             aug_max = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c36ad4-cd24-42ba-8a09-087f453b676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_augmented_text = synonym_aug.augment(text, num_thread = 4)\n",
    "print(synonym_augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35939835-17f6-4927-aaf6-3f536814f8c3",
   "metadata": {},
   "source": [
    "#### 4. Using contextual word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62efcf73-52ec-459b-9d2e-567ae0541cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_aug = naw.ContextualWordEmbsAug(model_path = 'distilbert-base-uncased', \n",
    "                                           action = \"substitute\",\n",
    "                                           top_k = 10,\n",
    "                                           aug_p = 0.7,\n",
    "                                           aug_min = 5,\n",
    "                                           aug_max = None,\n",
    "                                           device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e85f3c1-240b-4bc4-936d-098551e3aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website designer prepares, implements and develops network and database architecture or virtual web hardware. he / she directs user design and engineers internal hardware, designs hardware and tools of information hardware, automation, and industrial physics. he / she also builds computers, manufactures computing system components, and maintains and maintains performance analysis and assessment programs. he / she may work with application network developers to develop website and server products, and implements standards and compliance standards.. develops, analysing and verification procedures for network, web and web services. preparing, and supervising interactive networking, web and desktop applications. design and producing electronic audio, audio, graphic, multimedia, web and multimedia presentations, and graphical products. multimedia systems, graphics and components, presentation programs and interactive products. assists with application professionals to update components of hardware and application. develops and implements application repair and inspection techniques. prepares external hardware to identify weaknesses, to add it to new specifications or to update hardware and increase accessibility. performs internal upgrades and repair of hardware. preparing, managing, testing and implementing application procedure for application, web and web software. assistance in analysing, maintaining and implementing software software, web. performs engineering and maintenance procedures. consultation with personnel on evaluation of internet, web and embedded systems\n"
     ]
    }
   ],
   "source": [
    "distilbert_augmented_text = distilbert_aug.augment(text, num_thread = 4)\n",
    "print(distilbert_augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd06b0-1f5a-498c-9078-5d2eae4a2b58",
   "metadata": {},
   "source": [
    "#### 5. Using sentence augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e043ca-a196-4fe5-8877-81b09c5df567",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_aug = nas.ContextualWordEmbsForSentenceAug(model_path = 'distilgpt2',\n",
    "                                                    min_length = 100,\n",
    "                                                    max_length = 300,\n",
    "                                                    top_k = 50,\n",
    "                                                    top_p = .9,\n",
    "                                                    device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6b579-1e4c-4577-af53-3ea6ea94e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_augmented_text = sentence_aug.augment(text, num_thread = 4)\n",
    "print(sentence_augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bccb0-f6be-4565-b790-b53bc680415a",
   "metadata": {},
   "source": [
    "#### 6. Using summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff5459-bc6e-46f4-8b0e-1f978b273a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_aug = nas.AbstSummAug(model_path = 't5-base', \n",
    "                           min_length = 50,\n",
    "                           max_length = 100,\n",
    "                           top_k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69f5c-9fb0-4228-9863-45678c53031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_augmented_text = summ_aug.augment(text, num_thread = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d2662-dce8-4872-9191-2d55f1d817d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_augmented_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6804514-356f-4081-a136-775c3b01d1c6",
   "metadata": {},
   "source": [
    "#### 7. Adding spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8ffe5f0-0144-476e-80fb-4e806fce0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_aug = naw.SpellingAug(dict_path=None, \n",
    "                          name='Spelling_aug',\n",
    "                          aug_min=1, \n",
    "                          aug_max=10, \n",
    "                          aug_p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ac96ef1-1ff2-47cc-9d5a-ef1307b63a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_augmented_text = spl_aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9357b46-5b8a-4fdc-a156-9d957973d99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Software developer researches, designs and develops computer amd network software or specialised utility programs. He / she analisys user needs and develops software soliutions, applying princilpes and techniques of computer science, engineering, and mathematical analysis. He / whe also updates software, enhances existing softwares capabilities, and develops and directs software testing an validation procedures. He / she may work with computer hardware engineers to integrate hardware and software systems, and develop specifications and performance requirements. . researching, analysing and evaluating requirements for software, web and multimedia applications. designing, and developing computer sotfware, web and multimedia systems. designing and developing digital animations, imaging, presentations, games, audio and video clips, and Internet applications using multimedia software, tools and utilities, interactive graphics and programming languages. consulting with engineering staff to evaluate interface between hardware and software. developing and directing software testing and validation prosedures. modifying existing software to correct errors, to adapt it to new hardware or to upgrade interfaces and improve performance. directing software programming and development of documentation. assessing, developing, upgrading and documenting maintenance procedures for sowftware, web and multimedia applications. assisting in analysing, specifying and developing Internet strategies, web. based methodologies and development plans. consulting with customers concerning maintenance of software, web and multimedia systems'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_augmented_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2eae7-d6ae-49ae-823c-6e7ab82b8f2c",
   "metadata": {},
   "source": [
    "### C) Using GloVE embeddings to find and label more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ba96c0-c313-4345-89d1-0f3e22daa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "nlp = spacy.load('en_core_web_lg', disable = ['tagger', 'parser', 'ner', 'lemmatizer'])\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed20f1b-877b-49e5-8b08-3d7456f8068c",
   "metadata": {},
   "source": [
    "Add in additional preprocessing to remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7c0ccc-82c0-48fd-a713-c19b5ea59765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.additional_preprocessing(doc)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"additional_preprocessing\")\n",
    "def additional_preprocessing(doc):\n",
    "    lemma_list = [tok for tok in doc\n",
    "                  if tok.is_alpha and tok.text.lower() not in stopwords] \n",
    "    return lemma_list\n",
    "nlp.add_pipe('additional_preprocessing', last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43b7cf-a621-4782-b3bd-94e77b4c0126",
   "metadata": {},
   "source": [
    "Run the `nlp` processing pipeline over the two corpuses and convert the job postings into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fc08d5-97e6-4718-a309-f0f7576b3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSOC_2020_nlp = list(nlp.pipe(SSOC_2020['Description']))\n",
    "data_nlp = list(nlp.pipe(data['Cleaned_Description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e12ca34-948b-41b7-b5f6-d994489c0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job posting 42800/42842...\r"
     ]
    }
   ],
   "source": [
    "target_vecs = []\n",
    "for i, desc in enumerate(data_nlp):\n",
    "    if i % 100 == 0:\n",
    "        print(f'Job posting {i}/{len(data_nlp)}...\\r', end = '')\n",
    "    if len(desc) == 0:\n",
    "        target_vecs.append(np.array([0]*300))\n",
    "    else:\n",
    "        target_vecs.append(np.mean([token.vector for token in desc], axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe8d43-b1ab-4bf4-b6eb-49d6b1e19168",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "b421be98-6a9f-47d3-baa6-5c26f399a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "detailed_definitions_raw = pd.read_excel('Data/Raw/SSOC2020 Detailed Definitions.xlsx', skiprows = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ae8d3e5d-7aaa-41f0-b4fb-192a1574b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_definitions = detailed_definitions_raw[(~detailed_definitions_raw['SSOC 2020'].astype('str').str.contains('X')) & (detailed_definitions_raw['SSOC 2020'].astype('str').apply(len) >= 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "ed3da59e-67ae-4076-bed4-6bcc19691e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_40992/438202779.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  detailed_definitions['Jobs Cleaned'] = detailed_definitions['Examples of Job Classified Under this Code']\n",
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_40992/438202779.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  detailed_definitions['Jobs Cleaned'] = detailed_definitions['Jobs Cleaned'].str.replace(k, v)\n",
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_40992/438202779.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  detailed_definitions['Jobs Cleaned'] = detailed_definitions['Jobs Cleaned'].str.replace(k, v)\n"
     ]
    }
   ],
   "source": [
    "to_replace = {\n",
    "    '•': '',\n",
    "    '\\n': '.',\n",
    "    '<Blank>': '',\n",
    "    '\\([A-Za-z0-9 ]+\\)': ''\n",
    "}\n",
    "\n",
    "detailed_definitions['Jobs Cleaned'] = detailed_definitions['Examples of Job Classified Under this Code']\n",
    "\n",
    "for k, v in to_replace.items():\n",
    "    detailed_definitions['Jobs Cleaned'] = detailed_definitions['Jobs Cleaned'].str.replace(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "b7356cd1-be6f-43f3-ae46-670bb1514153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4               President .  Attorney general.  Minister \n",
       "6         Director-general.  High commissioner .  Perm...\n",
       "7         Chairman .  Chief executive .  Managing dire...\n",
       "9           Administrator of political party organisation\n",
       "11        Administrator of business association.  Admi...\n",
       "                              ...                        \n",
       "1599                                                     \n",
       "1601                               Newspaper delivery man\n",
       "1602      Parking meter reader.  Coin machine collecto...\n",
       "1603                                  Labourer.  Handyman\n",
       "1604                                Food delivery on foot\n",
       "Name: Jobs Cleaned, Length: 997, dtype: object"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_definitions['Jobs Cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431f306-0945-4d43-8629-9a4771b02b41",
   "metadata": {},
   "source": [
    "Write a simple function to identify the top `n` jobs that are closest to the selected SSOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95e3eedf-1a34-4d2a-b6f8-a0676f8d551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def identify_top_n(selected,\n",
    "                   data,\n",
    "                   extra_info,\n",
    "                   target_vecs,\n",
    "                   top_n = 10,\n",
    "                   threshold = 0.9):\n",
    "    \n",
    "    source_vec = np.array([np.mean([token.vector for token in selected], axis = 0)])\n",
    "    matrix = cosine_similarity(source_vec, target_vecs)\n",
    "    indices = np.apply_along_axis(lambda x: x.argsort()[-top_n:][::-1], axis = 1, arr = matrix)\n",
    "    above_threshold = matrix[0][indices][0] >= threshold\n",
    "    indices = [idx for idx, above in zip(indices[0], above_threshold) if above]\n",
    "    if len(indices) == 0:\n",
    "        print('None meet the threshold required.')\n",
    "    else:\n",
    "        cosine_similarity_index = 0\n",
    "        for i, row in data.loc[indices, :].iterrows():\n",
    "            print(f'Index: {i}')\n",
    "            print(f'Cosine similarity: {matrix[0][indices][cosine_similarity_index]}')\n",
    "            print(f'Predicted SSOC: {row[\"SSOC 2020\"]}')\n",
    "            print(f'Job title: {extra_info[\"title\"][i]}')\n",
    "            print(f'Description: {row[\"Cleaned_Description\"]}')\n",
    "            print('================================================================')\n",
    "            cosine_similarity_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "fdcb194f-2b03-4ac2-8629-381464f92bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_job_title(data,\n",
    "                            include,\n",
    "                            exclude):\n",
    "    \n",
    "    output = copy.deepcopy(data)\n",
    "    output['title'] = output['title'].str.lower()\n",
    "    \n",
    "    include_boolean = [False] * len(output)\n",
    "    for words in include:\n",
    "        entry_boolean = [True] * len(output)\n",
    "        for word in words.split(' '):\n",
    "            entry_boolean = entry_boolean & output['title'].str.contains(word.lower())\n",
    "        include_boolean = include_boolean | entry_boolean\n",
    "    \n",
    "    for words in exclude:\n",
    "        for word in words.split(' '):\n",
    "            include_boolean = include_boolean & ~output['title'].str.contains(word.lower())\n",
    "            \n",
    "    job_titles_idx = output[include_boolean.values].index.tolist()\n",
    "    return job_titles_idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "c49bdea3-17da-401e-8f6a-3a68386365eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "import copy\n",
    "import json\n",
    "# Run this to initialise the dictionary object\n",
    "# with open('manual_tagging.json', 'r') as outfile:\n",
    "#     manual_tagging1 = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "4070e05f-5ece-43fc-adca-ee844f52f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to export the manual tagging to the JSON file\n",
    "# with open('manual_tagging.json', 'w') as outfile:\n",
    "#     json.dump(manual_tagging, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b40010-733a-4d23-9cb3-41b09738bada",
   "metadata": {},
   "source": [
    "Set the SSOC you are scanning for here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "7e1bbdc2-1e40-420a-bef5-2c330598c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporate Relations Manager\n",
      "Director of marketing communications\n",
      "Public relations  director\n"
     ]
    }
   ],
   "source": [
    "ssoc = 12221\n",
    "include_job_titles = [17640, 30290, 34491, 36065, 36409, 37141, 42499]\n",
    "for detailed_def_job in detailed_definitions['Jobs Cleaned'][detailed_definitions['SSOC 2020'] == str(ssoc)].values[0].split('.'):\n",
    "    print(detailed_def_job.strip())\n",
    "    include_job_titles.append(detailed_def_job.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "d8f42705-cbe4-4fc7-b1e9-744638be2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_idx = find_matching_job_title(extra_info,\n",
    "                                         include = ['public relations manager'],\n",
    "                                         exclude = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "1bcaa1df-0052-4b8a-a3cb-eb8fc80b7868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8884, 17640, 30290, 34491, 36065, 36409, 37141, 42499]\n",
      "8884: Account Manager (Public Relations)\n",
      "17640: Digital Public Relations Manager #SGUP\n",
      "30290: Senior Manager, Public Relations\n",
      "34491: Public Relations Manager #SGUNITED\n",
      "36065: Public Relations Manager\n",
      "36409: Public Relations Manager\n",
      "37141: Public Relations Manager\n",
      "42499: Public Relations and Marketing Manager\n"
     ]
    }
   ],
   "source": [
    "print(job_titles_idx)\n",
    "for i, title in extra_info.loc[job_titles_idx, 'title'].iteritems():\n",
    "    print(f\"{i}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "5fac4437-1ec5-4eaa-9f8e-441f90592528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><strong>KEY ROLES &amp; RESPONSIBILITIES</strong></p>\n",
      "<p><strong>- </strong>Leading the planning and implementation of PR initiatives related to the company's brand(s).</p>\n",
      "<p>- Leveraging a variety of media channels to maximise brand and campaign exposure.</p>\n",
      "<p>- Building strong and positive relationships with media, influencers, and other related parties.</p>\n",
      "<p>- Tracking effectiveness of various campaigns and course correcting as required.</p>\n",
      "<p>- Maintain and develop relationship with the customers.</p>\n",
      "<p>-Execute any other job responsibilities which the management assigns from time to time.</p>\n",
      "<p><strong>Knowledge and Skill Requirements</strong></p>\n",
      "<p>&gt; Minimum Diploma in Marketing/Business/Creative Multimedia/Advertising/Media/Mass Communication or equivalent qualification</p>\n",
      "<p>&gt;Proficient in Microsoft Office (Word, Excel, PowerPoint) and prefer with Adobe Software skills.</p>\n",
      "<p>&gt;Proficient in managing social media networks for business purposes.</p>\n",
      "<p>&gt; Experience working in entertainment related field with corporate marketing role would be an advantage.</p>\n",
      "<p>&gt;Excellent verbal and written communication skills, preferably bilingual in English and Chinese Language</p>\n",
      "<p>&gt;Creative and meticulous individual</p>\n",
      "<p>&gt;Highly motivated and independent individual</p>\n"
     ]
    }
   ],
   "source": [
    "print(extra_info.loc[34491, 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d92d37df-b635-44ab-989b-f228a663ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 8185\n",
      "Cosine similarity: 0.955994710767933\n",
      "Predicted SSOC: 24320\n",
      "Job title: Senior Executive – Corporate Communications and Community Engagement\n",
      "Description: Create engaging and creative messages, collaterals and publicity materials to support programmes and activities, including fundraising campaigns. Planning and writing publications for Newsletters, Annual Report, Brochures etc. Assist in the updating and maintaining of website and social media platform. Handling of media enquiries, interviews and media liaison where required. Assist in the management of community partnership projects with schools, corporate companies, sponsors, etc. Management of organisation-wide communications plans (internal and external) Participate in events for Marketing/ Fundraising. Conceptualise and implement integrated branding campaign across a various communication platform including internal and outward-facing channels. Develop and nurture good relationships with the external stakeholders, media and influencers to build goodwill. Build stakeholder relationships - Identify opportunities for volunteer partnerships through engagements with social service organisations, community stakeholders, corporate partners and government agencies. Maintain relationships with social service organisations, community stakeholders, corporate partners and government agencies for volunteer recruitment. Drive engagement with key stakeholders through various platforms, working in conjunction with corporate communications. Identify key sector trends which may impact engagement strategy and volunteer management practices. Draft media releases, media statements, responses to media and public queries, and develop communications collaterals.\n",
      "================================================================\n",
      "Index: 39944\n",
      "Cosine similarity: 0.9472520950252847\n",
      "Predicted SSOC: 12211\n",
      "Job title: Marketing and Communications Manager \n",
      "Description: Managing the corporate marketing and related communications needs of the company. These range from preparing marketing brochures & collaterals to creating online content while complying with branding requirements. Planning regular and timely internal communications for all company news and crafting key messages and materials for business leaders. Managing media relations and external speaking engagements to profile our senior leaders as subject matter experts when required. Developing and managing the content framework and pipeline for all communications channels whether internal, external, online and in social media. Leading corporate marketing activities as required,  for example, at company exhibitions, trade fairs and at other functions and events. Support recruitment marketing in outreach to attract new talent into the company.\n",
      "================================================================\n",
      "Index: 29234\n",
      "Cosine similarity: 0.9462775308570235\n",
      "Predicted SSOC: 33499\n",
      "Job title: Public Relations Executive\n",
      "Description: Reporting to the Public Relations Manager, you will monitor and track all news stories and reports on all media channels, draft content for social media and print media that include notices, advisories and posters. You are responsible to administer and co-ordinate the production of corporate communication collaterals such as banners and perform procurement of corporate gifts. Furthermore, you will work with residents on S&CC arrears management and conduct outreach activities with schools and VWOs as well as to assist in events management and other duties on an ad-hoc basis.\n",
      "================================================================\n",
      "Index: 26419\n",
      "Cosine similarity: 0.9447415383773363\n",
      "Predicted SSOC: 11201\n",
      "Job title: PR & Media Relation - Senior Manager/Assistant DirectorD\n",
      "Description: Lead the development and implementation of end-to-end PR strategies and editorial/media influencer programmes including messaging, pitching, briefing facilitation and management of speaking engagements. Manage media relations activities in Asia, and act as the interface between the business and the media with the support of our PR agency. Manage together with the PR agency profiling and speaking engagements for our C-Suite, Investment and Client Portfolio Managers as well as media interviews. Push important corporate and investment topics and promote key brand messages. Provide sound counsel to LBU markers and oversee the engagement of regional / global media. Develop key UK/Europe and US media relations and contacts including proactive dissemination of investment insights, fund manager profiling involving print, broadcast and wires, timely investment commentary, corporate press announcements/releases. Support to the Head of Marketing and Communications to manage corporate reputation issues including high level issues management and preparedness. Collaborate with the manager for internal communications to lead in the development of speeches, Q&As and briefing materials for senior executives including members of the investment team. In partnership with the PR agency plan and deliver comprehensive media/client messaging training to support brand and embed media engagement protocols within senior leadership and client-facing staff (UK/Europe/US) Support the LBUs in all sensitive communications activities through best practice sharing, collaboration and contribute to the continued growth of a “communications community” Help to develop and to enhance communications expertise and skills of local market Marketing Managers / PR Managers through orientation and on-going coaching to build and protect Eastspring's brand in the local markets. Help Plan and manage the annual Communications and PR budget. Work closely with the Communications and Corporate Affairs department of Prudential Corporations Asia and Group Head Office. Develop a good understanding of the business as well as products and be able to navigate complexity for the function.\n",
      "================================================================\n",
      "Index: 7639\n",
      "Cosine similarity: 0.942478090695753\n",
      "Predicted SSOC: 12211\n",
      "Job title: Marketing and Communication Executive / Senior Executive (Media Relations and Public Relations)\n",
      "Description: Develop a public relations and communications plan including strategy, goals, budget and tactics. Develop government, trade associations and media relationship strategy, seeking high-level placements in print, broadcast and online media. Manage media inquiries, interview requests, press conferences and other corporate events. Drive issues management and crisis communications in cases of adverse publicity. Evaluate opportunities for partnerships, sponsorships, CSR and advertising on an on-going basis. Working with key internal stakeholders, to understand business and marketing imperatives to align communications objectives. Maintain a keen understanding of industry trends affecting and make appropriate recommendations regarding communication strategy Monitor, analyze and communicate PR results. Other duties as assigned.\n",
      "================================================================\n",
      "Index: 20716\n",
      "Cosine similarity: 0.9423735668250961\n",
      "Predicted SSOC: 24132\n",
      "Job title: R0122819 Corporate Bank - Junior Marketing Manager and Social Media Specialist - Assistant Vice President\n",
      "Description: Build relationships and work proactively with regional stakeholders to develop marketing communication plans which contribute to local/regional business objectives while supporting the global business and marketing goals. Regional assistance with the planning, coordination and execution of marketing activities across the CB business lines (Cash management, FX, Trade Finance and Lending, Securities Services and Trust & Agency Services) and across the APAC franchise (currently China, Singapore, India, Indonesia, Korea, Japan, Vietnam, Hong Kong, Malaysia, Thailand, Taiwan, Sri Lanka, Philippines) Develop relationships and work closely with the marketing business specialists in the global CB marketing team and regional and global corporate communications colleagues, and business colleagues as well as managing external media, agencies and vendor relations. Profile and showcase the CB APAC business by identifying, managing and delivering client-centric campaigns. Regional participation in awards, surveys and polls. Manage the process from identifying awards, writing, editing, reviewing, submissions and judges presentations. Identify and provide regular and meaningful regional content marketing contributions to the global CB website and client email campaigns along with regional print/digital collateral, advertising and social media. Special responsibility for the management of CB's global social media channels including performance measurement and audience growth activities. Regional assistance with budget planning/cost management. Ensure all appropriate approvals are obtained for all activities and maintain appropriate records of such approvals.\n",
      "================================================================\n",
      "Index: 40679\n",
      "Cosine similarity: 0.9403646001006172\n",
      "Predicted SSOC: 33229\n",
      "Job title: Retail Marketing Officer\n",
      "Description: Plan and execute all digital marketing strategies including social media and analyse its effectiveness. Develop ATL and BTL promotional activities for local and regional offices. Organize marketing events and provide on-site support. Act as a brand guardian for the retail business. Liaise and coordinate with media agencies, vendors and suppliers. Manage other ad-hoc projects.\n",
      "================================================================\n",
      "Index: 40407\n",
      "Cosine similarity: 0.9397320704444675\n",
      "Predicted SSOC: 41109\n",
      "Job title: Marketing and Communications Assistant #SGUnitedTraineeships\n",
      "Description: online marketing campaigns pro-active website and social media management (update and maintain with content publications/news, website monitoring, SEO, site referencing, ad-paid service…) Traditional Marketing: offline marketing campaign and publications (press releases, article, advertising, advertorial…) Participate to commercial and corporate events, PR and networking (conferences, exhibitions, presentations…) Assist in events and PR management: A to Z preparation and organisation (conferences, seminars, workshop, webinar, exhibitions, site visits…) Assist in Webinars preparation and organisation. Coordinate with Managers on research projects, such as behavioural studies, market analysis activities to understand industries trends/dynamics. Assist in identifying marketing trends and key opportunities for innovation. Conduct and analyse clients rating/feedback forms reports and questionnaires. Help in monitoring and maintaining knowledge management tools & databases. Work closely with Managers in content development (case study, industry spotlight, storytelling…) Liaise with external stakeholders (Media, PR, Communications agencies, government agencies, chambers, associations, partners…) Update and maintain internal communications.\n",
      "================================================================\n",
      "Index: 31208\n",
      "Cosine similarity: 0.9391798956499025\n",
      "Predicted SSOC: 12211\n",
      "Job title: Marketing Executive #SGUnitedTraineeships #SGUP\n",
      "Description: Liaising with listed companies and investor relation firms in organising dialogue sessions and/or any corporate events. Working closely with the internal events team on the execution and operation of the aforesaid events. The marketing activities, focusing on digital platforms, to promote the events. Attending to all administrative work related to the events. Assisting the team in planning and executing SIAS Associate Membership campaigns and programmes. Understand listed companies' corporate timeline and actions. How to engage different stakeholders to achieve results. Learnt B2B and B2B2C marketing. Understand job functions of public relations and investor relations.\n",
      "================================================================\n",
      "Index: 23875\n",
      "Cosine similarity: 0.9377103103918697\n",
      "Predicted SSOC: 24212\n",
      "Job title: Senior Consultant\n",
      "Description: Assist with client counsel on all aspects of their communications programmes. Provide input into and draft various communications materials for client programmes across our different offerings of corporate communications, public affairs and financial communications areas (e g communications strategies, press releases, client memos, stakeholder and issues mapping, and briefing documents) Deliver high-quality work within project teams. Under guidance and supervision, manage various tasks related to corporate communications and media relations engagements ranging from executive profiling and brand positioning to issues and crisis management and top-tier media engagement. Assist senior team members in developing new business initiatives (e g conducting research on prospects, developing strategic approaches and ideas, reviewing PowerPoint presentations and proposals, etc.) Actively contribute to both internal and client meetings by offering strategic and creative insights. Maintain a self-motivated passion to stay abreast of client issues and related industry trends, participate in industry networking events and inform team leaders of potential new business opportunities.\n",
      "================================================================\n",
      "Index: 12521\n",
      "Cosine similarity: 0.9376164043025479\n",
      "Predicted SSOC: 24320\n",
      "Job title: public relation officer\n",
      "Description: Management and engagement of residents and public queries, feedback and complaints. Manage press queries and print/social media. Coordinate community events and provide event management support to property teams. Organize Town Council events (involving 1,000 to 2,000 participants) and private events involving 100 to 200 participants. Liaison with various stakeholders including Residents' Committees, Grassroots Organizations, Government Agencies and Contractors to ensure corporate and client's image are improved. Attend all relevant site meetings. Produce print / online media and collaterals for public engagement. Perform Public Relations / Customer Service support to property teams and to assist CSO if required. Assist the Public Relations Manager on special projects as required by Town Council.\n",
      "================================================================\n",
      "Index: 30365\n",
      "Cosine similarity: 0.9375827058920629\n",
      "Predicted SSOC: 12199\n",
      "Job title: Senior Manager or Manager, Communications\n",
      "Description: Deliver and develop a robust communications strategy and implementation plan which encompass PR and other communication vehicles, including new social platforms, video and other multi-media tools to understand AHL's multiple stakeholders and customers and build equity of AHL brand. Develop powerful narratives and compelling content to promote the suite of festivals and programmes under the initiative (e g media materials, profile stories and videos) to various target audiences, e g media, public, partners etc. Manage media relations and take the lead in handling media queries, drafting media materials and analysing media coverage. Responsible for Communications Planning & Message Development: To develop and execute PR plans craft media materials including press releases, speeches, message houses, media responses and interview talking points. Grow media partnerships organically and coordinate media interest in AHL and ensure regular contact with target media and appropriate response to media requests. Act as AHL's representative with the media and develop strong relationships with media representatives, locally and internationally. Establish and drive internal and external communications strategy that positively affects employees' and customers' understanding, engagement and commitment with the result of an even stronger connection to AHL. Work closely with marketing colleagues on integrating media communications initiatives within marketing strategies and campaigns/activities for key festivals and programmes. Assist Head of Marcomms to develop the department's strategic short and long term plan, and address critical issues in the communications area. The plan will set out objectives for each strategic area of the department and outlines plans and budget for achieving those objectives. Keep abreast of the development in the field of communications and public relations, not-for-profit management and governance. To carry out and assume any other duties and responsibilities as and when assigned by the AHL Management.\n",
      "================================================================\n",
      "Index: 21227\n",
      "Cosine similarity: 0.9360830628480632\n",
      "Predicted SSOC: 24312\n",
      "Job title: Brand Manager\n",
      "Description: Analyse brand positioning and consumer's characteristics to develop marketing strategies to achieve the business objectives. Prepare and present reports on annual marketing spending, strategies and planning to update the management on the company's marketing direction. Work within marketing budget to increase public awareness of the company. Conceptualise and develop digital and social media strategies, and execute paid digital and social media campaigns to achieve business growth. Produce engaging content for the company website and social media to attract the targeted audience. Monitor and report performance of the effectiveness of marketing initiatives engaged to identify business opportunities. Collaborate with the events executive to conduct publicity events to increase customer base. Collaborate and communicate with managers to ensure information are relayed accordingly and right support is provided on both sides. Review marketing materials to support product sales and services, including copywriting, proofreading and communicating proper brief to graphic designer. Any other marketing related initiatives and projects as assigned by the director.\n",
      "================================================================\n",
      "Index: 23256\n",
      "Cosine similarity: 0.9356359546433781\n",
      "Predicted SSOC: 33221\n",
      "Job title: Business Development Executive\n",
      "Description: Maintain and contact our existing customer database, as well as finding new prospects, establishing new business and cultivating and managing long-term relationships. Identifying marketing and sponsorship opportunities (including awards, conferences and targeted events) Development and servicing support of new and current sponsorship of the Technical-related Programme as well as Institutional activities. Conceptualisation, planning, management and execution of all content/programming aspects of the events, activities and roadshow including creative formalisation of general programme flow and execution. Coordination and direct liaison with relevant department/ teams for all Programme related marketing operations. General Event Management, Marketing, Client Servicing, Registration and Administration. Generate regular reports and proposals relating to responsibilities of the department. Manage general enquiries and potential disputes of account management.\n",
      "================================================================\n",
      "Index: 16790\n",
      "Cosine similarity: 0.9355489071754267\n",
      "Predicted SSOC: 12211\n",
      "Job title: Marcom Executive (Entry Level/West/B2C)\n",
      "Description: Create collateral materials for marketing campaign across various communication platforms including marketing events, shopping malls promotions, government-related initiatives and etc. Create awareness of marketing campaigns by generating creative message on social media platforms. Liaise with relevant parties such as event, logistics, and financial department to assure the marketing campaigns is well conducted. Ad hoc duties as and when assigned.\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "ssoc_index = SSOC_2020[SSOC_2020['SSOC 2020'] == ssoc].index[0]\n",
    "identify_top_n(SSOC_2020_nlp[ssoc_index], data, extra_info, target_vecs, top_n = 15, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91919bb2-3089-43bf-9c42-26245abdf1c9",
   "metadata": {},
   "source": [
    "Use this to find job postings with the exact job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "972be88c-7440-4c4e-879a-2dd33bcb0365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 3712, 4096, 4234, 4282, 4814, 5087, 5125, 5657, 6845, 7185, 7221, 7245, 9761, 10365, 11048, 11254, 11894, 12867, 14357, 14573, 14652, 14748, 15072, 15314, 16527, 16822, 16999, 19009, 19756, 20075, 20098, 20105, 20209, 20319, 20426, 20815, 20899, 21097, 21911, 22086, 22249, 22414, 22588, 22625, 22739, 23144, 23148, 23370, 23488, 24173, 24440, 24704, 24999, 26027, 26832, 27082, 27821, 29594, 30976, 31347, 31541, 32018, 32176, 32478, 32516, 32967, 33005, 33542, 33724, 35692, 36537, 36674, 37348, 37682, 38313, 38352, 41510]\n"
     ]
    }
   ],
   "source": [
    "words = ['admin', 'manager'] # what words to include\n",
    "exclude = ['account', 'database', 'it', 'project'] # what words to exclude\n",
    "output = copy.deepcopy(extra_info)\n",
    "for word in words:\n",
    "    output = output[output['title'].str.lower().str.contains(word)]\n",
    "for word in exclude:\n",
    "    output = output[~output['title'].str.lower().str.contains(word)]\n",
    "job_titles_idx = output.index.tolist()\n",
    "print(job_titles_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69145a14-882b-484b-a6be-0abc3750c345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cb697-3a53-49c4-8487-767a233dd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra_info.loc[25834, 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f12ec-5146-4683-912b-a3b1b6911ca7",
   "metadata": {},
   "source": [
    "Change the list `inputting` here to input the indices of the job postings that you want to manually tag as that SSOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "c948c3c2-d29d-444d-aee8-8d31ca2d331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate detected for index 10797 which has already been marked for SSOC 12111\n",
      "Duplicate detected for index 40689 which has already been marked for SSOC 12111\n",
      "Duplicate detected for index 33662 which has already been marked for SSOC 12111\n",
      "Duplicate detected for index 16999 which has already been marked for SSOC 12121\n",
      "Duplicate detected for index 26832 which has already been marked for SSOC 12121\n",
      "Duplicate detected for index 14748 which has already been marked for SSOC 12121\n",
      "SSOC: 12112\n",
      "[4096, 5125, 7185, 14357, 5657, 9761, 36, 41510, 7221, 22588, 19009, 12867, 22086, 7245, 22625, 23144, 21097, 20075, 23148, 24173, 31347, 11894, 10365, 3712, 24704, 20098, 20105, 4234, 16527, 27821, 36537, 4282, 6845, 32967, 4814, 22739, 32478, 15072, 22249, 14573, 33005, 20209, 30976, 32516, 33542, 32018, 11048, 19756, 37682, 31541, 14652, 36674, 23370, 20815, 20319, 35692, 24440, 22414, 21911, 29594, 20899, 24999, 38313, 26027, 32176, 16822, 33724, 23488, 20426, 27082, 38352, 15314, 5087, 37348, 11254]\n"
     ]
    }
   ],
   "source": [
    "manual_tagging[ssoc] = []\n",
    "inputting = []\n",
    "inputting_dedup = list(set(inputting))\n",
    "for key in manual_tagging.keys():\n",
    "    for new_idx in inputting_dedup:\n",
    "        if new_idx in manual_tagging[key]:\n",
    "            print(f'Duplicate detected for index {new_idx} which has already been marked for SSOC {key}')\n",
    "            inputting_dedup.remove(new_idx)\n",
    "manual_tagging[ssoc].extend(inputting_dedup)\n",
    "print(f'SSOC: {ssoc}')\n",
    "print(manual_tagging[ssoc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5ac5910f-1ff1-4868-8666-7a5e280860de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "084a1431-9ea9-4b22-aba7-1f32712ee62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssoc_index = SSOC_2020[SSOC_2020['SSOC 2020'] == ssoc].index[0]\n",
    "# identify_top_n(SSOC_2020_nlp[ssoc_index], data, extra_info, target_vecs, top_n = 15, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804e260-1e35-4013-a7c4-5043c285229c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D) Trying out lambada\n",
    "\n",
    "Ref: https://github.com/makcedward/nlpaug/blob/master/example/lambada-train_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb197d9e-bad9-4a93-8781-cdf7a24704db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.sample(100)\n",
    "\n",
    "test_data = test_data[['SSOC 2020', 'Cleaned_Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57ecde3b-1665-4fb2-81ad-8f1425c623ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rename({'SSOC 2020': 'label', 'Cleaned_Description': 'text'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60c8f9aa-309f-4b1e-bd18-4dab864a9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4dae864-e575-426f-832a-5d98cf5aec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('Data/test/classification.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06967df2-b3ce-48e9-abd6-53acf1780a8d",
   "metadata": {},
   "source": [
    "Training classifier\n",
    "\n",
    "DL files from nlpaug\n",
    "Copy and paste scripts from nlpaug to Models folder\n",
    "Create file path model\\lambada\\cls in Models folder\n",
    "Uploaded c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24f3a89b-23ff-4ed2-9318-edeb794f1cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:586: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|#2        | 1/8 [00:04<00:34,  4.91s/it]\n",
      " 12%|#2        | 1/8 [00:04<00:34,  4.91s/it]C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "C:\\Users\\benjamin\\Desktop\\my_enviro\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "\n",
      "\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1 of 2:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "Running Epoch 0 of 2:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Epochs 0/2. Running Loss:    1.0667:   0%|          | 0/1 [00:03<?, ?it/s]\u001b[A\n",
      "\n",
      "Epochs 0/2. Running Loss:    1.0667: 100%|##########| 1/1 [00:09<00:00,  9.24s/it]\u001b[A\n",
      "Epochs 0/2. Running Loss:    1.0667: 100%|##########| 1/1 [00:09<00:00,  9.24s/it]\n",
      "\n",
      "Epoch 1 of 2:  50%|#####     | 1/2 [00:21<00:21, 21.15s/it]\n",
      "Epoch 2 of 2:  50%|#####     | 1/2 [00:21<00:21, 21.15s/it]\n",
      "\n",
      "Running Epoch 1 of 2:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Epochs 1/2. Running Loss:    1.0800:   0%|          | 0/1 [00:04<?, ?it/s]\u001b[A\n",
      "\n",
      "Epochs 1/2. Running Loss:    1.0800: 100%|##########| 1/1 [00:09<00:00,  9.87s/it]\u001b[A\n",
      "Epochs 1/2. Running Loss:    1.0800: 100%|##########| 1/1 [00:09<00:00,  9.87s/it]\n",
      "\n",
      "Epoch 2 of 2: 100%|##########| 2/2 [00:43<00:00, 21.60s/it]\n",
      "Epoch 2 of 2: 100%|##########| 2/2 [00:43<00:00, 21.53s/it]\n"
     ]
    }
   ],
   "source": [
    "!python Models/scripts/lambada/train_cls.py  \\\n",
    "    --train_data_path Data/test/classification.csv \\\n",
    "    --val_data_path Data/test/classification.csv \\\n",
    "    --output_dir Models/model/lambada/cls \\\n",
    "    --device cpu \\\n",
    "    --num_epoch 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a77c5-cd76-4395-a006-82a8f74bb7dd",
   "metadata": {},
   "source": [
    "Output processing data as mlm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "215d5d70-be1f-49f8-844a-71769ed33df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Models/scripts/lambada/data_processing.py \\\n",
    "    --data_path Data/test/classification.csv \\\n",
    "    --output_dir Data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bab36686-9f2f-44be-a1e2-baca36c41a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/15/2021 17:53:44 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "10/15/2021 17:53:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\n",
      "0 tables [00:00, ? tables/s]\n",
      "                            \n",
      "[INFO|file_utils.py:1631] 2021-10-15 17:53:46,440 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\benjamin\\.cache\\huggingface\\transformers\\tmpgxgi5rlu\n",
      "\n",
      "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]\n",
      "Downloading: 100%|##########| 665/665 [00:00<00:00, 166kB/s]\n",
      "[INFO|file_utils.py:1635] 2021-10-15 17:53:47,507 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at C:\\Users\\benjamin/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|file_utils.py:1643] 2021-10-15 17:53:47,510 >> creating metadata file for C:\\Users\\benjamin/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:545] 2021-10-15 17:53:47,519 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\benjamin/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:581] 2021-10-15 17:53:47,521 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1664] 2021-10-15 17:53:47,529 >> Didn't find file Models/model/lambada/cls\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,531 >> loading file Models/model/lambada/cls\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,532 >> loading file Models/model/lambada/cls\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,532 >> loading file Models/model/lambada/cls\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,532 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,532 >> loading file Models/model/lambada/cls\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1728] 2021-10-15 17:53:47,532 >> loading file Models/model/lambada/cls\\tokenizer_config.json\n",
      "[INFO|file_utils.py:1631] 2021-10-15 17:53:48,684 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\benjamin\\.cache\\huggingface\\transformers\\tmpjm6zvm74\n",
      "\n",
      "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]\n",
      "Downloading:   1%|          | 3.78M/548M [00:00<00:14, 37.8MB/s]\n",
      "Downloading:   2%|1         | 10.6M/548M [00:00<00:09, 55.4MB/s]\n",
      "Downloading:   3%|2         | 16.4M/548M [00:00<00:09, 56.3MB/s]\n",
      "Downloading:   4%|4         | 23.2M/548M [00:00<00:08, 60.7MB/s]\n",
      "Downloading:   5%|5         | 29.2M/548M [00:00<00:08, 58.8MB/s]\n",
      "Downloading:   7%|6         | 35.8M/548M [00:00<00:08, 61.0MB/s]\n",
      "Downloading:   8%|7         | 41.9M/548M [00:00<00:09, 54.3MB/s]\n",
      "Downloading:   9%|8         | 49.2M/548M [00:00<00:08, 59.7MB/s]\n",
      "Downloading:  10%|#         | 55.3M/548M [00:00<00:08, 55.4MB/s]\n",
      "Downloading:  11%|#1        | 62.5M/548M [00:01<00:08, 59.8MB/s]\n",
      "Downloading:  13%|#2        | 68.6M/548M [00:01<00:08, 57.5MB/s]\n",
      "Downloading:  14%|#3        | 75.5M/548M [00:01<00:07, 60.7MB/s]\n",
      "Downloading:  15%|#4        | 81.7M/548M [00:01<00:07, 58.4MB/s]\n",
      "Downloading:  16%|#6        | 88.6M/548M [00:01<00:07, 61.1MB/s]\n",
      "Downloading:  17%|#7        | 94.7M/548M [00:01<00:07, 60.8MB/s]\n",
      "Downloading:  18%|#8        | 101M/548M [00:01<00:07, 61.9MB/s] \n",
      "Downloading:  20%|#9        | 107M/548M [00:01<00:07, 55.9MB/s]\n",
      "Downloading:  21%|##        | 115M/548M [00:01<00:07, 60.3MB/s]\n",
      "Downloading:  22%|##2       | 121M/548M [00:02<00:06, 61.8MB/s]\n",
      "Downloading:  24%|##3       | 129M/548M [00:02<00:06, 66.3MB/s]\n",
      "Downloading:  25%|##4       | 136M/548M [00:02<00:06, 63.8MB/s]\n",
      "Downloading:  26%|##5       | 142M/548M [00:02<00:06, 62.9MB/s]\n",
      "Downloading:  27%|##7       | 149M/548M [00:02<00:06, 60.3MB/s]\n",
      "Downloading:  28%|##8       | 156M/548M [00:02<00:06, 63.3MB/s]\n",
      "Downloading:  30%|##9       | 162M/548M [00:02<00:06, 63.5MB/s]\n",
      "Downloading:  31%|###       | 169M/548M [00:02<00:05, 63.6MB/s]\n",
      "Downloading:  32%|###1      | 175M/548M [00:02<00:06, 59.2MB/s]\n",
      "Downloading:  33%|###3      | 183M/548M [00:03<00:05, 65.0MB/s]\n",
      "Downloading:  35%|###4      | 190M/548M [00:03<00:05, 62.8MB/s]\n",
      "Downloading:  36%|###5      | 196M/548M [00:03<00:05, 61.3MB/s]\n",
      "Downloading:  37%|###6      | 202M/548M [00:03<00:05, 60.4MB/s]\n",
      "Downloading:  38%|###8      | 209M/548M [00:03<00:05, 61.9MB/s]\n",
      "Downloading:  39%|###9      | 215M/548M [00:03<00:05, 60.8MB/s]\n",
      "Downloading:  40%|####      | 221M/548M [00:03<00:05, 62.2MB/s]\n",
      "Downloading:  42%|####1     | 228M/548M [00:03<00:04, 64.1MB/s]\n",
      "Downloading:  43%|####3     | 236M/548M [00:03<00:04, 69.1MB/s]\n",
      "Downloading:  44%|####4     | 243M/548M [00:03<00:04, 67.6MB/s]\n",
      "Downloading:  46%|####5     | 250M/548M [00:04<00:04, 64.6MB/s]\n",
      "Downloading:  47%|####6     | 257M/548M [00:04<00:04, 62.3MB/s]\n",
      "Downloading:  48%|####7     | 263M/548M [00:04<00:04, 59.8MB/s]\n",
      "Downloading:  49%|####9     | 270M/548M [00:04<00:04, 62.7MB/s]\n",
      "Downloading:  50%|#####     | 276M/548M [00:04<00:04, 60.1MB/s]\n",
      "Downloading:  52%|#####1    | 284M/548M [00:04<00:03, 66.0MB/s]\n",
      "Downloading:  53%|#####3    | 291M/548M [00:04<00:04, 63.3MB/s]\n",
      "Downloading:  54%|#####4    | 298M/548M [00:04<00:03, 64.8MB/s]\n",
      "Downloading:  56%|#####5    | 305M/548M [00:04<00:03, 62.3MB/s]\n",
      "Downloading:  57%|#####6    | 312M/548M [00:05<00:03, 65.0MB/s]\n",
      "Downloading:  58%|#####8    | 318M/548M [00:05<00:03, 62.8MB/s]\n",
      "Downloading:  59%|#####9    | 325M/548M [00:05<00:03, 62.8MB/s]\n",
      "Downloading:  60%|######    | 331M/548M [00:05<00:03, 59.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=Models/scripts/lambada/gen\\runs\\Oct15_17-53-44_benjamin-PC,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "output_dir=Models/scripts/lambada/gen,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gen,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=Models/scripts/lambada/gen,\n",
      "save_on_each_node=False,\n",
      "save_steps=10000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "here\n",
      "10/15/2021 17:53:45 - WARNING - datasets.builder -   Using custom data configuration default-aae446fc9adb2493\n",
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\benjamin\\.cache\\huggingface\\datasets\\text\\default-aae446fc9adb2493\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "Dataset text downloaded and prepared to C:\\Users\\benjamin\\.cache\\huggingface\\datasets\\text\\default-aae446fc9adb2493\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
      "{'train_runtime': 41.6475, 'train_samples_per_second': 0.096, 'train_steps_per_second': 0.048, 'train_loss': 42.01639938354492, 'epoch': 2.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =    42.0164\n",
      "  train_runtime            = 0:00:41.64\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      0.096\n",
      "  train_steps_per_second   =      0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  62%|######1   | 337M/548M [00:05<00:03, 60.2MB/s]\n",
      "Downloading:  63%|######2   | 343M/548M [00:05<00:03, 54.0MB/s]\n",
      "Downloading:  64%|######3   | 350M/548M [00:05<00:03, 58.2MB/s]\n",
      "Downloading:  65%|######5   | 357M/548M [00:05<00:03, 59.8MB/s]\n",
      "Downloading:  66%|######6   | 363M/548M [00:05<00:03, 60.5MB/s]\n",
      "Downloading:  67%|######7   | 369M/548M [00:06<00:02, 60.5MB/s]\n",
      "Downloading:  69%|######8   | 377M/548M [00:06<00:02, 67.2MB/s]\n",
      "Downloading:  70%|#######   | 384M/548M [00:06<00:02, 61.6MB/s]\n",
      "Downloading:  71%|#######1  | 392M/548M [00:06<00:02, 66.2MB/s]\n",
      "Downloading:  73%|#######2  | 399M/548M [00:06<00:02, 62.2MB/s]\n",
      "Downloading:  74%|#######3  | 405M/548M [00:06<00:02, 62.9MB/s]\n",
      "Downloading:  75%|#######5  | 412M/548M [00:06<00:02, 65.8MB/s]\n",
      "Downloading:  76%|#######6  | 419M/548M [00:06<00:01, 66.1MB/s]\n",
      "Downloading:  78%|#######7  | 426M/548M [00:06<00:01, 62.2MB/s]\n",
      "Downloading:  79%|#######8  | 432M/548M [00:07<00:01, 63.2MB/s]\n",
      "Downloading:  80%|########  | 439M/548M [00:07<00:01, 64.5MB/s]\n",
      "Downloading:  82%|########1 | 447M/548M [00:07<00:01, 67.8MB/s]\n",
      "Downloading:  83%|########2 | 455M/548M [00:07<00:01, 70.6MB/s]\n",
      "Downloading:  84%|########4 | 462M/548M [00:07<00:01, 70.6MB/s]\n",
      "Downloading:  86%|########5 | 469M/548M [00:07<00:01, 70.4MB/s]\n",
      "Downloading:  87%|########6 | 476M/548M [00:07<00:01, 66.7MB/s]\n",
      "Downloading:  88%|########8 | 483M/548M [00:07<00:00, 66.3MB/s]\n",
      "Downloading:  89%|########9 | 489M/548M [00:07<00:01, 58.6MB/s]\n",
      "Downloading:  91%|######### | 497M/548M [00:07<00:00, 62.2MB/s]\n",
      "Downloading:  92%|#########1| 503M/548M [00:08<00:00, 59.5MB/s]\n",
      "Downloading:  93%|#########2| 509M/548M [00:08<00:00, 54.0MB/s]\n",
      "Downloading:  94%|#########3| 515M/548M [00:08<00:00, 51.9MB/s]\n",
      "Downloading:  95%|#########5| 522M/548M [00:08<00:00, 57.0MB/s]\n",
      "Downloading:  96%|#########6| 528M/548M [00:08<00:00, 55.3MB/s]\n",
      "Downloading:  98%|#########7| 535M/548M [00:08<00:00, 59.3MB/s]\n",
      "Downloading:  99%|#########8| 541M/548M [00:08<00:00, 56.6MB/s]\n",
      "Downloading: 100%|#########9| 546M/548M [00:08<00:00, 53.6MB/s]\n",
      "Downloading: 100%|##########| 548M/548M [00:08<00:00, 61.3MB/s]\n",
      "[INFO|file_utils.py:1635] 2021-10-15 17:53:57,707 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at C:\\Users\\benjamin/.cache\\huggingface\\transformers\\752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|file_utils.py:1643] 2021-10-15 17:53:57,708 >> creating metadata file for C:\\Users\\benjamin/.cache\\huggingface\\transformers\\752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1275] 2021-10-15 17:53:57,709 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at C:\\Users\\benjamin/.cache\\huggingface\\transformers\\752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1514] 2021-10-15 17:53:59,120 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1523] 2021-10-15 17:53:59,120 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3251] 2021-10-15 17:53:59,688 >> Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|run_clm.py:350] 2021-10-15 17:53:59,688 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
      "\n",
      "Running tokenizer on dataset: 100%|##########| 1/1 [00:00<00:00, 52.63ba/s]\n",
      "\n",
      "Grouping texts in chunks of 512:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Grouping texts in chunks of 512: 100%|##########| 1/1 [00:00<00:00, 142.86ba/s]\n",
      "[INFO|trainer.py:1170] 2021-10-15 17:54:00,229 >> ***** Running training *****\n",
      "[INFO|trainer.py:1171] 2021-10-15 17:54:00,229 >>   Num examples = 2\n",
      "[INFO|trainer.py:1172] 2021-10-15 17:54:00,229 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1173] 2021-10-15 17:54:00,229 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1174] 2021-10-15 17:54:00,229 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1175] 2021-10-15 17:54:00,229 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1176] 2021-10-15 17:54:00,229 >>   Total optimization steps = 2\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:32<00:32, 32.51s/it]\n",
      "100%|##########| 2/2 [00:41<00:00, 18.75s/it][INFO|trainer.py:1366] 2021-10-15 17:54:41,877 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                             \n",
      "\n",
      "100%|##########| 2/2 [00:41<00:00, 18.75s/it]\n",
      "100%|##########| 2/2 [00:41<00:00, 20.81s/it]\n",
      "[INFO|trainer.py:1925] 2021-10-15 17:54:41,881 >> Saving model checkpoint to Models/scripts/lambada/gen\n",
      "[INFO|configuration_utils.py:379] 2021-10-15 17:54:41,883 >> Configuration saved in Models/scripts/lambada/gen\\config.json\n",
      "[INFO|modeling_utils.py:1001] 2021-10-15 17:54:45,320 >> Model weights saved in Models/scripts/lambada/gen\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2006] 2021-10-15 17:54:45,326 >> tokenizer config file saved in Models/scripts/lambada/gen\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2021-10-15 17:54:45,327 >> Special tokens file saved in Models/scripts/lambada/gen\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!python Models/scripts/lambada/run_clm.py \\\n",
    "    --tokenizer_name Models/model/lambada/cls \\\n",
    "    --model_name_or_path gpt2 \\\n",
    "    --model_type gpt2 \\\n",
    "    --train_file Data/test/mlm_data.txt \\\n",
    "    --output_dir Models/scripts/lambada/gen \\\n",
    "    --do_train \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --save_steps=10000 \\\n",
    "    --num_train_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d055e1-ac73-485d-af89-2146c33e9b91",
   "metadata": {},
   "source": [
    "Not tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046db47f-9683-43b5-9474-f6957fe6e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nas.LambadaAug(model_dir='../model/lambada', threshold=0.3, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de8a15-5791-48a8-ba84-06c32d1f44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug.augment(['24111', '23619'], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f5b32-838a-4617-a604-2543b11c33ed",
   "metadata": {},
   "source": [
    "This entry gave errors, a character is not UTF-8 compliant, not sure which one though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b608b122-4829-4eea-bc2d-f3c34834753a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Oracle Financial modules –Gen'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Atleast 7 years of working experience in Oracle Financial Cloud consultant and 2 live projects implementation. Strong expertise on Oracle Financial modules –General Ledger, Payables, Fixed Assets, Cash Management, Accounts Payables, Accounts Receivables, Inventory, Purchasing and order Management modules and preparing the Financial Statements for client. Strong technical and analytical skills on problem-solving and suggest solutions. Working experience on Integrations between Cloud and on-premise systems. Should have ability to work independently on Project deliverables.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87bbc6-f8da-4355-b57e-3d650e108d92",
   "metadata": {},
   "source": [
    "#### E) Implementing Data Augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
