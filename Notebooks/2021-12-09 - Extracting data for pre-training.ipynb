{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b357abd3",
   "metadata": {},
   "source": [
    "## Extracting data for pre-training\n",
    "\n",
    "**Author:** Benjamin Aw  \n",
    "**Date:** 9 Dec 2021  \n",
    "**Context:** Pre-training the model to contextualise the distilBERT model requires a particular format  \n",
    "**Objective:** Extracting the CSV files, and coverting them into text files while also removing all HTML tags  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7db20",
   "metadata": {},
   "source": [
    "#### A) Setting up\n",
    "\n",
    "Importing the libraries and obtaing the file path for the datasets required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de275eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# changing directory first in order to import the package\n",
    "os.chdir('..')\n",
    "from ssoc_autocoder.processing import final_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5e7c0",
   "metadata": {},
   "source": [
    "#### B) Writing out the necessary functions\n",
    "\n",
    "Because a cleaning code `final_cleaning` has been written out, we can utilise that function instead of writing out our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfc8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text_and_check(text):\n",
    "    \n",
    "    cleaned_text = final_cleaning(text)\n",
    "    \n",
    "    # add in additional check for proper sentences\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6305e",
   "metadata": {},
   "source": [
    "We need to loop through each individial file by year-month, and append each cleaned entry to the appropriate text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4021d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def output_individual_files(path):\n",
    "\n",
    "    for filename in os.listdir(path + \"mcf_api_responses_csv\"):\n",
    "\n",
    "        df = pd.read_csv(path + \"mcf_api_responses_csv/\" + filename)\n",
    "        df_desc = df['description'].apply(cleaning_text_and_check)\n",
    "\n",
    "        # Writing it out into a readable text file\n",
    "\n",
    "        filename_without_ext = filename.strip(\".csv\")\n",
    "\n",
    "        for jd in df_desc: \n",
    "            fi = open(\"../Data/Train/pre-training data/\" + filename_without_ext + \".txt\", \"a\")\n",
    "            fi.write(f'{jd}\\n')\n",
    "            fi.close()\n",
    "        \n",
    "        print(f\"{filename_without_ext}.txt saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146951e7-1e59-47d9-8710-c966ec1ec827",
   "metadata": {},
   "source": [
    "Writing a separate function that lumps all the text files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6644b64-145c-44bb-9a95-90c392db0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_combined_file(path):\n",
    "\n",
    "    df_list = []\n",
    "    for filename in os.listdir(path):\n",
    "\n",
    "        print(f'Processing {filename}...\\r', end = '')\n",
    "        df = pd.read_csv(path + \"/\" + filename)\n",
    "        df_desc = df['description'].apply(cleaning_text_and_check)\n",
    "        df_list.append(df_desc)\n",
    "    \n",
    "    print('')\n",
    "    combined_df = pd.concat(df_list, ignore_index = True)\n",
    "    print(f'Shape of combined dataframe: {combined_df.shape}')\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289d522",
   "metadata": {},
   "source": [
    "#### C) Putting it all together\n",
    "\n",
    "We run the functions as described above, to create the appropriate text files that is requried for pre-training of the distilledBERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a361323",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_individual_files('Data/Raw/mcf_api_responses_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1c9b0aa-f7ad-4295-a79c-93b81273f187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw_2021-05.csv...\n",
      "Shape of combined dataframe: (882214,)\n"
     ]
    }
   ],
   "source": [
    "combined_df = output_combined_file('Data/Raw/mcf_api_responses_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7048330-3c3c-443e-a014-01284b9b29b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Train/pre-training-full.txt saved!\n"
     ]
    }
   ],
   "source": [
    "# Writing it out into a readable text file\n",
    "\n",
    "filepath = 'Data/Train/pre-training-full.txt'\n",
    "\n",
    "with open(filepath, 'w+') as outfile:\n",
    "    for i, jd in enumerate(combined_df):\n",
    "#         if i >= 1000:\n",
    "#             break\n",
    "        outfile.write(f'{jd}\\n')\n",
    "\n",
    "print(f\"{filepath} saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149cf96-ecc6-49fe-8ec7-fb5e0b9a9d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
