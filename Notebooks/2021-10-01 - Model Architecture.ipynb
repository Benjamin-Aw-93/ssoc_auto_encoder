{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical/Straight through Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148bc89-7942-4bf5-a9f0-8d288f7bea0b",
   "metadata": {},
   "source": [
    "**Author:** Shaun Khoo  \n",
    "**Date:** 1 Oct 2021  \n",
    "**Context:** Adapting Shopify's approach to classifying products using a hierarchical classifier (see reference below)  \n",
    "**Objective:** Develop code for training a hierarchical classifier neural network\n",
    "\n",
    "Some references:\n",
    "\n",
    "* [this article by Shopify](https://shopify.engineering/introducing-linnet-using-rich-image-text-data-categorize-products)\n",
    "* [How to do transfer learning on PyTorch / Transformers](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b36d46-4848-4fc6-ae27-89d599a2dd0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A) Importing libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ccaa9-eb02-450c-b334-06082712e02b",
   "metadata": {},
   "source": [
    "Changing the working directory to the top-level project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9aafa3-bbf1-4af1-9327-b44a161799b9",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "# This doesn't seem to work for me though\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f924cb-0348-4741-bf2e-308e4122c6d9",
   "metadata": {},
   "source": [
    "Importing our training functions from our own codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed565c-ee2e-4f63-bcc3-0e7b31f06489",
   "metadata": {},
   "source": [
    "Filling in the required parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b570ba-9289-4103-8dd7-b29e154b1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'Predicted_SSOC_2020',\n",
    "    'job_description': 'description',\n",
    "    'job_title': 'title'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'architecture': 'hierarchical',\n",
    "    'version': 'V2pt2',\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 5,\n",
    "    'training_batch_size': 32,\n",
    "    'validation_batch_size': 32,\n",
    "    'epochs': 19,\n",
    "    'learning_rate': 0.001,\n",
    "    'pretrained_tokenizer': 'C:\\\\Users\\\\shaun\\\\PycharmProjects\\\\ssoc-autocoder\\\\Models\\\\distilbert-tokenizer-pretrained-7epoch',\n",
    "    'pretrained_model': 'C:\\\\Users\\\\shaun\\\\PycharmProjects\\\\ssoc-autocoder\\\\Models\\\\mcf-pretrained-7epoch', #'distilbert-base-uncased',\n",
    "    'local_files_only': True,\n",
    "    'num_workers': 4,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923285ed-ac7a-48b3-a3c8-5c2689d8b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/Train/Train.csv')\n",
    "test = pd.read_csv('Data/Train/Test.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Reference/SSOC_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe45fd22-2599-44be-9fee-f6f2dbfa6cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCF_Job_Ad_ID</th>\n",
       "      <th>Predicted_SSOC_2020</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCF-2021-0042824</td>\n",
       "      <td>42241</td>\n",
       "      <td>Admin/Receptionist</td>\n",
       "      <td>Handling telephone calls and enquiries. Attend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MCF-2021-0142643</td>\n",
       "      <td>51421</td>\n",
       "      <td>Beautician Supervisor</td>\n",
       "      <td>Understand customer needs &amp; skin condition, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCF-2021-0182163</td>\n",
       "      <td>21494</td>\n",
       "      <td>Senior / Quantity Surveyor (C&amp;S/Tender/Project)</td>\n",
       "      <td>Responsible for payment/progress claims, varia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MCF-2021-0090664</td>\n",
       "      <td>12133</td>\n",
       "      <td>Compliance Manager [FinTech / Risk Management ...</td>\n",
       "      <td>Manage compliance risk strategies, policies an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MCF-2021-0159738</td>\n",
       "      <td>21422</td>\n",
       "      <td>Building and Construction Site Engineer</td>\n",
       "      <td>Supervise and coordinate the activities of sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>MCF-2021-0103410</td>\n",
       "      <td>12222</td>\n",
       "      <td>Communications and Marketing Manager</td>\n",
       "      <td>Work with the communications and marketing tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>MCF-2021-0175627</td>\n",
       "      <td>51422</td>\n",
       "      <td>Manicurist/Beauty Therapist</td>\n",
       "      <td>Providing manicure and pedicure services as we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>MCF-2021-0072528</td>\n",
       "      <td>71220</td>\n",
       "      <td>Floor / Wall Tiler</td>\n",
       "      <td>Cut tiles and shape them properly to ensure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>MCF-2021-0064583</td>\n",
       "      <td>51312</td>\n",
       "      <td>Service Crew</td>\n",
       "      <td>Job Descriptions. Serve customers with quality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>MCF-2020-0325701</td>\n",
       "      <td>24213</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>In charge of all necessary submission to relev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2890 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MCF_Job_Ad_ID  Predicted_SSOC_2020  \\\n",
       "0     MCF-2021-0042824                42241   \n",
       "1     MCF-2021-0142643                51421   \n",
       "2     MCF-2021-0182163                21494   \n",
       "3     MCF-2021-0090664                12133   \n",
       "4     MCF-2021-0159738                21422   \n",
       "...                ...                  ...   \n",
       "2885  MCF-2021-0103410                12222   \n",
       "2886  MCF-2021-0175627                51422   \n",
       "2887  MCF-2021-0072528                71220   \n",
       "2888  MCF-2021-0064583                51312   \n",
       "2889  MCF-2020-0325701                24213   \n",
       "\n",
       "                                                  title  \\\n",
       "0                                    Admin/Receptionist   \n",
       "1                                 Beautician Supervisor   \n",
       "2       Senior / Quantity Surveyor (C&S/Tender/Project)   \n",
       "3     Compliance Manager [FinTech / Risk Management ...   \n",
       "4               Building and Construction Site Engineer   \n",
       "...                                                 ...   \n",
       "2885               Communications and Marketing Manager   \n",
       "2886                        Manicurist/Beauty Therapist   \n",
       "2887                                 Floor / Wall Tiler   \n",
       "2888                                       Service Crew   \n",
       "2889                                    Project Manager   \n",
       "\n",
       "                                            description  \n",
       "0     Handling telephone calls and enquiries. Attend...  \n",
       "1     Understand customer needs & skin condition, an...  \n",
       "2     Responsible for payment/progress claims, varia...  \n",
       "3     Manage compliance risk strategies, policies an...  \n",
       "4     Supervise and coordinate the activities of sit...  \n",
       "...                                                 ...  \n",
       "2885  Work with the communications and marketing tea...  \n",
       "2886  Providing manicure and pedicure services as we...  \n",
       "2887  Cut tiles and shape them properly to ensure th...  \n",
       "2888  Job Descriptions. Serve customers with quality...  \n",
       "2889  In charge of all necessary submission to relev...  \n",
       "\n",
       "[2890 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff9935-6db1-48d9-a7d2-60bea9f62db2",
   "metadata": {},
   "source": [
    "#### B) Preparing the model and data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426bfc7-30ae-4808-a289-e0d942b418fd",
   "metadata": {},
   "source": [
    "Encoding the SSOCs into indices for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bab7935-629c-4e2f-bc69-06647216a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = model_training.generate_encoding(SSOC_2020)\n",
    "encoded_train = model_training.encode_dataset(train, encoding, colnames)\n",
    "encoded_test = model_training.encode_dataset(test, encoding, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d940d568-80a2-4705-a068-539a0162a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>SSOC</th>\n",
       "      <th>SSOC_1D</th>\n",
       "      <th>SSOC_2D</th>\n",
       "      <th>SSOC_3D</th>\n",
       "      <th>SSOC_4D</th>\n",
       "      <th>SSOC_5D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Food/Drink stall assistant</td>\n",
       "      <td>Food/Drink stall assistant assists in serving ...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>Kitchen Assistant (Coffee Shop)</td>\n",
       "      <td>Kitchen Assistant (Coffee Shop) Full Time / Pa...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>Hawker Assistant</td>\n",
       "      <td>Jiak Song Mee Hoon Kway is looking for an hour...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>Hawker Assistant</td>\n",
       "      <td>Assistant to head chef:Cutting of Vegetables.W...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>NUS New Canteen Fruit Juice Stall Assistant</td>\n",
       "      <td>New canteen, spacious and friendly working env...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "969                      Food/Drink stall assistant   \n",
       "1131                Kitchen Assistant (Coffee Shop)   \n",
       "4325                               Hawker Assistant   \n",
       "11668                              Hawker Assistant   \n",
       "12339  NUS New Canteen Fruit Juice Stall Assistant    \n",
       "\n",
       "                                                    Text   SSOC  SSOC_1D  \\\n",
       "969    Food/Drink stall assistant assists in serving ...  94102        8   \n",
       "1131   Kitchen Assistant (Coffee Shop) Full Time / Pa...  94102        8   \n",
       "4325   Jiak Song Mee Hoon Kway is looking for an hour...  94102        8   \n",
       "11668  Assistant to head chef:Cutting of Vegetables.W...  94102        8   \n",
       "12339  New canteen, spacious and friendly working env...  94102        8   \n",
       "\n",
       "       SSOC_2D  SSOC_3D  SSOC_4D  SSOC_5D  \n",
       "969         40      140      405      969  \n",
       "1131        40      140      405      969  \n",
       "4325        40      140      405      969  \n",
       "11668       40      140      405      969  \n",
       "12339       40      140      405      969  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[encoded_train['SSOC'] == 94102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f28757-19cb-45e2-9332-b989f5c60b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>SSOC</th>\n",
       "      <th>SSOC_1D</th>\n",
       "      <th>SSOC_2D</th>\n",
       "      <th>SSOC_3D</th>\n",
       "      <th>SSOC_4D</th>\n",
       "      <th>SSOC_5D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>Coffee Shop Assistant</td>\n",
       "      <td>Table cleaning and clearing plates to dishwash...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Title  \\\n",
       "1691  Coffee Shop Assistant   \n",
       "\n",
       "                                                   Text   SSOC  SSOC_1D  \\\n",
       "1691  Table cleaning and clearing plates to dishwash...  94102        8   \n",
       "\n",
       "      SSOC_2D  SSOC_3D  SSOC_4D  SSOC_5D  \n",
       "1691       40      140      405      969  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test[encoded_test['SSOC'] == 94102]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10d8b7-c411-45a2-81af-e5a4f9e0dbce",
   "metadata": {},
   "source": [
    "Loading the DistilBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a002f18f-dfa4-405b-8ddc-82e8d94e9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_tokenizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3a769-bb97-49c9-ab0f-48ef45ea35e0",
   "metadata": {},
   "source": [
    "Creating the `DataLoader` object for both the train and test sets, as well as initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e12320-2a9e-42ce-aa38-abaa34f8b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder\\Models\\mcf-pretrained-7epoch were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = model_training.prepare_data(encoded_train, encoded_test, tokenizer, colnames, parameters)\n",
    "model, loss_function, optimizer = model_training.prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb2af0b0-fa9f-4623-ab7f-a7d4b734f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 09 Jan 2022 - 21:51:28\n",
      "====================================================================\n",
      "> Epoch 1 started on: 09 Jan 2022 - 21:51:28\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 32.8190 \n",
      ">> Training Accuracy per 50 steps: 21.44%\n",
      ">> Batch of 50 took 3.78 mins\n",
      ">> Training Loss per 50 steps: 32.9418 \n",
      ">> Training Accuracy per 50 steps: 21.03%\n",
      ">> Batch of 50 took 3.70 mins\n",
      ">> Training Loss per 50 steps: 32.3827 \n",
      ">> Training Accuracy per 50 steps: 22.54%\n",
      ">> Batch of 50 took 3.70 mins\n",
      ">> Training Loss per 50 steps: 32.2306 \n",
      ">> Training Accuracy per 50 steps: 22.86%\n",
      ">> Batch of 50 took 3.70 mins\n",
      ">> Training Loss per 50 steps: 31.7108 \n",
      ">> Training Accuracy per 50 steps: 23.62%\n",
      ">> Batch of 50 took 3.70 mins\n",
      ">> Training Loss per 50 steps: 31.4859 \n",
      ">> Training Accuracy per 50 steps: 24.00%\n",
      ">> Batch of 50 took 3.70 mins\n",
      ">> Training Loss per 50 steps: 31.2732 \n",
      ">> Training Accuracy per 50 steps: 24.35%\n",
      ">> Batch of 50 took 3.68 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 1 Loss = \tTraining: 31.106  \tValidation: 25.525\n",
      "> Epoch 1 Accuracy = \tTraining: 24.75%  \tValidation: 36.96%\n",
      "> Epoch 1 took 35.70 mins\n",
      "====================================================================\n",
      "> Epoch 2 started on: 09 Jan 2022 - 22:27:10\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 28.4561 \n",
      ">> Training Accuracy per 50 steps: 27.81%\n",
      ">> Batch of 50 took 3.68 mins\n",
      ">> Training Loss per 50 steps: 28.3296 \n",
      ">> Training Accuracy per 50 steps: 26.69%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 28.1471 \n",
      ">> Training Accuracy per 50 steps: 27.08%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 28.1668 \n",
      ">> Training Accuracy per 50 steps: 27.55%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 28.3541 \n",
      ">> Training Accuracy per 50 steps: 27.55%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 28.4347 \n",
      ">> Training Accuracy per 50 steps: 27.35%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 28.3536 \n",
      ">> Training Accuracy per 50 steps: 27.57%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 2 Loss = \tTraining: 28.328  \tValidation: 23.682\n",
      "> Epoch 2 Accuracy = \tTraining: 27.88%  \tValidation: 40.48%\n",
      "> Epoch 2 took 34.35 mins\n",
      "====================================================================\n",
      "> Epoch 3 started on: 09 Jan 2022 - 23:01:31\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 27.7170 \n",
      ">> Training Accuracy per 50 steps: 27.62%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 26.9451 \n",
      ">> Training Accuracy per 50 steps: 28.94%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 26.6800 \n",
      ">> Training Accuracy per 50 steps: 29.48%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 26.9428 \n",
      ">> Training Accuracy per 50 steps: 29.80%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 26.9980 \n",
      ">> Training Accuracy per 50 steps: 29.68%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 26.8715 \n",
      ">> Training Accuracy per 50 steps: 30.03%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 26.7522 \n",
      ">> Training Accuracy per 50 steps: 30.39%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 3 Loss = \tTraining: 26.738  \tValidation: 23.227\n",
      "> Epoch 3 Accuracy = \tTraining: 30.60%  \tValidation: 43.67%\n",
      "> Epoch 3 took 34.32 mins\n",
      "====================================================================\n",
      "> Epoch 4 started on: 09 Jan 2022 - 23:35:50\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 25.0292 \n",
      ">> Training Accuracy per 50 steps: 31.81%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 24.1934 \n",
      ">> Training Accuracy per 50 steps: 33.12%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.6341 \n",
      ">> Training Accuracy per 50 steps: 32.67%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 25.1034 \n",
      ">> Training Accuracy per 50 steps: 32.53%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 25.2730 \n",
      ">> Training Accuracy per 50 steps: 32.36%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 25.3726 \n",
      ">> Training Accuracy per 50 steps: 32.28%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 25.3837 \n",
      ">> Training Accuracy per 50 steps: 32.04%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 4 Loss = \tTraining: 25.470  \tValidation: 22.601\n",
      "> Epoch 4 Accuracy = \tTraining: 32.08%  \tValidation: 44.60%\n",
      "> Epoch 4 took 34.31 mins\n",
      "====================================================================\n",
      "> Epoch 5 started on: 10 Jan 2022 - 00:10:09\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 24.8639 \n",
      ">> Training Accuracy per 50 steps: 34.38%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 24.3271 \n",
      ">> Training Accuracy per 50 steps: 34.66%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.8053 \n",
      ">> Training Accuracy per 50 steps: 34.23%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.3984 \n",
      ">> Training Accuracy per 50 steps: 34.17%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.4262 \n",
      ">> Training Accuracy per 50 steps: 33.85%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.2745 \n",
      ">> Training Accuracy per 50 steps: 33.81%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 24.3083 \n",
      ">> Training Accuracy per 50 steps: 33.63%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 5 Loss = \tTraining: 24.449  \tValidation: 22.490\n",
      "> Epoch 5 Accuracy = \tTraining: 33.68%  \tValidation: 45.43%\n",
      "> Epoch 5 took 34.34 mins\n",
      "====================================================================\n",
      "> Epoch 6 started on: 10 Jan 2022 - 00:44:29\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 23.8816 \n",
      ">> Training Accuracy per 50 steps: 33.25%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 23.2649 \n",
      ">> Training Accuracy per 50 steps: 34.44%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 23.8237 \n",
      ">> Training Accuracy per 50 steps: 33.40%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 24.0763 \n",
      ">> Training Accuracy per 50 steps: 33.47%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 23.9408 \n",
      ">> Training Accuracy per 50 steps: 33.11%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 23.9861 \n",
      ">> Training Accuracy per 50 steps: 33.43%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 23.9174 \n",
      ">> Training Accuracy per 50 steps: 33.84%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 6 Loss = \tTraining: 23.907  \tValidation: 22.046\n",
      "> Epoch 6 Accuracy = \tTraining: 33.88%  \tValidation: 49.03%\n",
      "> Epoch 6 took 34.38 mins\n",
      "====================================================================\n",
      "> Epoch 7 started on: 10 Jan 2022 - 01:18:52\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 21.9368 \n",
      ">> Training Accuracy per 50 steps: 35.06%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 22.3192 \n",
      ">> Training Accuracy per 50 steps: 35.91%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 22.4938 \n",
      ">> Training Accuracy per 50 steps: 35.62%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.5847 \n",
      ">> Training Accuracy per 50 steps: 35.58%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.7960 \n",
      ">> Training Accuracy per 50 steps: 35.24%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.7336 \n",
      ">> Training Accuracy per 50 steps: 35.54%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.7510 \n",
      ">> Training Accuracy per 50 steps: 35.16%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 7 Loss = \tTraining: 22.993  \tValidation: 21.978\n",
      "> Epoch 7 Accuracy = \tTraining: 35.01%  \tValidation: 47.72%\n",
      "> Epoch 7 took 34.38 mins\n",
      "====================================================================\n",
      "> Epoch 8 started on: 10 Jan 2022 - 01:53:15\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 23.2802 \n",
      ">> Training Accuracy per 50 steps: 35.00%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 22.6681 \n",
      ">> Training Accuracy per 50 steps: 35.03%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.4504 \n",
      ">> Training Accuracy per 50 steps: 34.83%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.4083 \n",
      ">> Training Accuracy per 50 steps: 35.41%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.5241 \n",
      ">> Training Accuracy per 50 steps: 35.04%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.5084 \n",
      ">> Training Accuracy per 50 steps: 35.21%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 22.6082 \n",
      ">> Training Accuracy per 50 steps: 35.21%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 8 Loss = \tTraining: 22.593  \tValidation: 22.257\n",
      "> Epoch 8 Accuracy = \tTraining: 35.31%  \tValidation: 48.79%\n",
      "> Epoch 8 took 34.38 mins\n",
      "====================================================================\n",
      "> Epoch 9 started on: 10 Jan 2022 - 02:27:38\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 22.3197 \n",
      ">> Training Accuracy per 50 steps: 36.12%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 21.6310 \n",
      ">> Training Accuracy per 50 steps: 35.81%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.6104 \n",
      ">> Training Accuracy per 50 steps: 35.54%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.7059 \n",
      ">> Training Accuracy per 50 steps: 35.30%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.7482 \n",
      ">> Training Accuracy per 50 steps: 35.36%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.8796 \n",
      ">> Training Accuracy per 50 steps: 35.56%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.8207 \n",
      ">> Training Accuracy per 50 steps: 35.83%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 9 Loss = \tTraining: 21.838  \tValidation: 21.623\n",
      "> Epoch 9 Accuracy = \tTraining: 35.51%  \tValidation: 49.58%\n",
      "> Epoch 9 took 34.39 mins\n",
      "====================================================================\n",
      "> Epoch 10 started on: 10 Jan 2022 - 03:02:01\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 20.8313 \n",
      ">> Training Accuracy per 50 steps: 37.50%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 21.2477 \n",
      ">> Training Accuracy per 50 steps: 37.81%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.0516 \n",
      ">> Training Accuracy per 50 steps: 37.79%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.0554 \n",
      ">> Training Accuracy per 50 steps: 37.67%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.0386 \n",
      ">> Training Accuracy per 50 steps: 37.34%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.2491 \n",
      ">> Training Accuracy per 50 steps: 36.98%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.3217 \n",
      ">> Training Accuracy per 50 steps: 36.96%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 10 Loss = \tTraining: 21.542  \tValidation: 21.474\n",
      "> Epoch 10 Accuracy = \tTraining: 36.84%  \tValidation: 50.21%\n",
      "> Epoch 10 took 34.42 mins\n",
      "====================================================================\n",
      "> Epoch 11 started on: 10 Jan 2022 - 03:36:27\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 21.0378 \n",
      ">> Training Accuracy per 50 steps: 36.31%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 21.1636 \n",
      ">> Training Accuracy per 50 steps: 37.75%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 21.1319 \n",
      ">> Training Accuracy per 50 steps: 36.94%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 20.9395 \n",
      ">> Training Accuracy per 50 steps: 37.12%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 21.0208 \n",
      ">> Training Accuracy per 50 steps: 36.96%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 21.0233 \n",
      ">> Training Accuracy per 50 steps: 36.88%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 21.1943 \n",
      ">> Training Accuracy per 50 steps: 36.90%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 11 Loss = \tTraining: 21.183  \tValidation: 21.337\n",
      "> Epoch 11 Accuracy = \tTraining: 36.78%  \tValidation: 51.11%\n",
      "> Epoch 11 took 34.36 mins\n",
      "====================================================================\n",
      "> Epoch 12 started on: 10 Jan 2022 - 04:10:48\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 18.9803 \n",
      ">> Training Accuracy per 50 steps: 39.44%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 20.2486 \n",
      ">> Training Accuracy per 50 steps: 37.19%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 20.0654 \n",
      ">> Training Accuracy per 50 steps: 37.54%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 20.1442 \n",
      ">> Training Accuracy per 50 steps: 37.75%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 20.3574 \n",
      ">> Training Accuracy per 50 steps: 37.80%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 20.4023 \n",
      ">> Training Accuracy per 50 steps: 37.41%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 20.3831 \n",
      ">> Training Accuracy per 50 steps: 37.65%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 12 Loss = \tTraining: 20.351  \tValidation: 21.730\n",
      "> Epoch 12 Accuracy = \tTraining: 37.57%  \tValidation: 47.89%\n",
      "> Epoch 12 took 34.32 mins\n",
      "====================================================================\n",
      "> Epoch 13 started on: 10 Jan 2022 - 04:45:08\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 20.5013 \n",
      ">> Training Accuracy per 50 steps: 35.62%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 19.6416 \n",
      ">> Training Accuracy per 50 steps: 37.00%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.8591 \n",
      ">> Training Accuracy per 50 steps: 36.62%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.7456 \n",
      ">> Training Accuracy per 50 steps: 37.14%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.5696 \n",
      ">> Training Accuracy per 50 steps: 37.12%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.8626 \n",
      ">> Training Accuracy per 50 steps: 37.25%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.9411 \n",
      ">> Training Accuracy per 50 steps: 37.09%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 13 Loss = \tTraining: 20.121  \tValidation: 21.289\n",
      "> Epoch 13 Accuracy = \tTraining: 36.94%  \tValidation: 51.00%\n",
      "> Epoch 13 took 34.33 mins\n",
      "====================================================================\n",
      "> Epoch 14 started on: 10 Jan 2022 - 05:19:27\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 18.5602 \n",
      ">> Training Accuracy per 50 steps: 39.69%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 19.0978 \n",
      ">> Training Accuracy per 50 steps: 38.72%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.4482 \n",
      ">> Training Accuracy per 50 steps: 38.00%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.3878 \n",
      ">> Training Accuracy per 50 steps: 38.25%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.4616 \n",
      ">> Training Accuracy per 50 steps: 38.12%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.7633 \n",
      ">> Training Accuracy per 50 steps: 37.57%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.8563 \n",
      ">> Training Accuracy per 50 steps: 37.38%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 14 Loss = \tTraining: 19.828  \tValidation: 21.269\n",
      "> Epoch 14 Accuracy = \tTraining: 37.55%  \tValidation: 50.69%\n",
      "> Epoch 14 took 34.36 mins\n",
      "====================================================================\n",
      "> Epoch 15 started on: 10 Jan 2022 - 05:53:48\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 18.4733 \n",
      ">> Training Accuracy per 50 steps: 37.75%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 19.4652 \n",
      ">> Training Accuracy per 50 steps: 37.59%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.8917 \n",
      ">> Training Accuracy per 50 steps: 37.79%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.9857 \n",
      ">> Training Accuracy per 50 steps: 37.67%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.0278 \n",
      ">> Training Accuracy per 50 steps: 37.71%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.1591 \n",
      ">> Training Accuracy per 50 steps: 37.96%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 19.3161 \n",
      ">> Training Accuracy per 50 steps: 38.03%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 15 Loss = \tTraining: 19.437  \tValidation: 20.939\n",
      "> Epoch 15 Accuracy = \tTraining: 38.02%  \tValidation: 52.01%\n",
      "> Epoch 15 took 34.36 mins\n",
      "====================================================================\n",
      "> Epoch 16 started on: 10 Jan 2022 - 06:28:10\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 19.0325 \n",
      ">> Training Accuracy per 50 steps: 36.50%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 19.2320 \n",
      ">> Training Accuracy per 50 steps: 36.41%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.0476 \n",
      ">> Training Accuracy per 50 steps: 36.83%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.9732 \n",
      ">> Training Accuracy per 50 steps: 37.67%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.9779 \n",
      ">> Training Accuracy per 50 steps: 38.16%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.2240 \n",
      ">> Training Accuracy per 50 steps: 38.17%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 19.2458 \n",
      ">> Training Accuracy per 50 steps: 38.18%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 16 Loss = \tTraining: 19.299  \tValidation: 21.281\n",
      "> Epoch 16 Accuracy = \tTraining: 38.04%  \tValidation: 50.93%\n",
      "> Epoch 16 took 34.40 mins\n",
      "====================================================================\n",
      "> Epoch 17 started on: 10 Jan 2022 - 07:02:34\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 18.2066 \n",
      ">> Training Accuracy per 50 steps: 38.19%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 18.5962 \n",
      ">> Training Accuracy per 50 steps: 38.88%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.4512 \n",
      ">> Training Accuracy per 50 steps: 38.77%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.5787 \n",
      ">> Training Accuracy per 50 steps: 38.97%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.9406 \n",
      ">> Training Accuracy per 50 steps: 38.36%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.7557 \n",
      ">> Training Accuracy per 50 steps: 38.43%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.7489 \n",
      ">> Training Accuracy per 50 steps: 38.53%\n",
      ">> Batch of 50 took 3.54 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 17 Loss = \tTraining: 18.884  \tValidation: 21.236\n",
      "> Epoch 17 Accuracy = \tTraining: 38.44%  \tValidation: 52.18%\n",
      "> Epoch 17 took 34.40 mins\n",
      "====================================================================\n",
      "> Epoch 18 started on: 10 Jan 2022 - 07:36:58\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 18.0712 \n",
      ">> Training Accuracy per 50 steps: 38.06%\n",
      ">> Batch of 50 took 3.66 mins\n",
      ">> Training Loss per 50 steps: 18.5701 \n",
      ">> Training Accuracy per 50 steps: 38.41%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 18.3983 \n",
      ">> Training Accuracy per 50 steps: 38.67%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.6197 \n",
      ">> Training Accuracy per 50 steps: 38.72%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.6678 \n",
      ">> Training Accuracy per 50 steps: 38.26%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.6405 \n",
      ">> Training Accuracy per 50 steps: 38.00%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.5091 \n",
      ">> Training Accuracy per 50 steps: 38.57%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 18 Loss = \tTraining: 18.510  \tValidation: 21.315\n",
      "> Epoch 18 Accuracy = \tTraining: 38.87%  \tValidation: 52.04%\n",
      "> Epoch 18 took 34.34 mins\n",
      "====================================================================\n",
      "> Epoch 19 started on: 10 Jan 2022 - 08:11:18\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 17.7610 \n",
      ">> Training Accuracy per 50 steps: 38.50%\n",
      ">> Batch of 50 took 3.65 mins\n",
      ">> Training Loss per 50 steps: 17.5649 \n",
      ">> Training Accuracy per 50 steps: 39.38%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 17.8678 \n",
      ">> Training Accuracy per 50 steps: 38.83%\n",
      ">> Batch of 50 took 3.54 mins\n",
      ">> Training Loss per 50 steps: 17.8723 \n",
      ">> Training Accuracy per 50 steps: 38.34%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 17.9147 \n",
      ">> Training Accuracy per 50 steps: 38.59%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.1458 \n",
      ">> Training Accuracy per 50 steps: 38.86%\n",
      ">> Batch of 50 took 3.53 mins\n",
      ">> Training Loss per 50 steps: 18.2243 \n",
      ">> Training Accuracy per 50 steps: 39.04%\n",
      ">> Batch of 50 took 3.53 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 19 Loss = \tTraining: 18.301  \tValidation: 21.843\n",
      "> Epoch 19 Accuracy = \tTraining: 38.89%  \tValidation: 51.28%\n",
      "> Epoch 19 took 34.33 mins\n",
      "====================================================================\n",
      "Training ended on: 10 Jan 2022 - 08:45:38\n",
      "Total training time: 10.90 hours\n"
     ]
    }
   ],
   "source": [
    "model_training.train_model(model, loss_function, optimizer, train_loader, test_loader, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed35c25-617b-4695-97b8-face2f5369c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/autocoder-v2pt2-9jan-pretrained7epoch-20epoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b0545-63f1-4599-ae1c-7587ab7b4736",
   "metadata": {},
   "source": [
    "report x-d accuracy for eval set  \n",
    "begin training with only the 1D loss function  \n",
    "error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5964004-be1b-4b62-a14c-8a7d17914c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c5c0c7-9482-4ffd-8f1b-f78f5f25b272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder\\Models\\mcf-pretrained-1epoch were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model1 = DistilBertModel.from_pretrained(, local_files_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a2773b2a-2343-4c24-81a4-19043ea0df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "testing\n"
     ]
    }
   ],
   "source": [
    "sourceFile = open('demo.txt', 'w')\n",
    "print('Hello, Python!', file = sourceFile)\n",
    "print('hello', file = sourceFile)\n",
    "print('testing', file = sourceFile)\n",
    "sourceFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63c4af-432f-456e-b7e1-dab5c4277889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd2538fa-11f1-403b-a2e3-0deb65813683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder\\Models\\mcf-pretrained-3epoch were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_test, loss_function, optimizer = model_training.prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "712ef002-1668-4962-8669-75d1427c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d17eb8-2d12-43c0-b441-eb4c0cda93dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.load_state_dict(torch.load('Models/autocoder-30dec-pretrained-60epoch.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8c862-fb52-4d0e-9f95-e29518754e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3167b7c-f782-4378-a8b9-437d26c48fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# benchmark: 5.88%, 2.59; 6.94%, 2.44\n",
    "\n",
    "torch.save(model.state_dict(), 'Models/autocoder-30dec-pretrained-60epoch.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bc750-2bca-4ba3-b128-69532f890721",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee3d1a2-4c6b-44a8-a7bf-8727ced070fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'Data/Processed/Training/train-aws/sm-model-5D-20epoch.pth'\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aa9311a9-f7af-424a-9bde-5cf2d467b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "tokenizer2 = DistilBertTokenizer.from_pretrained(\"distilbert-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0aafbd6e-d77a-4b4a-b4fd-59a9cde414ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distilbert-tokenizer\\\\tokenizer_config.json',\n",
       " 'distilbert-tokenizer\\\\special_tokens_map.json',\n",
       " 'distilbert-tokenizer\\\\vocab.txt',\n",
       " 'distilbert-tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"distilbert-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1f9a98e0-8fb7-4657-8e91-18c18d7145d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3029c44e-e8fc-40a1-b61c-527baecfceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSOC 2020: 53203\n",
      "Manage the day to day work of dental clinic. Chair-side assistance of the dentist. Sterilisation of Equipment. Answering patient enquiries. Making appointments. Clinic management. Requirements: Singaporeans only. Minimum 2 years experience in Dental Clinic. Relevant experience preferred. Check out Abercare sg for more detail. Interested candidate may send resume to career@abercare sg or call +65 6721 9231. Abercare sg | EA License Number 18C9070 | Germaine Er Si Ying | Registration Number R1875721.\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SSOC_1D': {'predicted_ssoc': ['5', '4'],\n",
       "  'predicted_proba': [0.9915245, 0.005478936],\n",
       "  'accurate_prediction': True},\n",
       " 'SSOC_2D': {'predicted_ssoc': ['53', '42', '32', '96', '22'],\n",
       "  'predicted_proba': [0.99386096,\n",
       "   0.005356934,\n",
       "   0.00033705527,\n",
       "   0.00032240854,\n",
       "   7.1901624e-05],\n",
       "  'accurate_prediction': True},\n",
       " 'SSOC_3D': {'predicted_ssoc': ['532', '422', '322', '222', '962'],\n",
       "  'predicted_proba': [0.97498643,\n",
       "   0.01901617,\n",
       "   0.0037244232,\n",
       "   0.0017405519,\n",
       "   0.00020084006],\n",
       "  'accurate_prediction': True},\n",
       " 'SSOC_4D': {'predicted_ssoc': ['5320', '4224', '3220', '2220', '3240'],\n",
       "  'predicted_proba': [0.9883781,\n",
       "   0.008069869,\n",
       "   0.0017323107,\n",
       "   0.0016664745,\n",
       "   7.554526e-05],\n",
       "  'accurate_prediction': True},\n",
       " 'SSOC_5D': {'predicted_ssoc': ['22200',\n",
       "   '83229',\n",
       "   '34341',\n",
       "   '36100',\n",
       "   '93334',\n",
       "   '83321',\n",
       "   '51312',\n",
       "   '51201',\n",
       "   '83329',\n",
       "   '14121'],\n",
       "  'predicted_proba': [0.071881905,\n",
       "   0.06389979,\n",
       "   0.049810156,\n",
       "   0.037628226,\n",
       "   0.036753647,\n",
       "   0.035747174,\n",
       "   0.032840487,\n",
       "   0.021936942,\n",
       "   0.015732056,\n",
       "   0.015412996],\n",
       "  'accurate_prediction': False}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxx = 26\n",
    "text = test['description'][idxx]\n",
    "test_target = str(test['Predicted_SSOC_2020'][idxx])\n",
    "print(f'SSOC 2020: {test_target}')\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "available_ssoc_codes = SSOC_2020['SSOC 2020']\n",
    "#pd.read_excel(\"Data/Processed/Training/train-aws/SSOC2020_Detailed_Definitions.xlsx\", skiprows = 4)[\"SSOC 2020\"]\n",
    "print('-----------------------------------------------')\n",
    "generate_single_prediction(model_test, tokenizer2, text, test_target, encoding, parameters, ssoc_prediction_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279f1f3-d29c-435d-b572-0c999f27e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = data[10000:]\n",
    "test_data = other_data[other_data['SSOC 2020'] == data[0:10000]['SSOC 2020'].sample().values[0]].sample()\n",
    "test_target = test_data['SSOC 2020'].values[0]\n",
    "text = test_data['Cleaned_Description'].values[0]\n",
    "print(test_data['SSOC 2020'].values[0])\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de208c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_others_SSOC(predicted_SSOC_with_proba, threshold, available_ssoc_codes):\n",
    "    out_prediction = []\n",
    "    out_probability = []\n",
    "    \n",
    "    for prediction, probability in predicted_SSOC_with_proba:\n",
    "        if probability < threshold and prediction[-1] != '9':\n",
    "            new_prediction = prediction[:-1] + '9'\n",
    "            if new_prediction in available_ssoc_codes.values:\n",
    "                print(f'Converting {prediction} to {new_prediction}')\n",
    "                out_prediction.append(new_prediction)\n",
    "            else:\n",
    "                out_prediction.append(prediction)\n",
    "        else:\n",
    "            out_prediction.append(prediction)\n",
    "        out_probability.append(probability)\n",
    "            \n",
    "    return zip(out_prediction, out_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6fc63d8-6ffb-4db8-a278-d95fc6b8335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test['Predicted_SSOC_2020'].values[0]) == np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24181e3-b371-4fa2-afd1-e666a8326ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6be948-9bde-4ba8-81b4-54f7f00d9809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f716e9-cba2-4ba8-a918-3c8b847648a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ed846f1-5e88-4c4c-af28-69f4ec950f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ssoc_prediction_parameters = {\n",
    "    'SSOC_1D': {'top_n': 2, 'min_prob': 0.5},\n",
    "    'SSOC_2D': {'top_n': 5, 'min_prob': 0.4},\n",
    "    'SSOC_3D': {'top_n': 5, 'min_prob': 0.3},\n",
    "    'SSOC_4D': {'top_n': 5, 'min_prob': 0.05},\n",
    "    'SSOC_5D': {'top_n': 10, 'min_prob': 0.05}\n",
    "}\n",
    "\n",
    "def generate_single_prediction(model, \n",
    "                               tokenizer, \n",
    "                               title,\n",
    "                               text, \n",
    "                               target, \n",
    "                               encoding,\n",
    "                               training_parameters,\n",
    "                               ssoc_prediction_parameters, \n",
    "                               failsafe = True):\n",
    "        \n",
    "    \"\"\"\n",
    "    Generates a single prediction from the trained neural network.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Check data type\n",
    "    if type(title) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'text' argument.\")\n",
    "    if type(text) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'text' argument.\")\n",
    "    if type(target) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'target' argument.\")\n",
    "\n",
    "    # Tokenize the text using the DistilBERT tokenizer\n",
    "    tokenized_title = tokenizer(\n",
    "        text = title,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = training_parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    tokenized_text = tokenizer(\n",
    "        text = text,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = training_parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    # Extract the tensors from the tokenizer\n",
    "    test_title_ids = torch.tensor([tokenized_title['input_ids']], dtype = torch.long)\n",
    "    test_title_mask = torch.tensor([tokenized_title['attention_mask']], dtype = torch.long)\n",
    "    test_text_ids = torch.tensor([tokenized_text['input_ids']], dtype = torch.long)\n",
    "    test_text_mask = torch.tensor([tokenized_text['attention_mask']], dtype = torch.long)\n",
    "    \n",
    "    # Set the model to evaluation mode and generate the predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_title_ids, test_title_mask, test_text_ids, test_text_mask)\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    # Iteratively generate predictions for each SSOC level that is specified\n",
    "    predictions_with_proba = {}\n",
    "    for ssoc_level, ssoc_level_params in sorted(ssoc_prediction_parameters.items()):\n",
    "        \n",
    "        # Extract the indices of the top n predicted SSOCs for the given SSOC level\n",
    "        predicted_idx = preds[ssoc_level].detach().numpy().argsort()[0][::-1][:ssoc_level_params[\"top_n\"]]\n",
    "        \n",
    "        # Extract the actual predicted probabilities from the softmax layer using the indices\n",
    "        predicted_proba_all = m(preds[ssoc_level]).detach().numpy()[0]\n",
    "        predicted_proba = [predicted_proba_all[idx] for idx in predicted_idx]\n",
    "        \n",
    "        # Convert the indices to the actual SSOC using the encoding dictionary\n",
    "        predicted_ssoc = [encoding[ssoc_level]['idx_ssoc'][idx] for idx in predicted_idx]\n",
    "        \n",
    "        # Check if the model made an accurate prediction\n",
    "        # Meaning whether the correct SSOC appeared in the list of predictions\n",
    "        accurate_prediction = False\n",
    "        for ssoc in predicted_ssoc:\n",
    "            if ssoc == target[0:len(ssoc)]:\n",
    "                accurate_prediction = True\n",
    "        \n",
    "        # Append predictions with the predicted probability to the output\n",
    "        predictions_with_proba[ssoc_level] = {\n",
    "            'predicted_ssoc': predicted_ssoc,\n",
    "            'predicted_proba': predicted_proba,\n",
    "            'accurate_prediction': accurate_prediction\n",
    "        }\n",
    "        \n",
    "    return predictions_with_proba\n",
    "\n",
    "def generate_predictions(model, \n",
    "                         tokenizer, \n",
    "                         test_set,\n",
    "                         encoding,\n",
    "                         training_parameters,\n",
    "                         ssoc_prediction_parameters,\n",
    "                         ssoc_level = 'SSOC_4D'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    output = []\n",
    "    accurate_predictions = []\n",
    "    for i, row in test_set.iterrows():\n",
    "        print(f'Generating prediction for {i+1}/{len(test_set)}...', end = '\\r')\n",
    "        predictions_with_proba = generate_single_prediction(model, \n",
    "                                                            tokenizer, \n",
    "                                                            row['title'],\n",
    "                                                            row['description'], \n",
    "                                                            str(row['Predicted_SSOC_2020']),\n",
    "                                                            encoding,\n",
    "                                                            training_parameters,\n",
    "                                                            ssoc_prediction_parameters)\n",
    "        output.append(predictions_with_proba)\n",
    "        accurate_predictions.append(predictions_with_proba[ssoc_level]['accurate_prediction'])\n",
    "    \n",
    "    print('')\n",
    "    accuracy = sum(accurate_predictions)/len(accurate_predictions)\n",
    "    print(f'Overall {ssoc_level} accuracy: {accuracy:.2%}')\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c3a5d34-e28c-4fdb-8e0d-d3ddbfd10904",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrsd_val = pd.read_csv('Data/Train/MRSD_Validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea823b2-16f8-4deb-8c5f-64c9efd736ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalSSOCClassifier_V2pt2(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ssoc_1d_stack): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       "  (ssoc_2d_stack): Sequential(\n",
       "    (0): Linear(in_features=1545, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=42, bias=True)\n",
       "  )\n",
       "  (ssoc_3d_stack): Sequential(\n",
       "    (0): Linear(in_features=1587, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=144, bias=True)\n",
       "  )\n",
       "  (ssoc_4d_stack): Sequential(\n",
       "    (0): Linear(in_features=1731, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=413, bias=True)\n",
       "  )\n",
       "  (ssoc_5d_stack): Sequential(\n",
       "    (0): Linear(in_features=2144, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=997, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40b8d3-1c2b-4498-ad28-baaefbace5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction for 447/492...\r"
     ]
    }
   ],
   "source": [
    "all_predictions = generate_predictions(model, tokenizer, mrsd_val, encoding, parameters, ssoc_prediction_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74f06ca5-6b9a-42b1-88bd-c59e59f191e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_predictions_v2pt2_34ep.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de5e20b0-7bdd-482d-a025-b674896444af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_predictions.pickle', 'rb') as handle:\n",
    "    all_ = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe10d373-9e99-4167-bed0-037b9ec3a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 94.22%\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "for prediction in all_predictions:\n",
    "    selected.append(prediction['SSOC_5D']['accurate_prediction'])\n",
    "accuracy = sum(selected)/len(selected)\n",
    "print(f'Overall accuracy: {accuracy:.2%}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fe194c3-4808-4463-ab0f-b890ddb0cad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions == all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c698ef5-c958-47f5-9275-05f969c39975",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Check validty of inputs\n",
    "    if (type(texts) != list) or any([type(item) != str for item in texts]):\n",
    "        raise AssertionError(\"Please pass in a list of strings into the 'texts' argument.\")\n",
    "    if (type(targets) != list) or any([type(item) != np.int64 for item in targets]):\n",
    "        raise AssertionError(\"Please pass in a list of integers (np.int64) into the 'targets' argument.\")    \n",
    "    if len(text) != len(targets):\n",
    "        raise AssertionError(\"Length of text descriptions do not match length of targets.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    predicted_1D_idx = preds[\"SSOC_1D\"].detach().numpy().argsort()[0][::-1][:top_n_threshold[\"SSOC_1D\"][\"n_digit\"]]\n",
    "    predicted_1D = [encoding['SSOC_1D']['idx_ssoc'][idx] for idx in predicted_1D_idx]\n",
    "    predicted_1D_proba_all = m(preds['SSOC_1D']).detach().numpy()[0]\n",
    "    predicted_1D_proba = [predicted_1D_proba_all[idx] for idx in predicted_1D_idx]\n",
    "    predicted_1D_with_proba = zip(predicted_1D, predicted_1D_proba)\n",
    "\n",
    "    predicted_2D_idx = preds[\"SSOC_2D\"].detach().numpy().argsort()[0][::-1][:top_n_threshold[\"SSOC_2D\"][\"n_digit\"]]\n",
    "    predicted_2D = [encoding['SSOC_2D']['idx_ssoc'][idx] for idx in predicted_2D_idx]\n",
    "    predicted_2D_proba_all = m(preds['SSOC_2D']).detach().numpy()[0]\n",
    "    predicted_2D_proba = [predicted_2D_proba_all[idx] for idx in predicted_2D_idx]\n",
    "    predicted_2D_with_proba = zip(predicted_2D, predicted_2D_proba)\n",
    "    \n",
    "    predicted_3D_idx = preds[\"SSOC_3D\"].detach().numpy().argsort()[0][::-1][:top_n_threshold[\"SSOC_3D\"][\"n_digit\"]]\n",
    "    predicted_3D = [encoding['SSOC_3D']['idx_ssoc'][idx] for idx in predicted_3D_idx]\n",
    "    predicted_3D_proba_all = m(preds['SSOC_3D']).detach().numpy()[0]\n",
    "    predicted_3D_proba = [predicted_3D_proba_all[idx] for idx in predicted_3D_idx]\n",
    "    predicted_3D_with_proba = zip(predicted_3D, predicted_3D_proba)\n",
    "    \n",
    "    predicted_4D_idx = preds[\"SSOC_4D\"].detach().numpy().argsort()[0][::-1][:top_n_threshold[\"SSOC_4D\"][\"n_digit\"]]\n",
    "    predicted_4D = [encoding['SSOC_4D']['idx_ssoc'][idx] for idx in predicted_4D_idx]\n",
    "    predicted_4D_proba_all = m(preds['SSOC_4D']).detach().numpy()[0]\n",
    "    predicted_4D_proba = [predicted_4D_proba_all[idx] for idx in predicted_4D_idx]\n",
    "    predicted_4D_with_proba = zip(predicted_4D, predicted_4D_proba)\n",
    "    \n",
    "    predicted_5D_idx = preds[\"SSOC_5D\"].detach().numpy().argsort()[0][::-1][:top_n_threshold[\"SSOC_5D\"][\"n_digit\"]]\n",
    "    predicted_5D = [encoding['SSOC_5D']['idx_ssoc'][idx] for idx in predicted_5D_idx]\n",
    "    predicted_5D_proba_all = m(preds['SSOC_5D']).detach().numpy()[0]\n",
    "    predicted_5D_proba = [predicted_5D_proba_all[idx] for idx in predicted_5D_idx]\n",
    "    predicted_5D_with_proba = zip(predicted_5D, predicted_5D_proba)\n",
    "    \n",
    "    print(f\"Target: {target}\")\n",
    "    verboseprint(f'Model top {top_n_threshold[\"SSOC_1D\"][\"n_digit\"]} predicted 1D:')\n",
    "    for predicted, prob in predicted_1D_with_proba:\n",
    "        print(f'{predicted}: {prob*100:.2f}%')\n",
    "        \n",
    "    if failsafe:\n",
    "        predicted_2D_with_proba = convert_others_SSOC(predicted_2D_with_proba, top_n_threshold[\"SSOC_2D\"][\"threshold\"], available_ssoc_codes)\n",
    "    \n",
    "    print(f'Model top {top_n_threshold[\"SSOC_2D\"][\"n_digit\"]} predicted 2D:')\n",
    "    for predicted, prob in predicted_2D_with_proba:\n",
    "        print(f'{predicted}: {prob*100:.2f}%')\n",
    "        \n",
    "    if failsafe:\n",
    "        predicted_3D_with_proba = convert_others_SSOC(predicted_3D_with_proba, top_n_threshold[\"SSOC_3D\"][\"threshold\"], available_ssoc_codes)\n",
    "        \n",
    "    print(f'Model top {top_n_threshold[\"SSOC_3D\"][\"n_digit\"]} predicted 3D:')\n",
    "    for predicted, prob in predicted_3D_with_proba:\n",
    "        print(f'{predicted}: {prob*100:.2f}%')\n",
    "    \n",
    "    if failsafe:\n",
    "        predicted_4D_with_proba = convert_others_SSOC(predicted_4D_with_proba, top_n_threshold[\"SSOC_4D\"][\"threshold\"], available_ssoc_codes)\n",
    "       \n",
    "    print(f'Model top {top_n_threshold[\"SSOC_4D\"][\"n_digit\"]} predicted 4D:')\n",
    "    for predicted, prob in predicted_4D_with_proba:\n",
    "        print(f'{predicted}: {prob*100:.2f}%')\n",
    "    \n",
    "    if failsafe:    \n",
    "        predicted_5D_with_proba = convert_others_SSOC(predicted_5D_with_proba, top_n_threshold[\"SSOC_5D\"][\"threshold\"], available_ssoc_codes)\n",
    "        \n",
    "    print(f'Model top {top_n_threshold[\"SSOC_5D\"][\"n_digit\"]} predicted 5D:')\n",
    "    for predicted, prob in predicted_5D_with_proba:\n",
    "        print(f'{predicted}: {prob*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b86904-1a0a-453a-a963-48402249b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"encoding.json\", 'w') as outfile:\n",
    "    json.dump(encoding, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "230ed5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'111' in available_ssoc_codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d8645c3-1ae1-48b1-9221-005085543ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSOC_listing = pd.read_excel(\"Data/Processed/Training/train-aws/SSOC2020_Detailed_Definitions.xlsx\", skiprows = 4)\n",
    "available_ssoc_codes = SSOC_listing[\"SSOC 2020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9456a17-f3fe-467f-b605-3d97642d7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Models/sm-model-10epoch.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322bac6-26ae-4970-99ac-13bc1e25b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "va_n_correct = 0\n",
    "va_loss = 0\n",
    "nb_va_steps = 0\n",
    "nb_va_examples = 0\n",
    "def calculate_accuracy(big_idx, targets):\n",
    "    n_correct = (big_idx == targets).sum().item()\n",
    "    return n_correct\n",
    "# Disable the calculation of gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(validation_loader):\n",
    "\n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "        mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "        # Run the forward prop\n",
    "        predictions = model(ids, mask)\n",
    "\n",
    "        # Iterate through each SSOC level\n",
    "        for ssoc_level, preds in predictions.items():\n",
    "\n",
    "            # Extract the correct target for the SSOC level\n",
    "            targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Compute the loss function using the predictions and the targets\n",
    "            level_loss = loss_function(preds, targets)\n",
    "\n",
    "            # Initialise the loss variable if this is the 1D level\n",
    "            # Else add to the loss variable\n",
    "            # Note the weights on each level\n",
    "            if ssoc_level == 'SSOC_1D':\n",
    "                loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "            else:\n",
    "                loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "        # Use the deepest level predictions to calculate accuracy\n",
    "        # Exploit the fact that the last preds object is the deepest level one\n",
    "        top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "        va_n_correct += calculate_accuracy(top_probs_idx, targets)\n",
    "\n",
    "        # Add this batch's loss to the overall training loss\n",
    "        va_loss += loss.item()\n",
    "\n",
    "        # Keep count for the batch steps and number of examples\n",
    "        nb_va_steps += 1\n",
    "        nb_va_examples += targets.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8ec74-57cc-4cc8-bf1d-60334dba4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_va_loss = va_loss / nb_va_steps\n",
    "epoch_va_accu = (va_n_correct * 100) / nb_va_examples\n",
    "print(f\"Validation: {epoch_va_loss:.3f}\")\n",
    "print(f\"Validation: {epoch_va_accu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6acc3-ee89-4b7c-9807-6b8a98ed199b",
   "metadata": {},
   "source": [
    "## Analysis of the underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72164f17-e3fe-40bd-ab56-291a3d3faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[colnames['SSOC']].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ce07d-a215-461d-9047-508baed4a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train.generate_encoding(SSOC_2020)\n",
    "encoded_data = train.encode_dataset(data, encoding, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05435c85-2390-4ae7-a2d0-5c6a4d5bb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_1D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4315-d6a4-4702-b6bb-985c42e57ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_2D'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa63fd6-f7a9-408e-8104-80edfa29cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_encoding(reference_data, ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Generates encoding for SSOC to indices, as required by PyTorch\n",
    "    for multi-class classification, for the training data\n",
    "\n",
    "    Args:\n",
    "        reference_data: Pandas dataframe containing all SSOCs\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the SSOC to index mapping (for preparing the\n",
    "        dataset) and index to SSOC mapping (for interpreting the predictions),\n",
    "        for each SSOC level from 1D to 5D.\n",
    "    '''\n",
    "\n",
    "    # Initialise the dictionary object to store the encodings for each level\n",
    "    encoding = {}\n",
    "\n",
    "    # Iterate through each level from 1 to 5\n",
    "    for level in range(1, 6):\n",
    "\n",
    "        # Initialise a dictionary object to store the respective-way encodings\n",
    "        ssoc_idx_mapping = {}\n",
    "\n",
    "        # Slice the SSOC column by the level required, drop duplicates, and sort\n",
    "        ssocs = list(np.sort(reference_data[ssoc_colname].astype('str').str.slice(0, level).unique()))\n",
    "\n",
    "        # Iterate through each unique SSOC (at i-digit level) and add to dict\n",
    "        for i, ssoc in enumerate(ssocs):\n",
    "            ssoc_idx_mapping[ssoc] = i\n",
    "\n",
    "        # Add each level's encodings to the output dictionary\n",
    "        encoding[f'SSOC_{level}D'] = {\n",
    "\n",
    "            # Store the SSOC to index encoding\n",
    "            'ssoc_idx': ssoc_idx_mapping,\n",
    "            # Store the index to SSOC encoding\n",
    "            'idx_ssoc': {v: k for k, v in ssoc_idx_mapping.items()}\n",
    "        }\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def encode_dataset(data,\n",
    "                   encoding,\n",
    "                   ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Uses the generated encoding to encode the SSOCs at each\n",
    "    digit level.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe of the training data with the correct SSOC\n",
    "        encoding: Encoding for each SSOC level\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with each digit SSOC encoded correctly\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    encoded_data = copy.deepcopy(data)[~data[ssoc_colname].str.contains('X')]\n",
    "\n",
    "    # For each digit, encode the SSOC correctly\n",
    "    for ssoc_level, encodings in encoding.items():\n",
    "        encoded_data[ssoc_level] = encoded_data[ssoc_colname].astype('str').str.slice(0, int(ssoc_level[5])).replace(encodings['ssoc_idx'])\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "# Create a new Python class to handle the additional complexity\n",
    "class SSOC_Dataset(Dataset):\n",
    "\n",
    "    # Define the class attributes\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Define the iterable over the Dataset object \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Extract the text\n",
    "        text = self.data[colnames['job_description']][index]\n",
    "\n",
    "        # Pass in the data into the tokenizer\n",
    "        inputs = self.tokenizer(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        # Extract the IDs and attention mask\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Return all the outputs needed for training and evaluation\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'SSOC_1D': torch.tensor(self.data.SSOC_1D[index], dtype = torch.long),\n",
    "            'SSOC_2D': torch.tensor(self.data.SSOC_2D[index], dtype = torch.long),\n",
    "            'SSOC_3D': torch.tensor(self.data.SSOC_3D[index], dtype = torch.long),\n",
    "            'SSOC_4D': torch.tensor(self.data.SSOC_4D[index], dtype = torch.long),\n",
    "            'SSOC_5D': torch.tensor(self.data.SSOC_5D[index], dtype = torch.long),\n",
    "        } \n",
    "\n",
    "    # Define the length attribute\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d9967-0376-4345-b512-539480a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(encoded_data,\n",
    "                 colnames,\n",
    "                 parameters):\n",
    "    \n",
    "    # Split the dataset into training and validation\n",
    "    training_data, validation_data = train_test_split(encoded_data,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 2021)\n",
    "    training_data.reset_index(drop = True, inplace = True)\n",
    "    validation_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "    \n",
    "    # Creating the dataset and dataloader for the neural network\n",
    "    training_loader = DataLoader(SSOC_Dataset(training_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                 batch_size = parameters['training_batch_size'],\n",
    "                                 num_workers = parameters['num_workers'],\n",
    "                                 shuffle = True,\n",
    "                                 persistent_workers=True)\n",
    "    validation_loader = DataLoader(SSOC_Dataset(validation_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                   batch_size = parameters['training_batch_size'],\n",
    "                                   num_workers = parameters['num_workers'],\n",
    "                                   shuffle = True,\n",
    "                                   persistent_workers=True)\n",
    "    \n",
    "    return training_loader, validation_loader, tokenizer\n",
    "\n",
    "class HierarchicalSSOCClassifier(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(HierarchicalSSOCClassifier, self).__init__()\n",
    "            \n",
    "            self.l1 = DistilBertModel.from_pretrained(parameters['pretrained_model'])\n",
    "\n",
    "            # Generating dimensions\n",
    "            SSOC_1D_count = len(encoding['SSOC_1D']['ssoc_idx'].keys())\n",
    "            SSOC_2D_count = len(encoding['SSOC_2D']['ssoc_idx'].keys())\n",
    "            SSOC_3D_count = len(encoding['SSOC_3D']['ssoc_idx'].keys())\n",
    "            SSOC_4D_count = len(encoding['SSOC_4D']['ssoc_idx'].keys())\n",
    "            SSOC_5D_count = len(encoding['SSOC_5D']['ssoc_idx'].keys())            \n",
    "            \n",
    "            # Stack 1: Predicting 1D SSOC (9)\n",
    "            if parameters['max_level'] >= 1:\n",
    "                self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(768, 768), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(768, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_1D_count)\n",
    "                )\n",
    "\n",
    "            # Stack 2: Predicting 2D SSOC (42)\n",
    "            if parameters['max_level'] >= 2:\n",
    "                n_dims_2d = 768 + SSOC_1D_count\n",
    "                self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_dims_2d, n_dims_2d), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(n_dims_2d, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_2D_count)\n",
    "                )        \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "\n",
    "            # Obtain the sentence embeddings from the DistilBERT model\n",
    "            embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_state = embeddings[0]\n",
    "            X = hidden_state[:, 0]\n",
    "\n",
    "            predictions = {}\n",
    "            \n",
    "            # 1D Prediction\n",
    "            if parameters['max_level'] >= 1:\n",
    "                predictions['SSOC_1D'] = self.ssoc_1d_stack(X)\n",
    "\n",
    "            # 2D Prediction\n",
    "            if parameters['max_level'] >= 2:\n",
    "                X = torch.cat((X, predictions['SSOC_1D']), dim = 1)\n",
    "                predictions['SSOC_2D'] = self.ssoc_2d_stack(X)\n",
    "\n",
    "            return {f'SSOC_{i}D': predictions[f'SSOC_{i}D'] for i in range(1, parameters['max_level'] + 1)}\n",
    "\n",
    "def prepare_model(encoding, parameters):\n",
    "        \n",
    "    model = HierarchicalSSOCClassifier()\n",
    "    model.to(parameters['device'])\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr = parameters['learning_rate'])\n",
    "    \n",
    "    return model, loss_function, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd14b71-6820-4f26-a338-04d2590df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training started on:\", current_time)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Set the NN to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over each batch\n",
    "        for batch, data in enumerate(training_loader):\n",
    "\n",
    "            # Extract the data\n",
    "            ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "            mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Run the forward prop\n",
    "            predictions = model(ids, mask)\n",
    "\n",
    "            # Iterate through each SSOC level\n",
    "            for ssoc_level, preds in predictions.items():\n",
    "\n",
    "                # Extract the correct target for the SSOC level\n",
    "                targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "                # Compute the loss function using the predictions and the targets\n",
    "                level_loss = loss_function(preds, targets)\n",
    "\n",
    "                # Initialise the loss variable if this is the 1D level\n",
    "                # Else add to the loss variable\n",
    "                # Note the weights on each level\n",
    "                if ssoc_level == 'SSOC_1D':\n",
    "                    loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "                else:\n",
    "                    loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "            # Use the deepest level predictions to calculate accuracy\n",
    "            top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "            n_correct += calculate_accu(top_probs_idx, targets)\n",
    "\n",
    "            # Calculate the loss\n",
    "    #         targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "    #         targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "    #         loss1 = loss_function(preds_1d, targets_1d)\n",
    "    #         loss2 = loss_function(preds_2d, targets_2d)\n",
    "    #         loss = loss1*5 + loss2\n",
    "\n",
    "            # Add this batch's loss to the overall training loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 500 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples \n",
    "                print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "                print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "                print(f\"Batch of 500 took {(time.time() - batch_start_time)/60:.2f} mins\")\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "        print(f\"Epoch training time: {(time.time() - epoch_start_time)/60:.2f} mins\")\n",
    "\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} mins\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training ended on:\", current_time)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f8869-10b0-4ec8-9a37-977c4882278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 4,\n",
    "    'validation_batch_size': 2,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 1e-05,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 0,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0da49c-bf7e-4e30-827b-11da01be9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')\n",
    "encoding = generate_encoding(SSOC_2020)\n",
    "encoded_data = encode_dataset(data[0:10000], encoding)\n",
    "training_loader, validation_loader = prepare_data(encoded_data, colnames, parameters)\n",
    "model, loss_function, optimizer = prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d4b13-379e-4a1a-8f57-6342b26063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa083cd5-ccad-457b-9ebe-d95ad39b6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "10000*.8/4/50*45/3600*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb40d7-95ea-4ca4-bc1c-2334cd9a953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, loss_function, optimizer, parameters['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a796d-c742-49c5-9057-5fe0fdc7bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ddf71-b7a8-4291-aa38-0d1b489eb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6e0dc-4f0f-42e6-a568-45b55d2278b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data[other_data['Cleaned_Description'] == 'Duties and Responsibilities: Implementation of Sage 300 ERP (Financials, Distribution, Project) Providing Pre & Post-Sales Consulting. Perform Business requirement analysis and provide professional advises. Install, Implement, Train and Support users on Sage 300 ERP Software. On-site and Back-end support on ERP Software. Diploma or Degree in Accountancy/Business Admin/Computer Science, Information Systems. Good Knowledge in MSSQL Server, MS Excel, Crystal Report and Visual Basic. Good analytical and problem-solving skills are essential. Good interpersonal and communication skills. Must have Sage 300 ERP Software. At least 4 - 5 years of working experience in relevant field. Must be able to work in Singapore and travel to other country.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41208371-6984-4640-adb4-84aaa54e6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479bfaa7-55a6-4d51-b9eb-a485a4b629fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602728de-d8bc-4a4e-9eb1-0a09731df744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'Models/autocoder-v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47315b-d1c4-4352-9c93-1e17afa470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = HierarchicalSSOCClassifier()\n",
    "model1.load_state_dict(torch.load('Models/autocoder-v1.pt'))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939c615-0f03-4e83-8481-9ef4fd819d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\n",
    "    text = text,\n",
    "    text_pair = None,\n",
    "    add_special_tokens = True,\n",
    "    max_length = parameters['sequence_max_length'],\n",
    "    pad_to_max_length = True,\n",
    "    return_token_type_ids = True,\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3dd47-526d-4c11-a294-ffedab5a056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = torch.tensor([tokenized['input_ids']], dtype = torch.long)\n",
    "test_mask = torch.tensor([tokenized['attention_mask']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf396e12-b574-484c-b190-9b744790d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_ids, test_mask)\n",
    "targets = torch.tensor([encoding['SSOC_1D']['ssoc_idx']['2']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ad2d7-f060-4bf2-85d7-b0647fd085e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(preds[\"SSOC_1D\"], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dce3d3-1e01-4c4e-8f1b-aa7f0b9c1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['SSOC_1D']['idx_ssoc'][np.argmax(preds[\"SSOC_1D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0b4ea-6595-4604-85f0-c842dc16f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['SSOC_2D']['idx_ssoc'][np.argmax(preds[\"SSOC_2D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340a569-14e6-4610-879c-b69c78503a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "m(preds['SSOC_2D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39977202-1d8b-4511-b0de-dba63e556810",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Assist with installation, configuration and set-up of new IT accounts & IT equipment for new users. Liaising with vendors for procurement, logistic and maintenance of IT equipment. Managing & troubleshooting of office IT equipment & systems. Analyze, monitor and resolve application and system failures and provide operational support. Perform, review and enhance business and IT systems & processes for enhanced improvement for the company.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fc5b-365c-426b-ae4a-4ec7456edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06838cc-934b-48f7-8154-0726ffef7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.data.Description[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets_1d': torch.tensor(self.data.SSOC_1D[index], dtype=torch.long),\n",
    "            'targets_2d': torch.tensor(self.data.SSOC_2D[index], dtype=torch.long),\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f0ba-0af3-4c04-8867-b4a08f086106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "training_set = Triage(train, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef8b3b-d1d4-4dda-b174-835feec76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ca96-b200-45ba-a7ab-13c2cf8e2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Stack 1: Predicting 1D SSOC (9)\n",
    "        self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "        # Stack 2: Predicting 2D SSOC (40 + 2 nec)\n",
    "        self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(777, 777), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(777, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 42)\n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Obtain the sentence embeddings from the DistilBERT model\n",
    "        embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = embeddings[0]\n",
    "        X = hidden_state[:, 0]\n",
    "        \n",
    "        # 1D Prediction\n",
    "        preds_1d = self.ssoc_1d_stack(X)\n",
    "        \n",
    "        # 2D Prediction\n",
    "        X = torch.cat((X, preds_1d), dim = 1)\n",
    "        preds_2d = self.ssoc_2d_stack(X)\n",
    "        \n",
    "        return preds_1d, preds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec19da7-4ecb-4eef-92ff-3bba58243d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb5c-f2ed-4635-8d82-c5b125ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss_fn\n",
    "# think of how to adjust the crossentropyloss function\n",
    "# change the targets upfront before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cd92-b91b-45ca-b4e2-fd83532500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ssoc(predicted, actual):\n",
    "    base_penalty = 10\n",
    "    penalty = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] != actual[i]:\n",
    "            penalty += base_penalty/(i+1)\n",
    "    return penalty\n",
    "\n",
    "def custom_loss_fn(top_probs_idx, targets, ssoc_level):\n",
    "          \n",
    "    if ssoc_level == '1d':\n",
    "          mapping = idx_ssoc1d\n",
    "    elif ssoc_level == '2d':\n",
    "          mapping = idx_ssoc2d\n",
    "          \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(top_probs_idx)):\n",
    "        predicted_ssoc = mapping[top_probs_idx[i].item()]\n",
    "        actual_ssoc = mapping[targets[i].item()]\n",
    "        loss += compare_ssoc(predicted_ssoc, actual_ssoc)\n",
    "        \n",
    "    return Variable(torch.tensor(float(loss)), requires_grad = True)\n",
    "\n",
    "# need to use Torch variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb2b8-516b-4e26-ab24-741e51189f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1 = Variable(torch.tensor([float(5), float(15)]), requires_grad = True)\n",
    "print(testing1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27724a-6699-4599-9ff3-47d6cc53f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40566ccf-4440-4b93-a379-7203ac10dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.tensor(float(1)), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badb587-3f24-4fc6-b4db-0df4f210129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18bd4c-cb70-44bc-b997-01838fbbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61414221-77ae-40e8-a916-b5161c227dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    # Set the NN to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(training_loader):\n",
    "        \n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "        targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Run the forward prop\n",
    "        preds_1d, preds_2d = model(ids, mask)\n",
    "        \n",
    "        # Find the indices of the top prediction\n",
    "        top_probs_1d, top_probs_idx_1d = torch.max(preds_1d.data, dim = 1)\n",
    "        top_probs_2d, top_probs_idx_2d = torch.max(preds_2d.data, dim = 1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss1 = loss_function(preds_1d, targets_1d)\n",
    "        loss2 = loss_function(preds_2d, targets_2d)\n",
    "        loss = loss1*5 + loss2\n",
    "        #print(f'Overall loss: {loss} = {loss1} + {loss2}')\n",
    "\n",
    "        # Deprecated\n",
    "        #loss = loss_function(preds_1d, targets_1d) + loss_function(preds_2d, targets_2d)\n",
    "        \n",
    "        # Add this batch's loss to the overall training loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        n_correct += calcuate_accu(top_probs_idx_2d, targets_2d)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets_2d.size(0)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 50 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 50 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822fb71-153f-4f25-8ed0-5bfc38b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70808b3-f08f-4df4-a07e-4274823285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1dd6-c7b5-4518-acc6-d16c7ef7910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d0c-a5b1-45e3-981f-df1f0f1c0c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf138-632a-4705-ad11-70e59c904cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
