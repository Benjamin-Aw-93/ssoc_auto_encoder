{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical/Straight through Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148bc89-7942-4bf5-a9f0-8d288f7bea0b",
   "metadata": {},
   "source": [
    "**Author:** Shaun Khoo  \n",
    "**Date:** 1 Oct 2021  \n",
    "**Context:** Adapting Shopify's approach to classifying products using a hierarchical classifier (see reference below)  \n",
    "**Objective:** Develop code for training a hierarchical classifier neural network\n",
    "\n",
    "Some references:\n",
    "\n",
    "* [this article by Shopify](https://shopify.engineering/introducing-linnet-using-rich-image-text-data-categorize-products)\n",
    "* [How to do transfer learning on PyTorch / Transformers](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b36d46-4848-4fc6-ae27-89d599a2dd0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A) Importing libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ccaa9-eb02-450c-b334-06082712e02b",
   "metadata": {},
   "source": [
    "Changing the working directory to the top-level project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9aafa3-bbf1-4af1-9327-b44a161799b9",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "# This doesn't seem to work for me though\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f924cb-0348-4741-bf2e-308e4122c6d9",
   "metadata": {},
   "source": [
    "Importing our training functions from our own codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed565c-ee2e-4f63-bcc3-0e7b31f06489",
   "metadata": {},
   "source": [
    "Filling in the required parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34b570ba-9289-4103-8dd7-b29e154b1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'Predicted_SSOC_2020',\n",
    "    'job_description': 'description',\n",
    "    'job_title': 'title'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'architecture': 'hierarchical',\n",
    "    'version': 'V2pt2',\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 5,\n",
    "    'training_batch_size': 32,\n",
    "    'validation_batch_size': 32,\n",
    "    'epochs': 4,\n",
    "    'learning_rate': 0.0005,\n",
    "    'pretrained_tokenizer': 'C:\\\\Users\\\\shaun\\\\PycharmProjects\\\\ssoc-autocoder\\\\Models\\\\distilbert-tokenizer-pretrained-7epoch',\n",
    "    'pretrained_model': 'C:\\\\Users\\\\shaun\\\\PycharmProjects\\\\ssoc-autocoder\\\\Models\\\\mcf-pretrained-7epoch', #'distilbert-base-uncased',\n",
    "    'local_files_only': True,\n",
    "    'num_workers': 4,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923285ed-ac7a-48b3-a3c8-5c2689d8b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/Train/Train.csv')\n",
    "test = pd.read_csv('Data/Train/Test.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Reference/SSOC_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff9935-6db1-48d9-a7d2-60bea9f62db2",
   "metadata": {},
   "source": [
    "#### B) Preparing the model and data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426bfc7-30ae-4808-a289-e0d942b418fd",
   "metadata": {},
   "source": [
    "Encoding the SSOCs into indices for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bab7935-629c-4e2f-bc69-06647216a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = model_training.generate_encoding(SSOC_2020)\n",
    "encoded_train = model_training.encode_dataset(train, encoding, colnames)\n",
    "encoded_test = model_training.encode_dataset(test, encoding, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d940d568-80a2-4705-a068-539a0162a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>SSOC</th>\n",
       "      <th>SSOC_1D</th>\n",
       "      <th>SSOC_2D</th>\n",
       "      <th>SSOC_3D</th>\n",
       "      <th>SSOC_4D</th>\n",
       "      <th>SSOC_5D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Food/Drink stall assistant</td>\n",
       "      <td>Food/Drink stall assistant assists in serving ...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>Kitchen Assistant (Coffee Shop)</td>\n",
       "      <td>Kitchen Assistant (Coffee Shop) Full Time / Pa...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>Hawker Assistant</td>\n",
       "      <td>Jiak Song Mee Hoon Kway is looking for an hour...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>Hawker Assistant</td>\n",
       "      <td>Assistant to head chef:Cutting of Vegetables.W...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>NUS New Canteen Fruit Juice Stall Assistant</td>\n",
       "      <td>New canteen, spacious and friendly working env...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "969                      Food/Drink stall assistant   \n",
       "1131                Kitchen Assistant (Coffee Shop)   \n",
       "4325                               Hawker Assistant   \n",
       "11668                              Hawker Assistant   \n",
       "12339  NUS New Canteen Fruit Juice Stall Assistant    \n",
       "\n",
       "                                                    Text   SSOC  SSOC_1D  \\\n",
       "969    Food/Drink stall assistant assists in serving ...  94102        8   \n",
       "1131   Kitchen Assistant (Coffee Shop) Full Time / Pa...  94102        8   \n",
       "4325   Jiak Song Mee Hoon Kway is looking for an hour...  94102        8   \n",
       "11668  Assistant to head chef:Cutting of Vegetables.W...  94102        8   \n",
       "12339  New canteen, spacious and friendly working env...  94102        8   \n",
       "\n",
       "       SSOC_2D  SSOC_3D  SSOC_4D  SSOC_5D  \n",
       "969         40      140      405      969  \n",
       "1131        40      140      405      969  \n",
       "4325        40      140      405      969  \n",
       "11668       40      140      405      969  \n",
       "12339       40      140      405      969  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[encoded_train['SSOC'] == 94102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f28757-19cb-45e2-9332-b989f5c60b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>SSOC</th>\n",
       "      <th>SSOC_1D</th>\n",
       "      <th>SSOC_2D</th>\n",
       "      <th>SSOC_3D</th>\n",
       "      <th>SSOC_4D</th>\n",
       "      <th>SSOC_5D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>Coffee Shop Assistant</td>\n",
       "      <td>Table cleaning and clearing plates to dishwash...</td>\n",
       "      <td>94102</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>405</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Title  \\\n",
       "1691  Coffee Shop Assistant   \n",
       "\n",
       "                                                   Text   SSOC  SSOC_1D  \\\n",
       "1691  Table cleaning and clearing plates to dishwash...  94102        8   \n",
       "\n",
       "      SSOC_2D  SSOC_3D  SSOC_4D  SSOC_5D  \n",
       "1691       40      140      405      969  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test[encoded_test['SSOC'] == 94102]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10d8b7-c411-45a2-81af-e5a4f9e0dbce",
   "metadata": {},
   "source": [
    "Loading the DistilBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a002f18f-dfa4-405b-8ddc-82e8d94e9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_tokenizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3a769-bb97-49c9-ab0f-48ef45ea35e0",
   "metadata": {},
   "source": [
    "Creating the `DataLoader` object for both the train and test sets, as well as initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e12320-2a9e-42ce-aa38-abaa34f8b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder\\Models\\mcf-pretrained-7epoch were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = model_training.prepare_data(encoded_train, encoded_test, tokenizer, colnames, parameters)\n",
    "model, loss_function, optimizer = model_training.prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6207660-deb5-4e2f-af59-cc8b62d30906",
   "metadata": {},
   "source": [
    "#### C) Running the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb2af0b0-fa9f-4623-ab7f-a7d4b734f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 11 Jan 2022 - 08:53:57\n",
      "====================================================================\n",
      "> Epoch 1 started on: 11 Jan 2022 - 08:53:57\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 15.6882 \n",
      ">> Training Accuracy per 50 steps: 36.94%\n",
      ">> Batch of 50 took 3.37 mins\n",
      ">> Training Loss per 50 steps: 16.1021 \n",
      ">> Training Accuracy per 50 steps: 36.84%\n",
      ">> Batch of 50 took 3.27 mins\n",
      ">> Training Loss per 50 steps: 15.9761 \n",
      ">> Training Accuracy per 50 steps: 37.17%\n",
      ">> Batch of 50 took 3.23 mins\n",
      ">> Training Loss per 50 steps: 15.9697 \n",
      ">> Training Accuracy per 50 steps: 36.81%\n",
      ">> Batch of 50 took 3.23 mins\n",
      ">> Training Loss per 50 steps: 15.9663 \n",
      ">> Training Accuracy per 50 steps: 36.65%\n",
      ">> Batch of 50 took 3.23 mins\n",
      ">> Training Loss per 50 steps: 16.0164 \n",
      ">> Training Accuracy per 50 steps: 36.58%\n",
      ">> Batch of 50 took 3.23 mins\n",
      ">> Training Loss per 50 steps: 16.1063 \n",
      ">> Training Accuracy per 50 steps: 36.41%\n",
      ">> Batch of 50 took 3.22 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 1 Loss = \tTraining: 16.070  \tValidation: 22.133\n",
      "> Epoch 1 Accuracy = \tTraining: 36.31%  \tValidation: 48.24%\n",
      "> Epoch 1 took 31.43 mins\n",
      "====================================================================\n",
      "> Epoch 2 started on: 11 Jan 2022 - 09:25:23\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 15.0996 \n",
      ">> Training Accuracy per 50 steps: 36.56%\n",
      ">> Batch of 50 took 3.36 mins\n",
      ">> Training Loss per 50 steps: 15.0719 \n",
      ">> Training Accuracy per 50 steps: 37.09%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.4416 \n",
      ">> Training Accuracy per 50 steps: 36.90%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.7053 \n",
      ">> Training Accuracy per 50 steps: 36.50%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.7979 \n",
      ">> Training Accuracy per 50 steps: 36.31%\n",
      ">> Batch of 50 took 3.22 mins\n",
      ">> Training Loss per 50 steps: 15.7912 \n",
      ">> Training Accuracy per 50 steps: 36.83%\n",
      ">> Batch of 50 took 3.22 mins\n",
      ">> Training Loss per 50 steps: 15.8613 \n",
      ">> Training Accuracy per 50 steps: 37.02%\n",
      ">> Batch of 50 took 3.22 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 2 Loss = \tTraining: 15.852  \tValidation: 21.900\n",
      "> Epoch 2 Accuracy = \tTraining: 37.16%  \tValidation: 49.03%\n",
      "> Epoch 2 took 31.29 mins\n",
      "====================================================================\n",
      "> Epoch 3 started on: 11 Jan 2022 - 09:56:40\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 15.6195 \n",
      ">> Training Accuracy per 50 steps: 36.25%\n",
      ">> Batch of 50 took 3.36 mins\n",
      ">> Training Loss per 50 steps: 15.6878 \n",
      ">> Training Accuracy per 50 steps: 36.62%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.7037 \n",
      ">> Training Accuracy per 50 steps: 37.29%\n",
      ">> Batch of 50 took 3.20 mins\n",
      ">> Training Loss per 50 steps: 15.5788 \n",
      ">> Training Accuracy per 50 steps: 37.55%\n",
      ">> Batch of 50 took 3.20 mins\n",
      ">> Training Loss per 50 steps: 15.6755 \n",
      ">> Training Accuracy per 50 steps: 37.60%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.7038 \n",
      ">> Training Accuracy per 50 steps: 37.36%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.8430 \n",
      ">> Training Accuracy per 50 steps: 37.20%\n",
      ">> Batch of 50 took 3.21 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 3 Loss = \tTraining: 15.878  \tValidation: 22.272\n",
      "> Epoch 3 Accuracy = \tTraining: 37.11%  \tValidation: 51.87%\n",
      "> Epoch 3 took 31.24 mins\n",
      "====================================================================\n",
      "> Epoch 4 started on: 11 Jan 2022 - 10:27:55\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 50 steps: 14.5898 \n",
      ">> Training Accuracy per 50 steps: 39.25%\n",
      ">> Batch of 50 took 3.37 mins\n",
      ">> Training Loss per 50 steps: 15.0142 \n",
      ">> Training Accuracy per 50 steps: 38.56%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.0047 \n",
      ">> Training Accuracy per 50 steps: 38.75%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 14.9952 \n",
      ">> Training Accuracy per 50 steps: 37.78%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.0640 \n",
      ">> Training Accuracy per 50 steps: 37.95%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.2293 \n",
      ">> Training Accuracy per 50 steps: 37.76%\n",
      ">> Batch of 50 took 3.21 mins\n",
      ">> Training Loss per 50 steps: 15.4172 \n",
      ">> Training Accuracy per 50 steps: 37.67%\n",
      ">> Batch of 50 took 3.21 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 4 Loss = \tTraining: 15.658  \tValidation: 22.148\n",
      "> Epoch 4 Accuracy = \tTraining: 37.45%  \tValidation: 51.73%\n",
      "> Epoch 4 took 31.28 mins\n",
      "====================================================================\n",
      "Training ended on: 11 Jan 2022 - 10:59:11\n",
      "Total training time: 2.09 hours\n"
     ]
    }
   ],
   "source": [
    "model_training.train_model(model, loss_function, optimizer, train_loader, test_loader, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed35c25-617b-4695-97b8-face2f5369c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/autocoder-v2pt2-9jan-pretrained7epoch-34epoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b0545-63f1-4599-ae1c-7587ab7b4736",
   "metadata": {},
   "source": [
    "report x-d accuracy for eval set  \n",
    "begin training with only the 1D loss function  \n",
    "error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2784-4473-49fe-8cb5-78692b3c8b1f",
   "metadata": {},
   "source": [
    "#### D) Generating predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed846f1-5e88-4c4c-af28-69f4ec950f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ssoc_prediction_parameters = {\n",
    "    'SSOC_1D': {'top_n': 2, 'min_prob': 0.5},\n",
    "    'SSOC_2D': {'top_n': 5, 'min_prob': 0.4},\n",
    "    'SSOC_3D': {'top_n': 5, 'min_prob': 0.3},\n",
    "    'SSOC_4D': {'top_n': 5, 'min_prob': 0.05},\n",
    "    'SSOC_5D': {'top_n': 10, 'min_prob': 0.05}\n",
    "}\n",
    "\n",
    "def generate_single_prediction(model, \n",
    "                               tokenizer, \n",
    "                               title,\n",
    "                               text, \n",
    "                               target, \n",
    "                               encoding,\n",
    "                               training_parameters,\n",
    "                               ssoc_prediction_parameters, \n",
    "                               failsafe = True):\n",
    "        \n",
    "    \"\"\"\n",
    "    Generates a single prediction from the trained neural network.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Check data type\n",
    "    if type(title) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'text' argument.\")\n",
    "    if type(text) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'text' argument.\")\n",
    "    if type(target) != str:\n",
    "        raise TypeError(\"Please enter a string for the 'target' argument.\")\n",
    "\n",
    "    # Tokenize the text using the DistilBERT tokenizer\n",
    "    tokenized_title = tokenizer(\n",
    "        text = title,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = training_parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    tokenized_text = tokenizer(\n",
    "        text = text,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = training_parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    # Extract the tensors from the tokenizer\n",
    "    test_title_ids = torch.tensor([tokenized_title['input_ids']], dtype = torch.long)\n",
    "    test_title_mask = torch.tensor([tokenized_title['attention_mask']], dtype = torch.long)\n",
    "    test_text_ids = torch.tensor([tokenized_text['input_ids']], dtype = torch.long)\n",
    "    test_text_mask = torch.tensor([tokenized_text['attention_mask']], dtype = torch.long)\n",
    "    \n",
    "    # Set the model to evaluation mode and generate the predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_title_ids, test_title_mask, test_text_ids, test_text_mask)\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    # Iteratively generate predictions for each SSOC level that is specified\n",
    "    predictions_with_proba = {}\n",
    "    for ssoc_level, ssoc_level_params in sorted(ssoc_prediction_parameters.items()):\n",
    "        \n",
    "        # Extract the indices of the top n predicted SSOCs for the given SSOC level\n",
    "        predicted_idx = preds[ssoc_level].detach().numpy().argsort()[0][::-1][:ssoc_level_params[\"top_n\"]]\n",
    "        \n",
    "        # Extract the actual predicted probabilities from the softmax layer using the indices\n",
    "        predicted_proba_all = m(preds[ssoc_level]).detach().numpy()[0]\n",
    "        predicted_proba = [predicted_proba_all[idx] for idx in predicted_idx]\n",
    "        \n",
    "        # Convert the indices to the actual SSOC using the encoding dictionary\n",
    "        predicted_ssoc = [encoding[ssoc_level]['idx_ssoc'][idx] for idx in predicted_idx]\n",
    "        \n",
    "        # Check if the model made an accurate prediction\n",
    "        # Meaning whether the correct SSOC appeared in the list of predictions\n",
    "        accurate_prediction = False\n",
    "        for ssoc in predicted_ssoc:\n",
    "            if ssoc == target[0:len(ssoc)]:\n",
    "                accurate_prediction = True\n",
    "        \n",
    "        # Append predictions with the predicted probability to the output\n",
    "        predictions_with_proba[ssoc_level] = {\n",
    "            'predicted_ssoc': predicted_ssoc,\n",
    "            'predicted_proba': predicted_proba,\n",
    "            'accurate_prediction': accurate_prediction\n",
    "        }\n",
    "        \n",
    "    return predictions_with_proba\n",
    "\n",
    "def generate_predictions(model, \n",
    "                         tokenizer, \n",
    "                         test_set,\n",
    "                         encoding,\n",
    "                         training_parameters,\n",
    "                         ssoc_prediction_parameters,\n",
    "                         ssoc_level = 'SSOC_4D'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    output = []\n",
    "    accurate_predictions = []\n",
    "    for i, row in test_set.iterrows():\n",
    "        print(f'Generating prediction for {i+1}/{len(test_set)}...', end = '\\r')\n",
    "        predictions_with_proba = generate_single_prediction(model, \n",
    "                                                            tokenizer, \n",
    "                                                            row['title'],\n",
    "                                                            row['description'], \n",
    "                                                            str(row['Predicted_SSOC_2020']),\n",
    "                                                            encoding,\n",
    "                                                            training_parameters,\n",
    "                                                            ssoc_prediction_parameters)\n",
    "        output.append(predictions_with_proba)\n",
    "        accurate_predictions.append(predictions_with_proba[ssoc_level]['accurate_prediction'])\n",
    "    \n",
    "    print('')\n",
    "    accuracy = sum(accurate_predictions)/len(accurate_predictions)\n",
    "    print(f'Overall {ssoc_level} accuracy: {accuracy:.2%}')\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a5d34-e28c-4fdb-8e0d-d3ddbfd10904",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/Train/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea823b2-16f8-4deb-8c5f-64c9efd736ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40b8d3-1c2b-4498-ad28-baaefbace5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction for 492/492...\n",
      "Overall SSOC_4D accuracy: 74.19%\n"
     ]
    }
   ],
   "source": [
    "all_predictions = generate_predictions(model, tokenizer, mrsd_val, encoding, parameters, ssoc_prediction_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74f06ca5-6b9a-42b1-88bd-c59e59f191e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_predictions.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de5e20b0-7bdd-482d-a025-b674896444af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_predictions.pickle', 'rb') as handle:\n",
    "    all_ = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe10d373-9e99-4167-bed0-037b9ec3a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 61.99%\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "for prediction in all_predictions:\n",
    "    selected.append(prediction['SSOC_5D']['accurate_prediction'])\n",
    "accuracy = sum(selected)/len(selected)\n",
    "print(f'Overall accuracy: {accuracy:.2%}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b86904-1a0a-453a-a963-48402249b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"encoding.json\", 'w') as outfile:\n",
    "    json.dump(encoding, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72164f17-e3fe-40bd-ab56-291a3d3faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[colnames['SSOC']].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ce07d-a215-461d-9047-508baed4a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train.generate_encoding(SSOC_2020)\n",
    "encoded_data = train.encode_dataset(data, encoding, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05435c85-2390-4ae7-a2d0-5c6a4d5bb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_1D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4315-d6a4-4702-b6bb-985c42e57ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_2D'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
