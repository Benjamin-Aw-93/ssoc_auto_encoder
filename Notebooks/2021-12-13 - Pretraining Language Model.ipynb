{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b32dbb2-ca2a-4fd2-918f-4a113be3e2e9",
   "metadata": {},
   "source": [
    "## Pretraining Language Model\n",
    "\n",
    "**Author:** Shaun Khoo  \n",
    "**Date:** 13 Dec 2021  \n",
    "**Context:** Following discussions with Prof. Hwee Kuan (technical adviser to the QS team), we decided to try pre-training the DistilBERT model with unlabelled MCF data so the model 'learns' more about MCF data. This would improve downstream performance in the hierarchical classifier.  \n",
    "**Objective:** Pre-train the DistilBERT model on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bc303-dbfa-4fbb-8c9c-7bdbdb7917bd",
   "metadata": {},
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d64a62-3666-4a05-8199-a337e19f6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967c99e-130f-4220-b821-1c00577a6705",
   "metadata": {},
   "source": [
    "Execute the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58944284-ed1c-43e1-b063-4760aa129798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19042.1348]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(venv) C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder>\n",
      "(venv) C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder>python \"ssoc_autocoder/run_mlm.py\"^\n",
      "More?     --model_name_or_path distilbert-base-uncased^\n",
      "More?     --train_file \"Data/Train/pre-training-sample100.txt\"^\n",
      "More?     --line_by_line^\n",
      "More?     --do_train^\n",
      "More?     --do_eval^\n",
      "More?     --evaluation_strategy \"steps\"^\n",
      "More?     --validation_split_percentage 10^\n",
      "More?     --overwrite_output_dir^\n",
      "More?     --output_dir \"Models/pretrain/20211228_pretrain_test_local\"\n",
      "12/28/2021 09:14:51 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/28/2021 09:14:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=Models/pretrain/20211228_pretrain_test_local\\runs\\Dec28_09-14-51_DESKTOP-6H34NC5,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "output_dir=Models/pretrain/20211228_pretrain_test_local,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=20211228_pretrain_test_local,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=Models/pretrain/20211228_pretrain_test_local,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "12/28/2021 09:14:52 - WARNING - datasets.builder - Using custom data configuration default-c37b6cfae00bf3e9\n",
      "12/28/2021 09:14:52 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "12/28/2021 09:14:52 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "12/28/2021 09:14:52 - WARNING - datasets.builder - Reusing dataset text (C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "12/28/2021 09:14:52 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "12/28/2021 09:14:53 - WARNING - datasets.builder - Using custom data configuration default-c37b6cfae00bf3e9\n",
      "12/28/2021 09:14:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "12/28/2021 09:14:53 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "12/28/2021 09:14:53 - WARNING - datasets.builder - Reusing dataset text (C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "12/28/2021 09:14:53 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "12/28/2021 09:14:54 - WARNING - datasets.builder - Using custom data configuration default-c37b6cfae00bf3e9\n",
      "12/28/2021 09:14:54 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "12/28/2021 09:14:54 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "12/28/2021 09:14:54 - WARNING - datasets.builder - Reusing dataset text (C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "12/28/2021 09:14:54 - INFO - datasets.info - Loading Dataset info from C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "{'text': 'Evaluate and renew implemented big data architecture solutions to ensure their relevance and effectiveness in supporting business needs and growth. Design, develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources, process large volume of structured / unstructured data from various sources. Understand business requirements and solution designs to develop and implement solutions that adhere to big data architectural guidelines and address business requirements. Support and maintain previously implemented big data projects, as well as provide guidance and consultation on other projects in active development as needed. Drive optimization, testing and tooling to improve data quality. Document and communicate technical complexities completely and clearly to team members and other key stakeholders. Develop architecture solutions for varied latency needs like batch, real-time, near-real-time and on-demand APIs. Work closely with our data scientist team to gather data requirements to support modelling. Review and approve high level & detailed designs to ensure that the solution delivers to the business needs and at the same time, aligns to the data & analytics architecture principles and roadmap. Help establish and maintain the data governance processes and mechanisms for data lake and EDW. Understand various data security standards and use secure data governance tools to apply and adhere to the required controls on a per data set basis for user access. Maintain and optimize the performance of our data analytics environment.'}\n",
      "{'text': 'The School of Mechanical and Aerospace Engineering (MAE) boasts comprehensive state-of-the-art facilities and a faculty comprising more than 130 professors, making it one of the largest mechanical engineering schools in the world. Faculty members are drawn from renowned universities worldwide, providing a wealth of collective expertise in traditional mechanical and aerospace engineering, and in specialities including manufacturing, mechatronics, innovative design, nanotechnology, and biomedical and computational applications. A Research Associate (RA) positions are available for a research project working on the investigating and understanding the new paradigm for high impact research work. Singapore has made great strides in establishing R&D as a major pillar in the transformation of its economy. Understanding the new paradigm of high quality research work in Singapore is of paramount importance to the progress and sustainability of research in Singapore. Key Responsibilities: Conduct literature searches and write literature reviews. Collect data on publications, collaborations and their impact. Identify factors that contribute to the conducting of high quality research work resulting in high impact collaborations and publications. Assist in writing research reports.'}\n",
      "12/28/2021 09:15:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-6b8c7772af24a786.arrow\n",
      "12/28/2021 09:15:06 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\shaun\\.cache\\huggingface\\datasets\\text\\default-c37b6cfae00bf3e9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-3a73a3b4b7c41f9e.arrow\n",
      "{'train_runtime': 44.9497, 'train_samples_per_second': 6.007, 'train_steps_per_second': 3.003, 'train_loss': 2.754508463541667, 'epoch': 3.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     2.7545\n",
      "  train_runtime            = 0:00:44.94\n",
      "  train_samples            =         90\n",
      "  train_samples_per_second =      6.007\n",
      "  train_steps_per_second   =      3.003\n",
      "12/28/2021 09:15:53 - INFO - __main__ - *** Evaluate ***\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     3.0228\n",
      "  eval_runtime            = 0:00:00.49\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     20.397\n",
      "  eval_steps_per_second   =      4.079\n",
      "  perplexity              =    20.5496\n",
      "\n",
      "(venv) C:\\Users\\shaun\\PycharmProjects\\ssoc-autocoder>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 1104.35it/s]\n",
      "[INFO|configuration_utils.py:561] 2021-12-28 09:14:55,313 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "[INFO|configuration_utils.py:598] 2021-12-28 09:14:55,314 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:561] 2021-12-28 09:14:57,325 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "[INFO|configuration_utils.py:598] 2021-12-28 09:14:57,326 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1739] 2021-12-28 09:15:03,493 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1739] 2021-12-28 09:15:03,493 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1739] 2021-12-28 09:15:03,493 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1739] 2021-12-28 09:15:03,493 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1739] 2021-12-28 09:15:03,493 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:561] 2021-12-28 09:15:04,508 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "[INFO|configuration_utils.py:598] 2021-12-28 09:15:04,509 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1279] 2021-12-28 09:15:05,608 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\shaun/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "[INFO|modeling_utils.py:1524] 2021-12-28 09:15:06,276 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1532] 2021-12-28 09:15:06,276 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "Running tokenizer on dataset line_by_line: 100%|##########| 5/5 [00:00<00:00, 294.09ba/s]\n",
      "[INFO|trainer.py:520] 2021-12-28 09:15:08,205 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1168] 2021-12-28 09:15:08,211 >> ***** Running training *****\n",
      "[INFO|trainer.py:1169] 2021-12-28 09:15:08,211 >>   Num examples = 90\n",
      "[INFO|trainer.py:1170] 2021-12-28 09:15:08,211 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1171] 2021-12-28 09:15:08,211 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1172] 2021-12-28 09:15:08,211 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1173] 2021-12-28 09:15:08,211 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1174] 2021-12-28 09:15:08,211 >>   Total optimization steps = 135\n",
      "100%|##########| 135/135 [00:44<00:00,  2.65it/s][INFO|trainer.py:1366] 2021-12-28 09:15:53,110 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|##########| 135/135 [00:44<00:00,  3.00it/s]\n",
      "[INFO|trainer.py:1935] 2021-12-28 09:15:53,163 >> Saving model checkpoint to Models/pretrain/20211228_pretrain_test_local\n",
      "[INFO|configuration_utils.py:391] 2021-12-28 09:15:53,165 >> Configuration saved in Models/pretrain/20211228_pretrain_test_local\\config.json\n",
      "[INFO|modeling_utils.py:1001] 2021-12-28 09:15:53,721 >> Model weights saved in Models/pretrain/20211228_pretrain_test_local\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2020] 2021-12-28 09:15:53,722 >> tokenizer config file saved in Models/pretrain/20211228_pretrain_test_local\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2021-12-28 09:15:53,723 >> Special tokens file saved in Models/pretrain/20211228_pretrain_test_local\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-12-28 09:15:53,769 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2181] 2021-12-28 09:15:53,771 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2183] 2021-12-28 09:15:53,771 >>   Num examples = 10\n",
      "[INFO|trainer.py:2186] 2021-12-28 09:15:53,771 >>   Batch size = 8\n",
      "100%|##########| 2/2 [00:00<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "\n",
    "python \"ssoc_autocoder/run_mlm.py\"^\n",
    "    --model_name_or_path distilbert-base-uncased^\n",
    "    --train_file \"Data/Train/pre-training-sample1000.txt\"^\n",
    "    --line_by_line^\n",
    "    --do_train^\n",
    "    --do_eval^\n",
    "    --evaluation_strategy \"steps\"^\n",
    "    --validation_split_percentage 5^\n",
    "    --overwrite_output_dir^\n",
    "    --output_dir \"Models/pretrain/20211228_pretrain_test_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93e88a-a490-4c10-bce2-1d65d392ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98e4d9-76ab-4de7-a5be-020677d50dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277ea53-4275-4287-a734-e172e40ea671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
