{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e984e1-4149-449d-8339-888305301829",
   "metadata": {},
   "source": [
    "Importing the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa63fd6-f7a9-408e-8104-80edfa29cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_encoding(data, ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Generates encoding for SSOC to indices, as required by PyTorch\n",
    "    for multi-class classification, for the training data\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe containing all SSOCs\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the SSOC to index mapping (for preparing the\n",
    "        dataset) and index to SSOC mapping (for interpreting the predictions),\n",
    "        for each SSOC level from 1D to 5D.\n",
    "    '''\n",
    "\n",
    "    # Initialise the dictionary object to store the encodings for each level\n",
    "    encoding = {}\n",
    "\n",
    "    # Iterate through each level from 1 to 5\n",
    "    for level in range(1, 6):\n",
    "\n",
    "        # Initialise a dictionary object to store the respective-way encodings\n",
    "        ssoc_idx_mapping = {}\n",
    "\n",
    "        # Slice the SSOC column by the level required, drop duplicates, and sort\n",
    "        ssocs = list(np.sort(data[ssoc_colname].astype('str').str.slice(0, level).unique()))\n",
    "\n",
    "        # Iterate through each unique SSOC (at i-digit level) and add to dict\n",
    "        for i, ssoc in enumerate(ssocs):\n",
    "            ssoc_idx_mapping[ssoc] = i\n",
    "\n",
    "        # Add each level's encodings to the output dictionary\n",
    "        encoding[f'SSOC_{level}D'] = {\n",
    "\n",
    "            # Store the SSOC to index encoding\n",
    "            'ssoc_idx': ssoc_idx_mapping,\n",
    "            # Store the index to SSOC encoding\n",
    "            'idx_ssoc': {v: k for k, v in ssoc_idx_mapping.items()}\n",
    "        }\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def encode_dataset(data,\n",
    "                   encoding,\n",
    "                   ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Uses the generated encoding to encode the SSOCs at each\n",
    "    digit level.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe of the training data with the correct SSOC\n",
    "        encoding: Encoding for each SSOC level\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with each digit SSOC encoded correctly\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    encoded_data = copy.deepcopy(data)\n",
    "\n",
    "    # For each digit, encode the SSOC correctly\n",
    "    for ssoc_level, encodings in encoding.items():\n",
    "        encoded_data[ssoc_level] = encoded_data[ssoc_colname].astype('str').str.slice(0, int(ssoc_level[5])).replace(encodings['ssoc_idx'])\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "# Create a new Python class to handle the additional complexity\n",
    "class SSOC_Dataset(Dataset):\n",
    "\n",
    "    # Define the class attributes\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Define the iterable over the Dataset object \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Extract the text\n",
    "        text = self.data[colnames['job_description']][index]\n",
    "\n",
    "        # Pass in the data into the tokenizer\n",
    "        inputs = self.tokenizer(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        # Extract the IDs and attention mask\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Return all the outputs needed for training and evaluation\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'SSOC_1D': torch.tensor(self.data.SSOC_1D[index], dtype = torch.long),\n",
    "            'SSOC_2D': torch.tensor(self.data.SSOC_2D[index], dtype = torch.long),\n",
    "            'SSOC_3D': torch.tensor(self.data.SSOC_3D[index], dtype = torch.long),\n",
    "            'SSOC_4D': torch.tensor(self.data.SSOC_4D[index], dtype = torch.long),\n",
    "            'SSOC_5D': torch.tensor(self.data.SSOC_5D[index], dtype = torch.long),\n",
    "        } \n",
    "\n",
    "    # Define the length attribute\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44d9967-0376-4345-b512-539480a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data,\n",
    "                 colnames,\n",
    "                 parameters):\n",
    "    \n",
    "    # Encode the dataset with the right targets for each SSOC digit\n",
    "    encoding = generate_encoding(data)\n",
    "    encoded_data = encode_dataset(data, encoding)\n",
    "    \n",
    "    # Split the dataset into training and testing\n",
    "    training_set_number = int(len(encoded_data)*0.8)\n",
    "    testing_set_number = len(encoded_data) - int(len(encoded_data)*0.8)\n",
    "    training_data, testing_data = train_test_split(encoded_data,\n",
    "                                                   test_size = 0.8,\n",
    "                                                   random_state = 2021)\n",
    "    training_data.reset_index(drop = True, inplace = True)\n",
    "    testing_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "    \n",
    "    # Creating the dataset and dataloader for the neural network\n",
    "    training_loader = DataLoader(SSOC_Dataset(training_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                 batch_size = parameters['training_batch_size'],\n",
    "                                 num_workers = parameters['num_workers'],\n",
    "                                 shuffle = True)\n",
    "    testing_loader = DataLoader(SSOC_Dataset(testing_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                batch_size = parameters['training_batch_size'],\n",
    "                                num_workers = parameters['num_workers'],\n",
    "                                shuffle = True)\n",
    "    \n",
    "    return training_loader, testing_loader\n",
    "\n",
    "def prepare_model(encoding, parameters):\n",
    "    \n",
    "    class HierarchicalSSOCClassifier(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(HierarchicalSSOCClassifier, self).__init__()\n",
    "            \n",
    "            self.l1 = DistilBertModel.from_pretrained(parameters['pretrained_model'])\n",
    "\n",
    "            # Generating dimensions\n",
    "            SSOC_1D_count = len(encoding['SSOC_1D']['ssoc_idx'].keys())\n",
    "            SSOC_2D_count = len(encoding['SSOC_2D']['ssoc_idx'].keys())\n",
    "            SSOC_3D_count = len(encoding['SSOC_3D']['ssoc_idx'].keys())\n",
    "            SSOC_4D_count = len(encoding['SSOC_4D']['ssoc_idx'].keys())\n",
    "            SSOC_5D_count = len(encoding['SSOC_5D']['ssoc_idx'].keys())            \n",
    "            \n",
    "            # Stack 1: Predicting 1D SSOC (9)\n",
    "            if parameters['max_level'] >= 1:\n",
    "                self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(768, 768), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(768, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_1D_count)\n",
    "                )\n",
    "\n",
    "            # Stack 2: Predicting 2D SSOC (42)\n",
    "            if parameters['max_level'] >= 2:\n",
    "                n_dims_2d = 768 + SSOC_1D_count\n",
    "                self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_dims_2d, n_dims_2d), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(n_dims_2d, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_2D_count)\n",
    "                )        \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "\n",
    "            # Obtain the sentence embeddings from the DistilBERT model\n",
    "            embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_state = embeddings[0]\n",
    "            X = hidden_state[:, 0]\n",
    "\n",
    "            predictions = {}\n",
    "            \n",
    "            # 1D Prediction\n",
    "            if parameters['max_level'] >= 1:\n",
    "                predictions['SSOC_1D'] = self.ssoc_1d_stack(X)\n",
    "\n",
    "            # 2D Prediction\n",
    "            if parameters['max_level'] >= 2:\n",
    "                X = torch.cat((X, predictions['SSOC_1D']), dim = 1)\n",
    "                predictions['SSOC_2D'] = self.ssoc_2d_stack(X)\n",
    "\n",
    "            return {f'SSOC_{i}D': predictions[f'SSOC_{i}D'] for i in range(1, parameters['max_level'] + 1)}\n",
    "        \n",
    "    model = HierarchicalSSOCClassifier()\n",
    "    model.to(parameters['device'])\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr = parameters['learning_rate'])\n",
    "    \n",
    "    return model, loss_function, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd14b71-6820-4f26-a338-04d2590df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training started on:\", current_time)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Set the NN to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over each batch\n",
    "        for batch, data in enumerate(training_loader):\n",
    "\n",
    "            # Extract the data\n",
    "            ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "            mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Run the forward prop\n",
    "            predictions = model(ids, mask)\n",
    "\n",
    "            # Iterate through each SSOC level\n",
    "            for ssoc_level, preds in predictions.items():\n",
    "\n",
    "                # Extract the correct target for the SSOC level\n",
    "                targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "                # Compute the loss function using the predictions and the targets\n",
    "                level_loss = loss_function(preds, targets)\n",
    "\n",
    "                # Initialise the loss variable if this is the 1D level\n",
    "                # Else add to the loss variable\n",
    "                # Note the weights on each level\n",
    "                if ssoc_level == 'SSOC_1D':\n",
    "                    loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "                else:\n",
    "                    loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "            # Use the deepest level predictions to calculate accuracy\n",
    "            top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "            n_correct += calculate_accu(top_probs_idx, targets)\n",
    "\n",
    "            # Calculate the loss\n",
    "    #         targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "    #         targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "    #         loss1 = loss_function(preds_1d, targets_1d)\n",
    "    #         loss2 = loss_function(preds_2d, targets_2d)\n",
    "    #         loss = loss1*5 + loss2\n",
    "\n",
    "            # Add this batch's loss to the overall training loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 500 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples \n",
    "                print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "                print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "                print(f\"Batch of 500 took {(time.time() - batch_start_time)/60:.2f} mins\")\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "        print(f\"Epoch training time: {(time.time() - epoch_start_time)/60:.2f} mins\")\n",
    "\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} mins\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training ended on:\", current_time)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9f8869-10b0-4ec8-9a37-977c4882278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 2,\n",
    "    'validation_batch_size': 2,\n",
    "    'epochs': 3,\n",
    "    'learning_rate': 1e-05,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 0,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0da49c-bf7e-4e30-827b-11da01be9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoding = generate_encoding(SSOC_2020)\n",
    "encoded_data = encode_dataset(data, encoding)\n",
    "training_loader, testing_loader = prepare_data(encoded_data, colnames, parameters)\n",
    "model, loss_function, optimizer = prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa083cd5-ccad-457b-9ebe-d95ad39b6896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.03325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)/4/50*45/3600*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43fb40d7-95ea-4ca4-bc1c-2334cd9a953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 03 Oct 2021 - 07:28:32\n",
      "Training Loss per 500 steps: 53.78696832275391\n",
      "Training Accuracy per 500 steps: 14.1\n",
      "Batch of 500 took 21.69 mins\n",
      "Training Loss per 500 steps: 49.25872950744629\n",
      "Training Accuracy per 500 steps: 19.4\n",
      "Batch of 500 took 21.43 mins\n",
      "Training Loss per 500 steps: 46.09987485758464\n",
      "Training Accuracy per 500 steps: 22.166666666666668\n",
      "Batch of 500 took 21.47 mins\n",
      "Training Loss per 500 steps: 43.99423152756691\n",
      "Training Accuracy per 500 steps: 23.575\n",
      "Batch of 500 took 21.44 mins\n",
      "Training Loss per 500 steps: 42.02171524372101\n",
      "Training Accuracy per 500 steps: 25.36\n",
      "Batch of 500 took 21.60 mins\n",
      "Training Loss per 500 steps: 40.484713579654695\n",
      "Training Accuracy per 500 steps: 26.983333333333334\n",
      "Batch of 500 took 21.51 mins\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 10 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23488/903107679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23488/475382340.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_function, optimizer, epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# Compute the loss function using the predictions and the targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mlevel_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;31m# Initialise the loss variable if this is the 1D level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 10 is out of bounds."
     ]
    }
   ],
   "source": [
    "train_model(model, loss_function, optimizer, parameters['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a796d-c742-49c5-9057-5fe0fdc7bc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39977202-1d8b-4511-b0de-dba63e556810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SSOC_1D': {'ssoc_idx': {'1': 0,\n",
       "   '2': 1,\n",
       "   '3': 2,\n",
       "   '4': 3,\n",
       "   '5': 4,\n",
       "   '6': 5,\n",
       "   '7': 6,\n",
       "   '8': 7,\n",
       "   '9': 8},\n",
       "  'idx_ssoc': {0: '1',\n",
       "   1: '2',\n",
       "   2: '3',\n",
       "   3: '4',\n",
       "   4: '5',\n",
       "   5: '6',\n",
       "   6: '7',\n",
       "   7: '8',\n",
       "   8: '9'}},\n",
       " 'SSOC_2D': {'ssoc_idx': {'11': 0,\n",
       "   '12': 1,\n",
       "   '13': 2,\n",
       "   '14': 3,\n",
       "   '21': 4,\n",
       "   '22': 5,\n",
       "   '23': 6,\n",
       "   '24': 7,\n",
       "   '25': 8,\n",
       "   '26': 9,\n",
       "   '31': 10,\n",
       "   '32': 11,\n",
       "   '33': 12,\n",
       "   '34': 13,\n",
       "   '35': 14,\n",
       "   '36': 15,\n",
       "   '39': 16,\n",
       "   '40': 17,\n",
       "   '41': 18,\n",
       "   '42': 19,\n",
       "   '43': 20,\n",
       "   '44': 21,\n",
       "   '51': 22,\n",
       "   '52': 23,\n",
       "   '53': 24,\n",
       "   '54': 25,\n",
       "   '59': 26,\n",
       "   '61': 27,\n",
       "   '62': 28,\n",
       "   '71': 29,\n",
       "   '72': 30,\n",
       "   '73': 31,\n",
       "   '74': 32,\n",
       "   '75': 33,\n",
       "   '81': 34,\n",
       "   '82': 35,\n",
       "   '83': 36,\n",
       "   '91': 37,\n",
       "   '92': 38,\n",
       "   '93': 39,\n",
       "   '94': 40,\n",
       "   '96': 41},\n",
       "  'idx_ssoc': {0: '11',\n",
       "   1: '12',\n",
       "   2: '13',\n",
       "   3: '14',\n",
       "   4: '21',\n",
       "   5: '22',\n",
       "   6: '23',\n",
       "   7: '24',\n",
       "   8: '25',\n",
       "   9: '26',\n",
       "   10: '31',\n",
       "   11: '32',\n",
       "   12: '33',\n",
       "   13: '34',\n",
       "   14: '35',\n",
       "   15: '36',\n",
       "   16: '39',\n",
       "   17: '40',\n",
       "   18: '41',\n",
       "   19: '42',\n",
       "   20: '43',\n",
       "   21: '44',\n",
       "   22: '51',\n",
       "   23: '52',\n",
       "   24: '53',\n",
       "   25: '54',\n",
       "   26: '59',\n",
       "   27: '61',\n",
       "   28: '62',\n",
       "   29: '71',\n",
       "   30: '72',\n",
       "   31: '73',\n",
       "   32: '74',\n",
       "   33: '75',\n",
       "   34: '81',\n",
       "   35: '82',\n",
       "   36: '83',\n",
       "   37: '91',\n",
       "   38: '92',\n",
       "   39: '93',\n",
       "   40: '94',\n",
       "   41: '96'}},\n",
       " 'SSOC_3D': {'ssoc_idx': {'111': 0,\n",
       "   '112': 1,\n",
       "   '121': 2,\n",
       "   '122': 3,\n",
       "   '131': 4,\n",
       "   '132': 5,\n",
       "   '133': 6,\n",
       "   '134': 7,\n",
       "   '141': 8,\n",
       "   '142': 9,\n",
       "   '143': 10,\n",
       "   '211': 11,\n",
       "   '212': 12,\n",
       "   '213': 13,\n",
       "   '214': 14,\n",
       "   '215': 15,\n",
       "   '216': 16,\n",
       "   '217': 17,\n",
       "   '221': 18,\n",
       "   '222': 19,\n",
       "   '223': 20,\n",
       "   '225': 21,\n",
       "   '226': 22,\n",
       "   '231': 23,\n",
       "   '232': 24,\n",
       "   '233': 25,\n",
       "   '234': 26,\n",
       "   '235': 27,\n",
       "   '236': 28,\n",
       "   '241': 29,\n",
       "   '242': 30,\n",
       "   '243': 31,\n",
       "   '251': 32,\n",
       "   '252': 33,\n",
       "   '261': 34,\n",
       "   '262': 35,\n",
       "   '263': 36,\n",
       "   '264': 37,\n",
       "   '265': 38,\n",
       "   '310': 39,\n",
       "   '311': 40,\n",
       "   '312': 41,\n",
       "   '313': 42,\n",
       "   '314': 43,\n",
       "   '315': 44,\n",
       "   '316': 45,\n",
       "   '317': 46,\n",
       "   '321': 47,\n",
       "   '322': 48,\n",
       "   '323': 49,\n",
       "   '324': 50,\n",
       "   '325': 51,\n",
       "   '331': 52,\n",
       "   '332': 53,\n",
       "   '333': 54,\n",
       "   '334': 55,\n",
       "   '335': 56,\n",
       "   '336': 57,\n",
       "   '341': 58,\n",
       "   '342': 59,\n",
       "   '343': 60,\n",
       "   '344': 61,\n",
       "   '351': 62,\n",
       "   '352': 63,\n",
       "   '361': 64,\n",
       "   '362': 65,\n",
       "   '369': 66,\n",
       "   '399': 67,\n",
       "   '400': 68,\n",
       "   '411': 69,\n",
       "   '412': 70,\n",
       "   '413': 71,\n",
       "   '421': 72,\n",
       "   '422': 73,\n",
       "   '431': 74,\n",
       "   '432': 75,\n",
       "   '441': 76,\n",
       "   '511': 77,\n",
       "   '512': 78,\n",
       "   '513': 79,\n",
       "   '514': 80,\n",
       "   '515': 81,\n",
       "   '517': 82,\n",
       "   '519': 83,\n",
       "   '521': 84,\n",
       "   '522': 85,\n",
       "   '523': 86,\n",
       "   '524': 87,\n",
       "   '531': 88,\n",
       "   '532': 89,\n",
       "   '541': 90,\n",
       "   '590': 91,\n",
       "   '611': 92,\n",
       "   '612': 93,\n",
       "   '619': 94,\n",
       "   '622': 95,\n",
       "   '710': 96,\n",
       "   '711': 97,\n",
       "   '712': 98,\n",
       "   '713': 99,\n",
       "   '720': 100,\n",
       "   '721': 101,\n",
       "   '722': 102,\n",
       "   '723': 103,\n",
       "   '730': 104,\n",
       "   '731': 105,\n",
       "   '732': 106,\n",
       "   '740': 107,\n",
       "   '741': 108,\n",
       "   '742': 109,\n",
       "   '750': 110,\n",
       "   '751': 111,\n",
       "   '752': 112,\n",
       "   '753': 113,\n",
       "   '754': 114,\n",
       "   '810': 115,\n",
       "   '811': 116,\n",
       "   '812': 117,\n",
       "   '813': 118,\n",
       "   '814': 119,\n",
       "   '815': 120,\n",
       "   '816': 121,\n",
       "   '817': 122,\n",
       "   '818': 123,\n",
       "   '820': 124,\n",
       "   '821': 125,\n",
       "   '830': 126,\n",
       "   '831': 127,\n",
       "   '832': 128,\n",
       "   '833': 129,\n",
       "   '834': 130,\n",
       "   '835': 131,\n",
       "   '910': 132,\n",
       "   '911': 133,\n",
       "   '912': 134,\n",
       "   '913': 135,\n",
       "   '921': 136,\n",
       "   '931': 137,\n",
       "   '932': 138,\n",
       "   '933': 139,\n",
       "   '941': 140,\n",
       "   '960': 141,\n",
       "   '961': 142,\n",
       "   '962': 143},\n",
       "  'idx_ssoc': {0: '111',\n",
       "   1: '112',\n",
       "   2: '121',\n",
       "   3: '122',\n",
       "   4: '131',\n",
       "   5: '132',\n",
       "   6: '133',\n",
       "   7: '134',\n",
       "   8: '141',\n",
       "   9: '142',\n",
       "   10: '143',\n",
       "   11: '211',\n",
       "   12: '212',\n",
       "   13: '213',\n",
       "   14: '214',\n",
       "   15: '215',\n",
       "   16: '216',\n",
       "   17: '217',\n",
       "   18: '221',\n",
       "   19: '222',\n",
       "   20: '223',\n",
       "   21: '225',\n",
       "   22: '226',\n",
       "   23: '231',\n",
       "   24: '232',\n",
       "   25: '233',\n",
       "   26: '234',\n",
       "   27: '235',\n",
       "   28: '236',\n",
       "   29: '241',\n",
       "   30: '242',\n",
       "   31: '243',\n",
       "   32: '251',\n",
       "   33: '252',\n",
       "   34: '261',\n",
       "   35: '262',\n",
       "   36: '263',\n",
       "   37: '264',\n",
       "   38: '265',\n",
       "   39: '310',\n",
       "   40: '311',\n",
       "   41: '312',\n",
       "   42: '313',\n",
       "   43: '314',\n",
       "   44: '315',\n",
       "   45: '316',\n",
       "   46: '317',\n",
       "   47: '321',\n",
       "   48: '322',\n",
       "   49: '323',\n",
       "   50: '324',\n",
       "   51: '325',\n",
       "   52: '331',\n",
       "   53: '332',\n",
       "   54: '333',\n",
       "   55: '334',\n",
       "   56: '335',\n",
       "   57: '336',\n",
       "   58: '341',\n",
       "   59: '342',\n",
       "   60: '343',\n",
       "   61: '344',\n",
       "   62: '351',\n",
       "   63: '352',\n",
       "   64: '361',\n",
       "   65: '362',\n",
       "   66: '369',\n",
       "   67: '399',\n",
       "   68: '400',\n",
       "   69: '411',\n",
       "   70: '412',\n",
       "   71: '413',\n",
       "   72: '421',\n",
       "   73: '422',\n",
       "   74: '431',\n",
       "   75: '432',\n",
       "   76: '441',\n",
       "   77: '511',\n",
       "   78: '512',\n",
       "   79: '513',\n",
       "   80: '514',\n",
       "   81: '515',\n",
       "   82: '517',\n",
       "   83: '519',\n",
       "   84: '521',\n",
       "   85: '522',\n",
       "   86: '523',\n",
       "   87: '524',\n",
       "   88: '531',\n",
       "   89: '532',\n",
       "   90: '541',\n",
       "   91: '590',\n",
       "   92: '611',\n",
       "   93: '612',\n",
       "   94: '619',\n",
       "   95: '622',\n",
       "   96: '710',\n",
       "   97: '711',\n",
       "   98: '712',\n",
       "   99: '713',\n",
       "   100: '720',\n",
       "   101: '721',\n",
       "   102: '722',\n",
       "   103: '723',\n",
       "   104: '730',\n",
       "   105: '731',\n",
       "   106: '732',\n",
       "   107: '740',\n",
       "   108: '741',\n",
       "   109: '742',\n",
       "   110: '750',\n",
       "   111: '751',\n",
       "   112: '752',\n",
       "   113: '753',\n",
       "   114: '754',\n",
       "   115: '810',\n",
       "   116: '811',\n",
       "   117: '812',\n",
       "   118: '813',\n",
       "   119: '814',\n",
       "   120: '815',\n",
       "   121: '816',\n",
       "   122: '817',\n",
       "   123: '818',\n",
       "   124: '820',\n",
       "   125: '821',\n",
       "   126: '830',\n",
       "   127: '831',\n",
       "   128: '832',\n",
       "   129: '833',\n",
       "   130: '834',\n",
       "   131: '835',\n",
       "   132: '910',\n",
       "   133: '911',\n",
       "   134: '912',\n",
       "   135: '913',\n",
       "   136: '921',\n",
       "   137: '931',\n",
       "   138: '932',\n",
       "   139: '933',\n",
       "   140: '941',\n",
       "   141: '960',\n",
       "   142: '961',\n",
       "   143: '962'}},\n",
       " 'SSOC_4D': {'ssoc_idx': {'1111': 0,\n",
       "   '1112': 1,\n",
       "   '1114': 2,\n",
       "   '1115': 3,\n",
       "   '1116': 4,\n",
       "   '1120': 5,\n",
       "   '1211': 6,\n",
       "   '1212': 7,\n",
       "   '1213': 8,\n",
       "   '1219': 9,\n",
       "   '1221': 10,\n",
       "   '1222': 11,\n",
       "   '1223': 12,\n",
       "   '1224': 13,\n",
       "   '1310': 14,\n",
       "   '1321': 15,\n",
       "   '1323': 16,\n",
       "   '1324': 17,\n",
       "   '1329': 18,\n",
       "   '1330': 19,\n",
       "   '1341': 20,\n",
       "   '1342': 21,\n",
       "   '1343': 22,\n",
       "   '1344': 23,\n",
       "   '1345': 24,\n",
       "   '1346': 25,\n",
       "   '1349': 26,\n",
       "   '1411': 27,\n",
       "   '1412': 28,\n",
       "   '1420': 29,\n",
       "   '1431': 30,\n",
       "   '1432': 31,\n",
       "   '1433': 32,\n",
       "   '1439': 33,\n",
       "   '2111': 34,\n",
       "   '2112': 35,\n",
       "   '2113': 36,\n",
       "   '2114': 37,\n",
       "   '2121': 38,\n",
       "   '2122': 39,\n",
       "   '2123': 40,\n",
       "   '2131': 41,\n",
       "   '2132': 42,\n",
       "   '2133': 43,\n",
       "   '2134': 44,\n",
       "   '2141': 45,\n",
       "   '2142': 46,\n",
       "   '2143': 47,\n",
       "   '2144': 48,\n",
       "   '2145': 49,\n",
       "   '2146': 50,\n",
       "   '2147': 51,\n",
       "   '2149': 52,\n",
       "   '2151': 53,\n",
       "   '2152': 54,\n",
       "   '2153': 55,\n",
       "   '2161': 56,\n",
       "   '2162': 57,\n",
       "   '2163': 58,\n",
       "   '2164': 59,\n",
       "   '2165': 60,\n",
       "   '2166': 61,\n",
       "   '2167': 62,\n",
       "   '2171': 63,\n",
       "   '2172': 64,\n",
       "   '2211': 65,\n",
       "   '2212': 66,\n",
       "   '2213': 67,\n",
       "   '2214': 68,\n",
       "   '2215': 69,\n",
       "   '2220': 70,\n",
       "   '2230': 71,\n",
       "   '2250': 72,\n",
       "   '2261': 73,\n",
       "   '2262': 74,\n",
       "   '2263': 75,\n",
       "   '2264': 76,\n",
       "   '2265': 77,\n",
       "   '2266': 78,\n",
       "   '2267': 79,\n",
       "   '2268': 80,\n",
       "   '2269': 81,\n",
       "   '2310': 82,\n",
       "   '2320': 83,\n",
       "   '2330': 84,\n",
       "   '2340': 85,\n",
       "   '2350': 86,\n",
       "   '2361': 87,\n",
       "   '2362': 88,\n",
       "   '2369': 89,\n",
       "   '2411': 90,\n",
       "   '2412': 91,\n",
       "   '2413': 92,\n",
       "   '2414': 93,\n",
       "   '2415': 94,\n",
       "   '2416': 95,\n",
       "   '2421': 96,\n",
       "   '2422': 97,\n",
       "   '2423': 98,\n",
       "   '2424': 99,\n",
       "   '2425': 100,\n",
       "   '2429': 101,\n",
       "   '2431': 102,\n",
       "   '2432': 103,\n",
       "   '2433': 104,\n",
       "   '2435': 105,\n",
       "   '2436': 106,\n",
       "   '2511': 107,\n",
       "   '2512': 108,\n",
       "   '2514': 109,\n",
       "   '2515': 110,\n",
       "   '2519': 111,\n",
       "   '2521': 112,\n",
       "   '2522': 113,\n",
       "   '2523': 114,\n",
       "   '2524': 115,\n",
       "   '2529': 116,\n",
       "   '2611': 117,\n",
       "   '2612': 118,\n",
       "   '2619': 119,\n",
       "   '2621': 120,\n",
       "   '2622': 121,\n",
       "   '2631': 122,\n",
       "   '2632': 123,\n",
       "   '2633': 124,\n",
       "   '2634': 125,\n",
       "   '2635': 126,\n",
       "   '2636': 127,\n",
       "   '2637': 128,\n",
       "   '2641': 129,\n",
       "   '2642': 130,\n",
       "   '2643': 131,\n",
       "   '2651': 132,\n",
       "   '2652': 133,\n",
       "   '2653': 134,\n",
       "   '2654': 135,\n",
       "   '2655': 136,\n",
       "   '2656': 137,\n",
       "   '3100': 138,\n",
       "   '3111': 139,\n",
       "   '3112': 140,\n",
       "   '3113': 141,\n",
       "   '3114': 142,\n",
       "   '3115': 143,\n",
       "   '3116': 144,\n",
       "   '3117': 145,\n",
       "   '3118': 146,\n",
       "   '3121': 147,\n",
       "   '3129': 148,\n",
       "   '3131': 149,\n",
       "   '3132': 150,\n",
       "   '3133': 151,\n",
       "   '3134': 152,\n",
       "   '3135': 153,\n",
       "   '3139': 154,\n",
       "   '3141': 155,\n",
       "   '3142': 156,\n",
       "   '3151': 157,\n",
       "   '3152': 158,\n",
       "   '3154': 159,\n",
       "   '3156': 160,\n",
       "   '3157': 161,\n",
       "   '3159': 162,\n",
       "   '3160': 163,\n",
       "   '3171': 164,\n",
       "   '3172': 165,\n",
       "   '3211': 166,\n",
       "   '3212': 167,\n",
       "   '3213': 168,\n",
       "   '3214': 169,\n",
       "   '3220': 170,\n",
       "   '3230': 171,\n",
       "   '3240': 172,\n",
       "   '3251': 173,\n",
       "   '3253': 174,\n",
       "   '3254': 175,\n",
       "   '3255': 176,\n",
       "   '3257': 177,\n",
       "   '3259': 178,\n",
       "   '3312': 179,\n",
       "   '3313': 180,\n",
       "   '3315': 181,\n",
       "   '3321': 182,\n",
       "   '3322': 183,\n",
       "   '3323': 184,\n",
       "   '3329': 185,\n",
       "   '3331': 186,\n",
       "   '3332': 187,\n",
       "   '3333': 188,\n",
       "   '3334': 189,\n",
       "   '3339': 190,\n",
       "   '3346': 191,\n",
       "   '3349': 192,\n",
       "   '3351': 193,\n",
       "   '3355': 194,\n",
       "   '3359': 195,\n",
       "   '3361': 196,\n",
       "   '3411': 197,\n",
       "   '3412': 198,\n",
       "   '3421': 199,\n",
       "   '3422': 200,\n",
       "   '3431': 201,\n",
       "   '3432': 202,\n",
       "   '3433': 203,\n",
       "   '3434': 204,\n",
       "   '3439': 205,\n",
       "   '3440': 206,\n",
       "   '3511': 207,\n",
       "   '3512': 208,\n",
       "   '3514': 209,\n",
       "   '3521': 210,\n",
       "   '3522': 211,\n",
       "   '3523': 212,\n",
       "   '3529': 213,\n",
       "   '3610': 214,\n",
       "   '3620': 215,\n",
       "   '3691': 216,\n",
       "   '3699': 217,\n",
       "   '3991': 218,\n",
       "   '4000': 219,\n",
       "   '4110': 220,\n",
       "   '4120': 221,\n",
       "   '4131': 222,\n",
       "   '4132': 223,\n",
       "   '4211': 224,\n",
       "   '4213': 225,\n",
       "   '4214': 226,\n",
       "   '4221': 227,\n",
       "   '4223': 228,\n",
       "   '4224': 229,\n",
       "   '4229': 230,\n",
       "   '4311': 231,\n",
       "   '4312': 232,\n",
       "   '4314': 233,\n",
       "   '4315': 234,\n",
       "   '4321': 235,\n",
       "   '4322': 236,\n",
       "   '4323': 237,\n",
       "   '4411': 238,\n",
       "   '4412': 239,\n",
       "   '4417': 240,\n",
       "   '4419': 241,\n",
       "   '5111': 242,\n",
       "   '5112': 243,\n",
       "   '5113': 244,\n",
       "   '5120': 245,\n",
       "   '5131': 246,\n",
       "   '5132': 247,\n",
       "   '5133': 248,\n",
       "   '5139': 249,\n",
       "   '5141': 250,\n",
       "   '5142': 251,\n",
       "   '5149': 252,\n",
       "   '5150': 253,\n",
       "   '5170': 254,\n",
       "   '5191': 255,\n",
       "   '5193': 256,\n",
       "   '5194': 257,\n",
       "   '5195': 258,\n",
       "   '5199': 259,\n",
       "   '5211': 260,\n",
       "   '5212': 261,\n",
       "   '5213': 262,\n",
       "   '5219': 263,\n",
       "   '5220': 264,\n",
       "   '5230': 265,\n",
       "   '5241': 266,\n",
       "   '5242': 267,\n",
       "   '5244': 268,\n",
       "   '5249': 269,\n",
       "   '5311': 270,\n",
       "   '5312': 271,\n",
       "   '5320': 272,\n",
       "   '5411': 273,\n",
       "   '5412': 274,\n",
       "   '5413': 275,\n",
       "   '5414': 276,\n",
       "   '5415': 277,\n",
       "   '5419': 278,\n",
       "   '5900': 279,\n",
       "   '6111': 280,\n",
       "   '6113': 281,\n",
       "   '6121': 282,\n",
       "   '6122': 283,\n",
       "   '6190': 284,\n",
       "   '6221': 285,\n",
       "   '6222': 286,\n",
       "   '7100': 287,\n",
       "   '7112': 288,\n",
       "   '7113': 289,\n",
       "   '7114': 290,\n",
       "   '7115': 291,\n",
       "   '7119': 292,\n",
       "   '7121': 293,\n",
       "   '7122': 294,\n",
       "   '7123': 295,\n",
       "   '7124': 296,\n",
       "   '7125': 297,\n",
       "   '7126': 298,\n",
       "   '7127': 299,\n",
       "   '7129': 300,\n",
       "   '7131': 301,\n",
       "   '7132': 302,\n",
       "   '7133': 303,\n",
       "   '7200': 304,\n",
       "   '7211': 305,\n",
       "   '7212': 306,\n",
       "   '7213': 307,\n",
       "   '7214': 308,\n",
       "   '7215': 309,\n",
       "   '7221': 310,\n",
       "   '7222': 311,\n",
       "   '7224': 312,\n",
       "   '7231': 313,\n",
       "   '7232': 314,\n",
       "   '7233': 315,\n",
       "   '7234': 316,\n",
       "   '7235': 317,\n",
       "   '7239': 318,\n",
       "   '7300': 319,\n",
       "   '7311': 320,\n",
       "   '7312': 321,\n",
       "   '7313': 322,\n",
       "   '7314': 323,\n",
       "   '7315': 324,\n",
       "   '7316': 325,\n",
       "   '7321': 326,\n",
       "   '7322': 327,\n",
       "   '7400': 328,\n",
       "   '7411': 329,\n",
       "   '7412': 330,\n",
       "   '7413': 331,\n",
       "   '7421': 332,\n",
       "   '7422': 333,\n",
       "   '7500': 334,\n",
       "   '7511': 335,\n",
       "   '7512': 336,\n",
       "   '7515': 337,\n",
       "   '7519': 338,\n",
       "   '7521': 339,\n",
       "   '7522': 340,\n",
       "   '7529': 341,\n",
       "   '7531': 342,\n",
       "   '7532': 343,\n",
       "   '7534': 344,\n",
       "   '7536': 345,\n",
       "   '7539': 346,\n",
       "   '7541': 347,\n",
       "   '7544': 348,\n",
       "   '7549': 349,\n",
       "   '8100': 350,\n",
       "   '8113': 351,\n",
       "   '8114': 352,\n",
       "   '8121': 353,\n",
       "   '8122': 354,\n",
       "   '8123': 355,\n",
       "   '8124': 356,\n",
       "   '8125': 357,\n",
       "   '8131': 358,\n",
       "   '8132': 359,\n",
       "   '8139': 360,\n",
       "   '8141': 361,\n",
       "   '8142': 362,\n",
       "   '8143': 363,\n",
       "   '8150': 364,\n",
       "   '8160': 365,\n",
       "   '8170': 366,\n",
       "   '8181': 367,\n",
       "   '8182': 368,\n",
       "   '8183': 369,\n",
       "   '8184': 370,\n",
       "   '8189': 371,\n",
       "   '8200': 372,\n",
       "   '8211': 373,\n",
       "   '8212': 374,\n",
       "   '8213': 375,\n",
       "   '8219': 376,\n",
       "   '8300': 377,\n",
       "   '8311': 378,\n",
       "   '8312': 379,\n",
       "   '8321': 380,\n",
       "   '8322': 381,\n",
       "   '8331': 382,\n",
       "   '8332': 383,\n",
       "   '8342': 384,\n",
       "   '8343': 385,\n",
       "   '8344': 386,\n",
       "   '8349': 387,\n",
       "   '8350': 388,\n",
       "   '9100': 389,\n",
       "   '9112': 390,\n",
       "   '9113': 391,\n",
       "   '9115': 392,\n",
       "   '9116': 393,\n",
       "   '9121': 394,\n",
       "   '9122': 395,\n",
       "   '9123': 396,\n",
       "   '9129': 397,\n",
       "   '9130': 398,\n",
       "   '9214': 399,\n",
       "   '9219': 400,\n",
       "   '9310': 401,\n",
       "   '9320': 402,\n",
       "   '9331': 403,\n",
       "   '9333': 404,\n",
       "   '9410': 405,\n",
       "   '9600': 406,\n",
       "   '9611': 407,\n",
       "   '9621': 408,\n",
       "   '9625': 409,\n",
       "   '9626': 410,\n",
       "   '9627': 411,\n",
       "   '9629': 412},\n",
       "  'idx_ssoc': {0: '1111',\n",
       "   1: '1112',\n",
       "   2: '1114',\n",
       "   3: '1115',\n",
       "   4: '1116',\n",
       "   5: '1120',\n",
       "   6: '1211',\n",
       "   7: '1212',\n",
       "   8: '1213',\n",
       "   9: '1219',\n",
       "   10: '1221',\n",
       "   11: '1222',\n",
       "   12: '1223',\n",
       "   13: '1224',\n",
       "   14: '1310',\n",
       "   15: '1321',\n",
       "   16: '1323',\n",
       "   17: '1324',\n",
       "   18: '1329',\n",
       "   19: '1330',\n",
       "   20: '1341',\n",
       "   21: '1342',\n",
       "   22: '1343',\n",
       "   23: '1344',\n",
       "   24: '1345',\n",
       "   25: '1346',\n",
       "   26: '1349',\n",
       "   27: '1411',\n",
       "   28: '1412',\n",
       "   29: '1420',\n",
       "   30: '1431',\n",
       "   31: '1432',\n",
       "   32: '1433',\n",
       "   33: '1439',\n",
       "   34: '2111',\n",
       "   35: '2112',\n",
       "   36: '2113',\n",
       "   37: '2114',\n",
       "   38: '2121',\n",
       "   39: '2122',\n",
       "   40: '2123',\n",
       "   41: '2131',\n",
       "   42: '2132',\n",
       "   43: '2133',\n",
       "   44: '2134',\n",
       "   45: '2141',\n",
       "   46: '2142',\n",
       "   47: '2143',\n",
       "   48: '2144',\n",
       "   49: '2145',\n",
       "   50: '2146',\n",
       "   51: '2147',\n",
       "   52: '2149',\n",
       "   53: '2151',\n",
       "   54: '2152',\n",
       "   55: '2153',\n",
       "   56: '2161',\n",
       "   57: '2162',\n",
       "   58: '2163',\n",
       "   59: '2164',\n",
       "   60: '2165',\n",
       "   61: '2166',\n",
       "   62: '2167',\n",
       "   63: '2171',\n",
       "   64: '2172',\n",
       "   65: '2211',\n",
       "   66: '2212',\n",
       "   67: '2213',\n",
       "   68: '2214',\n",
       "   69: '2215',\n",
       "   70: '2220',\n",
       "   71: '2230',\n",
       "   72: '2250',\n",
       "   73: '2261',\n",
       "   74: '2262',\n",
       "   75: '2263',\n",
       "   76: '2264',\n",
       "   77: '2265',\n",
       "   78: '2266',\n",
       "   79: '2267',\n",
       "   80: '2268',\n",
       "   81: '2269',\n",
       "   82: '2310',\n",
       "   83: '2320',\n",
       "   84: '2330',\n",
       "   85: '2340',\n",
       "   86: '2350',\n",
       "   87: '2361',\n",
       "   88: '2362',\n",
       "   89: '2369',\n",
       "   90: '2411',\n",
       "   91: '2412',\n",
       "   92: '2413',\n",
       "   93: '2414',\n",
       "   94: '2415',\n",
       "   95: '2416',\n",
       "   96: '2421',\n",
       "   97: '2422',\n",
       "   98: '2423',\n",
       "   99: '2424',\n",
       "   100: '2425',\n",
       "   101: '2429',\n",
       "   102: '2431',\n",
       "   103: '2432',\n",
       "   104: '2433',\n",
       "   105: '2435',\n",
       "   106: '2436',\n",
       "   107: '2511',\n",
       "   108: '2512',\n",
       "   109: '2514',\n",
       "   110: '2515',\n",
       "   111: '2519',\n",
       "   112: '2521',\n",
       "   113: '2522',\n",
       "   114: '2523',\n",
       "   115: '2524',\n",
       "   116: '2529',\n",
       "   117: '2611',\n",
       "   118: '2612',\n",
       "   119: '2619',\n",
       "   120: '2621',\n",
       "   121: '2622',\n",
       "   122: '2631',\n",
       "   123: '2632',\n",
       "   124: '2633',\n",
       "   125: '2634',\n",
       "   126: '2635',\n",
       "   127: '2636',\n",
       "   128: '2637',\n",
       "   129: '2641',\n",
       "   130: '2642',\n",
       "   131: '2643',\n",
       "   132: '2651',\n",
       "   133: '2652',\n",
       "   134: '2653',\n",
       "   135: '2654',\n",
       "   136: '2655',\n",
       "   137: '2656',\n",
       "   138: '3100',\n",
       "   139: '3111',\n",
       "   140: '3112',\n",
       "   141: '3113',\n",
       "   142: '3114',\n",
       "   143: '3115',\n",
       "   144: '3116',\n",
       "   145: '3117',\n",
       "   146: '3118',\n",
       "   147: '3121',\n",
       "   148: '3129',\n",
       "   149: '3131',\n",
       "   150: '3132',\n",
       "   151: '3133',\n",
       "   152: '3134',\n",
       "   153: '3135',\n",
       "   154: '3139',\n",
       "   155: '3141',\n",
       "   156: '3142',\n",
       "   157: '3151',\n",
       "   158: '3152',\n",
       "   159: '3154',\n",
       "   160: '3156',\n",
       "   161: '3157',\n",
       "   162: '3159',\n",
       "   163: '3160',\n",
       "   164: '3171',\n",
       "   165: '3172',\n",
       "   166: '3211',\n",
       "   167: '3212',\n",
       "   168: '3213',\n",
       "   169: '3214',\n",
       "   170: '3220',\n",
       "   171: '3230',\n",
       "   172: '3240',\n",
       "   173: '3251',\n",
       "   174: '3253',\n",
       "   175: '3254',\n",
       "   176: '3255',\n",
       "   177: '3257',\n",
       "   178: '3259',\n",
       "   179: '3312',\n",
       "   180: '3313',\n",
       "   181: '3315',\n",
       "   182: '3321',\n",
       "   183: '3322',\n",
       "   184: '3323',\n",
       "   185: '3329',\n",
       "   186: '3331',\n",
       "   187: '3332',\n",
       "   188: '3333',\n",
       "   189: '3334',\n",
       "   190: '3339',\n",
       "   191: '3346',\n",
       "   192: '3349',\n",
       "   193: '3351',\n",
       "   194: '3355',\n",
       "   195: '3359',\n",
       "   196: '3361',\n",
       "   197: '3411',\n",
       "   198: '3412',\n",
       "   199: '3421',\n",
       "   200: '3422',\n",
       "   201: '3431',\n",
       "   202: '3432',\n",
       "   203: '3433',\n",
       "   204: '3434',\n",
       "   205: '3439',\n",
       "   206: '3440',\n",
       "   207: '3511',\n",
       "   208: '3512',\n",
       "   209: '3514',\n",
       "   210: '3521',\n",
       "   211: '3522',\n",
       "   212: '3523',\n",
       "   213: '3529',\n",
       "   214: '3610',\n",
       "   215: '3620',\n",
       "   216: '3691',\n",
       "   217: '3699',\n",
       "   218: '3991',\n",
       "   219: '4000',\n",
       "   220: '4110',\n",
       "   221: '4120',\n",
       "   222: '4131',\n",
       "   223: '4132',\n",
       "   224: '4211',\n",
       "   225: '4213',\n",
       "   226: '4214',\n",
       "   227: '4221',\n",
       "   228: '4223',\n",
       "   229: '4224',\n",
       "   230: '4229',\n",
       "   231: '4311',\n",
       "   232: '4312',\n",
       "   233: '4314',\n",
       "   234: '4315',\n",
       "   235: '4321',\n",
       "   236: '4322',\n",
       "   237: '4323',\n",
       "   238: '4411',\n",
       "   239: '4412',\n",
       "   240: '4417',\n",
       "   241: '4419',\n",
       "   242: '5111',\n",
       "   243: '5112',\n",
       "   244: '5113',\n",
       "   245: '5120',\n",
       "   246: '5131',\n",
       "   247: '5132',\n",
       "   248: '5133',\n",
       "   249: '5139',\n",
       "   250: '5141',\n",
       "   251: '5142',\n",
       "   252: '5149',\n",
       "   253: '5150',\n",
       "   254: '5170',\n",
       "   255: '5191',\n",
       "   256: '5193',\n",
       "   257: '5194',\n",
       "   258: '5195',\n",
       "   259: '5199',\n",
       "   260: '5211',\n",
       "   261: '5212',\n",
       "   262: '5213',\n",
       "   263: '5219',\n",
       "   264: '5220',\n",
       "   265: '5230',\n",
       "   266: '5241',\n",
       "   267: '5242',\n",
       "   268: '5244',\n",
       "   269: '5249',\n",
       "   270: '5311',\n",
       "   271: '5312',\n",
       "   272: '5320',\n",
       "   273: '5411',\n",
       "   274: '5412',\n",
       "   275: '5413',\n",
       "   276: '5414',\n",
       "   277: '5415',\n",
       "   278: '5419',\n",
       "   279: '5900',\n",
       "   280: '6111',\n",
       "   281: '6113',\n",
       "   282: '6121',\n",
       "   283: '6122',\n",
       "   284: '6190',\n",
       "   285: '6221',\n",
       "   286: '6222',\n",
       "   287: '7100',\n",
       "   288: '7112',\n",
       "   289: '7113',\n",
       "   290: '7114',\n",
       "   291: '7115',\n",
       "   292: '7119',\n",
       "   293: '7121',\n",
       "   294: '7122',\n",
       "   295: '7123',\n",
       "   296: '7124',\n",
       "   297: '7125',\n",
       "   298: '7126',\n",
       "   299: '7127',\n",
       "   300: '7129',\n",
       "   301: '7131',\n",
       "   302: '7132',\n",
       "   303: '7133',\n",
       "   304: '7200',\n",
       "   305: '7211',\n",
       "   306: '7212',\n",
       "   307: '7213',\n",
       "   308: '7214',\n",
       "   309: '7215',\n",
       "   310: '7221',\n",
       "   311: '7222',\n",
       "   312: '7224',\n",
       "   313: '7231',\n",
       "   314: '7232',\n",
       "   315: '7233',\n",
       "   316: '7234',\n",
       "   317: '7235',\n",
       "   318: '7239',\n",
       "   319: '7300',\n",
       "   320: '7311',\n",
       "   321: '7312',\n",
       "   322: '7313',\n",
       "   323: '7314',\n",
       "   324: '7315',\n",
       "   325: '7316',\n",
       "   326: '7321',\n",
       "   327: '7322',\n",
       "   328: '7400',\n",
       "   329: '7411',\n",
       "   330: '7412',\n",
       "   331: '7413',\n",
       "   332: '7421',\n",
       "   333: '7422',\n",
       "   334: '7500',\n",
       "   335: '7511',\n",
       "   336: '7512',\n",
       "   337: '7515',\n",
       "   338: '7519',\n",
       "   339: '7521',\n",
       "   340: '7522',\n",
       "   341: '7529',\n",
       "   342: '7531',\n",
       "   343: '7532',\n",
       "   344: '7534',\n",
       "   345: '7536',\n",
       "   346: '7539',\n",
       "   347: '7541',\n",
       "   348: '7544',\n",
       "   349: '7549',\n",
       "   350: '8100',\n",
       "   351: '8113',\n",
       "   352: '8114',\n",
       "   353: '8121',\n",
       "   354: '8122',\n",
       "   355: '8123',\n",
       "   356: '8124',\n",
       "   357: '8125',\n",
       "   358: '8131',\n",
       "   359: '8132',\n",
       "   360: '8139',\n",
       "   361: '8141',\n",
       "   362: '8142',\n",
       "   363: '8143',\n",
       "   364: '8150',\n",
       "   365: '8160',\n",
       "   366: '8170',\n",
       "   367: '8181',\n",
       "   368: '8182',\n",
       "   369: '8183',\n",
       "   370: '8184',\n",
       "   371: '8189',\n",
       "   372: '8200',\n",
       "   373: '8211',\n",
       "   374: '8212',\n",
       "   375: '8213',\n",
       "   376: '8219',\n",
       "   377: '8300',\n",
       "   378: '8311',\n",
       "   379: '8312',\n",
       "   380: '8321',\n",
       "   381: '8322',\n",
       "   382: '8331',\n",
       "   383: '8332',\n",
       "   384: '8342',\n",
       "   385: '8343',\n",
       "   386: '8344',\n",
       "   387: '8349',\n",
       "   388: '8350',\n",
       "   389: '9100',\n",
       "   390: '9112',\n",
       "   391: '9113',\n",
       "   392: '9115',\n",
       "   393: '9116',\n",
       "   394: '9121',\n",
       "   395: '9122',\n",
       "   396: '9123',\n",
       "   397: '9129',\n",
       "   398: '9130',\n",
       "   399: '9214',\n",
       "   400: '9219',\n",
       "   401: '9310',\n",
       "   402: '9320',\n",
       "   403: '9331',\n",
       "   404: '9333',\n",
       "   405: '9410',\n",
       "   406: '9600',\n",
       "   407: '9611',\n",
       "   408: '9621',\n",
       "   409: '9625',\n",
       "   410: '9626',\n",
       "   411: '9627',\n",
       "   412: '9629'}},\n",
       " 'SSOC_5D': {'ssoc_idx': {'11110': 0,\n",
       "   '11121': 1,\n",
       "   '11122': 2,\n",
       "   '11140': 3,\n",
       "   '11150': 4,\n",
       "   '11160': 5,\n",
       "   '11201': 6,\n",
       "   '11202': 7,\n",
       "   '11203': 8,\n",
       "   '12111': 9,\n",
       "   '12112': 10,\n",
       "   '12113': 11,\n",
       "   '12121': 12,\n",
       "   '12122': 13,\n",
       "   '12123': 14,\n",
       "   '12131': 15,\n",
       "   '12132': 16,\n",
       "   '12133': 17,\n",
       "   '12191': 18,\n",
       "   '12192': 19,\n",
       "   '12193': 20,\n",
       "   '12194': 21,\n",
       "   '12195': 22,\n",
       "   '12199': 23,\n",
       "   '12211': 24,\n",
       "   '12212': 25,\n",
       "   '12213': 26,\n",
       "   '12214': 27,\n",
       "   '12215': 28,\n",
       "   '12221': 29,\n",
       "   '12222': 30,\n",
       "   '12230': 31,\n",
       "   '12241': 32,\n",
       "   '12242': 33,\n",
       "   '13100': 34,\n",
       "   '13210': 35,\n",
       "   '13230': 36,\n",
       "   '13241': 37,\n",
       "   '13242': 38,\n",
       "   '13243': 39,\n",
       "   '13244': 40,\n",
       "   '13245': 41,\n",
       "   '13291': 42,\n",
       "   '13292': 43,\n",
       "   '13299': 44,\n",
       "   '13301': 45,\n",
       "   '13302': 46,\n",
       "   '13303': 47,\n",
       "   '13304': 48,\n",
       "   '13410': 49,\n",
       "   '13420': 50,\n",
       "   '13430': 51,\n",
       "   '13441': 52,\n",
       "   '13442': 53,\n",
       "   '13451': 54,\n",
       "   '13459': 55,\n",
       "   '13461': 56,\n",
       "   '13462': 57,\n",
       "   '13463': 58,\n",
       "   '13491': 59,\n",
       "   '13492': 60,\n",
       "   '13493': 61,\n",
       "   '13499': 62,\n",
       "   '14110': 63,\n",
       "   '14121': 64,\n",
       "   '14122': 65,\n",
       "   '14123': 66,\n",
       "   '14201': 67,\n",
       "   '14202': 68,\n",
       "   '14310': 69,\n",
       "   '14321': 70,\n",
       "   '14322': 71,\n",
       "   '14323': 72,\n",
       "   '14324': 73,\n",
       "   '14325': 74,\n",
       "   '14329': 75,\n",
       "   '14330': 76,\n",
       "   '14391': 77,\n",
       "   '14392': 78,\n",
       "   '14399': 79,\n",
       "   '21110': 80,\n",
       "   '21120': 81,\n",
       "   '21130': 82,\n",
       "   '21141': 83,\n",
       "   '21142': 84,\n",
       "   '21149': 85,\n",
       "   '21211': 86,\n",
       "   '21212': 87,\n",
       "   '21213': 88,\n",
       "   '21221': 89,\n",
       "   '21222': 90,\n",
       "   '21231': 91,\n",
       "   '21239': 92,\n",
       "   '21311': 93,\n",
       "   '21312': 94,\n",
       "   '21319': 95,\n",
       "   '21321': 96,\n",
       "   '21329': 97,\n",
       "   '21331': 98,\n",
       "   '21332': 99,\n",
       "   '21339': 100,\n",
       "   '21341': 101,\n",
       "   '21342': 102,\n",
       "   '21343': 103,\n",
       "   '21344': 104,\n",
       "   '21345': 105,\n",
       "   '21346': 106,\n",
       "   '21347': 107,\n",
       "   '21349': 108,\n",
       "   '21411': 109,\n",
       "   '21412': 110,\n",
       "   '21413': 111,\n",
       "   '21414': 112,\n",
       "   '21415': 113,\n",
       "   '21421': 114,\n",
       "   '21422': 115,\n",
       "   '21430': 116,\n",
       "   '21441': 117,\n",
       "   '21442': 118,\n",
       "   '21443': 119,\n",
       "   '21444': 120,\n",
       "   '21451': 121,\n",
       "   '21452': 122,\n",
       "   '21453': 123,\n",
       "   '21454': 124,\n",
       "   '21460': 125,\n",
       "   '21471': 126,\n",
       "   '21472': 127,\n",
       "   '21473': 128,\n",
       "   '21474': 129,\n",
       "   '21475': 130,\n",
       "   '21491': 131,\n",
       "   '21492': 132,\n",
       "   '21493': 133,\n",
       "   '21494': 134,\n",
       "   '21495': 135,\n",
       "   '21496': 136,\n",
       "   '21497': 137,\n",
       "   '21498': 138,\n",
       "   '21499': 139,\n",
       "   '21511': 140,\n",
       "   '21512': 141,\n",
       "   '21513': 142,\n",
       "   '21521': 143,\n",
       "   '21522': 144,\n",
       "   '21523': 145,\n",
       "   '21524': 146,\n",
       "   '21525': 147,\n",
       "   '21526': 148,\n",
       "   '21531': 149,\n",
       "   '21532': 150,\n",
       "   '21610': 151,\n",
       "   '21621': 152,\n",
       "   '21622': 153,\n",
       "   '21631': 154,\n",
       "   '21632': 155,\n",
       "   '21641': 156,\n",
       "   '21649': 157,\n",
       "   '21651': 158,\n",
       "   '21652': 159,\n",
       "   '21659': 160,\n",
       "   '21661': 161,\n",
       "   '21662': 162,\n",
       "   '21663': 163,\n",
       "   '21664': 164,\n",
       "   '21669': 165,\n",
       "   '21670': 166,\n",
       "   '21711': 167,\n",
       "   '21712': 168,\n",
       "   '21713': 169,\n",
       "   '21714': 170,\n",
       "   '21721': 171,\n",
       "   '21722': 172,\n",
       "   '21723': 173,\n",
       "   '21724': 174,\n",
       "   '21729': 175,\n",
       "   '22110': 176,\n",
       "   '22121': 177,\n",
       "   '22122': 178,\n",
       "   '22123': 179,\n",
       "   '22124': 180,\n",
       "   '22125': 181,\n",
       "   '22126': 182,\n",
       "   '22127': 183,\n",
       "   '22128': 184,\n",
       "   '22129': 185,\n",
       "   '22131': 186,\n",
       "   '22132': 187,\n",
       "   '22133': 188,\n",
       "   '22134': 189,\n",
       "   '22135': 190,\n",
       "   '22136': 191,\n",
       "   '22137': 192,\n",
       "   '22138': 193,\n",
       "   '22139': 194,\n",
       "   '22141': 195,\n",
       "   '22142': 196,\n",
       "   '22143': 197,\n",
       "   '22144': 198,\n",
       "   '22145': 199,\n",
       "   '22146': 200,\n",
       "   '22147': 201,\n",
       "   '22148': 202,\n",
       "   '22150': 203,\n",
       "   '22200': 204,\n",
       "   '22301': 205,\n",
       "   '22302': 206,\n",
       "   '22500': 207,\n",
       "   '22611': 208,\n",
       "   '22612': 209,\n",
       "   '22621': 210,\n",
       "   '22629': 211,\n",
       "   '22631': 212,\n",
       "   '22632': 213,\n",
       "   '22639': 214,\n",
       "   '22640': 215,\n",
       "   '22651': 216,\n",
       "   '22652': 217,\n",
       "   '22661': 218,\n",
       "   '22662': 219,\n",
       "   '22670': 220,\n",
       "   '22680': 221,\n",
       "   '22691': 222,\n",
       "   '22692': 223,\n",
       "   '22693': 224,\n",
       "   '22694': 225,\n",
       "   '22699': 226,\n",
       "   '23101': 227,\n",
       "   '23102': 228,\n",
       "   '23103': 229,\n",
       "   '23109': 230,\n",
       "   '23200': 231,\n",
       "   '23300': 232,\n",
       "   '23400': 233,\n",
       "   '23500': 234,\n",
       "   '23611': 235,\n",
       "   '23612': 236,\n",
       "   '23619': 237,\n",
       "   '23621': 238,\n",
       "   '23622': 239,\n",
       "   '23629': 240,\n",
       "   '23690': 241,\n",
       "   '24111': 242,\n",
       "   '24112': 243,\n",
       "   '24113': 244,\n",
       "   '24121': 245,\n",
       "   '24122': 246,\n",
       "   '24131': 247,\n",
       "   '24132': 248,\n",
       "   '24133': 249,\n",
       "   '24134': 250,\n",
       "   '24135': 251,\n",
       "   '24139': 252,\n",
       "   '24141': 253,\n",
       "   '24142': 254,\n",
       "   '24143': 255,\n",
       "   '24149': 256,\n",
       "   '24151': 257,\n",
       "   '24152': 258,\n",
       "   '24153': 259,\n",
       "   '24154': 260,\n",
       "   '24159': 261,\n",
       "   '24160': 262,\n",
       "   '24211': 263,\n",
       "   '24212': 264,\n",
       "   '24213': 265,\n",
       "   '24220': 266,\n",
       "   '24231': 267,\n",
       "   '24232': 268,\n",
       "   '24233': 269,\n",
       "   '24234': 270,\n",
       "   '24240': 271,\n",
       "   '24251': 272,\n",
       "   '24252': 273,\n",
       "   '24291': 274,\n",
       "   '24299': 275,\n",
       "   '24311': 276,\n",
       "   '24312': 277,\n",
       "   '24313': 278,\n",
       "   '24314': 279,\n",
       "   '24315': 280,\n",
       "   '24320': 281,\n",
       "   '24331': 282,\n",
       "   '24332': 283,\n",
       "   '24333': 284,\n",
       "   '24334': 285,\n",
       "   '24339': 286,\n",
       "   '24351': 287,\n",
       "   '24352': 288,\n",
       "   '24353': 289,\n",
       "   '24361': 290,\n",
       "   '24362': 291,\n",
       "   '25111': 292,\n",
       "   '25112': 293,\n",
       "   '25113': 294,\n",
       "   '25121': 295,\n",
       "   '25122': 296,\n",
       "   '25123': 297,\n",
       "   '25140': 298,\n",
       "   '25151': 299,\n",
       "   '25152': 300,\n",
       "   '25190': 301,\n",
       "   '25211': 302,\n",
       "   '25212': 303,\n",
       "   '25220': 304,\n",
       "   '25231': 305,\n",
       "   '25232': 306,\n",
       "   '25239': 307,\n",
       "   '25241': 308,\n",
       "   '25242': 309,\n",
       "   '25243': 310,\n",
       "   '25244': 311,\n",
       "   '25245': 312,\n",
       "   '25249': 313,\n",
       "   '25291': 314,\n",
       "   '25299': 315,\n",
       "   '26111': 316,\n",
       "   '26112': 317,\n",
       "   '26119': 318,\n",
       "   '26120': 319,\n",
       "   '26191': 320,\n",
       "   '26199': 321,\n",
       "   '26211': 322,\n",
       "   '26212': 323,\n",
       "   '26213': 324,\n",
       "   '26221': 325,\n",
       "   '26229': 326,\n",
       "   '26310': 327,\n",
       "   '26321': 328,\n",
       "   '26322': 329,\n",
       "   '26331': 330,\n",
       "   '26339': 331,\n",
       "   '26341': 332,\n",
       "   '26342': 333,\n",
       "   '26343': 334,\n",
       "   '26349': 335,\n",
       "   '26351': 336,\n",
       "   '26352': 337,\n",
       "   '26353': 338,\n",
       "   '26359': 339,\n",
       "   '26361': 340,\n",
       "   '26369': 341,\n",
       "   '26371': 342,\n",
       "   '26372': 343,\n",
       "   '26373': 344,\n",
       "   '26374': 345,\n",
       "   '26375': 346,\n",
       "   '26379': 347,\n",
       "   '26411': 348,\n",
       "   '26412': 349,\n",
       "   '26413': 350,\n",
       "   '26414': 351,\n",
       "   '26415': 352,\n",
       "   '26419': 353,\n",
       "   '26421': 354,\n",
       "   '26422': 355,\n",
       "   '26431': 356,\n",
       "   '26432': 357,\n",
       "   '26511': 358,\n",
       "   '26519': 359,\n",
       "   '26521': 360,\n",
       "   '26522': 361,\n",
       "   '26523': 362,\n",
       "   '26524': 363,\n",
       "   '26529': 364,\n",
       "   '26530': 365,\n",
       "   '26541': 366,\n",
       "   '26542': 367,\n",
       "   '26543': 368,\n",
       "   '26544': 369,\n",
       "   '26549': 370,\n",
       "   '26550': 371,\n",
       "   '26561': 372,\n",
       "   '26569': 373,\n",
       "   '31001': 374,\n",
       "   '31002': 375,\n",
       "   '31003': 376,\n",
       "   '31004': 377,\n",
       "   '31005': 378,\n",
       "   '31006': 379,\n",
       "   '31009': 380,\n",
       "   '31111': 381,\n",
       "   '31112': 382,\n",
       "   '31119': 383,\n",
       "   '31121': 384,\n",
       "   '31122': 385,\n",
       "   '31123': 386,\n",
       "   '31124': 387,\n",
       "   '31129': 388,\n",
       "   '31131': 389,\n",
       "   '31132': 390,\n",
       "   '31141': 391,\n",
       "   '31142': 392,\n",
       "   '31143': 393,\n",
       "   '31144': 394,\n",
       "   '31151': 395,\n",
       "   '31152': 396,\n",
       "   '31153': 397,\n",
       "   '31161': 398,\n",
       "   '31162': 399,\n",
       "   '31163': 400,\n",
       "   '31164': 401,\n",
       "   '31171': 402,\n",
       "   '31172': 403,\n",
       "   '31173': 404,\n",
       "   '31174': 405,\n",
       "   '31175': 406,\n",
       "   '31181': 407,\n",
       "   '31182': 408,\n",
       "   '31183': 409,\n",
       "   '31184': 410,\n",
       "   '31185': 411,\n",
       "   '31189': 412,\n",
       "   '31211': 413,\n",
       "   '31212': 414,\n",
       "   '31213': 415,\n",
       "   '31214': 416,\n",
       "   '31291': 417,\n",
       "   '31292': 418,\n",
       "   '31293': 419,\n",
       "   '31294': 420,\n",
       "   '31295': 421,\n",
       "   '31299': 422,\n",
       "   '31310': 423,\n",
       "   '31321': 424,\n",
       "   '31322': 425,\n",
       "   '31323': 426,\n",
       "   '31324': 427,\n",
       "   '31325': 428,\n",
       "   '31329': 429,\n",
       "   '31331': 430,\n",
       "   '31332': 431,\n",
       "   '31341': 432,\n",
       "   '31342': 433,\n",
       "   '31350': 434,\n",
       "   '31391': 435,\n",
       "   '31392': 436,\n",
       "   '31399': 437,\n",
       "   '31411': 438,\n",
       "   '31412': 439,\n",
       "   '31419': 440,\n",
       "   '31421': 441,\n",
       "   '31422': 442,\n",
       "   '31423': 443,\n",
       "   '31510': 444,\n",
       "   '31521': 445,\n",
       "   '31522': 446,\n",
       "   '31529': 447,\n",
       "   '31540': 448,\n",
       "   '31560': 449,\n",
       "   '31571': 450,\n",
       "   '31572': 451,\n",
       "   '31573': 452,\n",
       "   '31574': 453,\n",
       "   '31579': 454,\n",
       "   '31591': 455,\n",
       "   '31592': 456,\n",
       "   '31593': 457,\n",
       "   '31594': 458,\n",
       "   '31595': 459,\n",
       "   '31596': 460,\n",
       "   '31597': 461,\n",
       "   '31599': 462,\n",
       "   '31601': 463,\n",
       "   '31602': 464,\n",
       "   '31603': 465,\n",
       "   '31711': 466,\n",
       "   '31719': 467,\n",
       "   '31720': 468,\n",
       "   '32111': 469,\n",
       "   '32112': 470,\n",
       "   '32119': 471,\n",
       "   '32120': 472,\n",
       "   '32130': 473,\n",
       "   '32141': 474,\n",
       "   '32142': 475,\n",
       "   '32143': 476,\n",
       "   '32200': 477,\n",
       "   '32300': 478,\n",
       "   '32400': 479,\n",
       "   '32510': 480,\n",
       "   '32530': 481,\n",
       "   '32540': 482,\n",
       "   '32551': 483,\n",
       "   '32559': 484,\n",
       "   '32571': 485,\n",
       "   '32572': 486,\n",
       "   '32591': 487,\n",
       "   '32599': 488,\n",
       "   '33121': 489,\n",
       "   '33129': 490,\n",
       "   '33131': 491,\n",
       "   '33132': 492,\n",
       "   '33133': 493,\n",
       "   '33151': 494,\n",
       "   '33152': 495,\n",
       "   '33153': 496,\n",
       "   '33211': 497,\n",
       "   '33219': 498,\n",
       "   '33221': 499,\n",
       "   '33222': 500,\n",
       "   '33223': 501,\n",
       "   '33224': 502,\n",
       "   '33225': 503,\n",
       "   '33229': 504,\n",
       "   '33231': 505,\n",
       "   '33232': 506,\n",
       "   '33291': 507,\n",
       "   '33299': 508,\n",
       "   '33311': 509,\n",
       "   '33312': 510,\n",
       "   '33313': 511,\n",
       "   '33320': 512,\n",
       "   '33330': 513,\n",
       "   '33340': 514,\n",
       "   '33391': 515,\n",
       "   '33392': 516,\n",
       "   '33393': 517,\n",
       "   '33394': 518,\n",
       "   '33399': 519,\n",
       "   '33461': 520,\n",
       "   '33462': 521,\n",
       "   '33491': 522,\n",
       "   '33492': 523,\n",
       "   '33493': 524,\n",
       "   '33499': 525,\n",
       "   '33510': 526,\n",
       "   '33551': 527,\n",
       "   '33552': 528,\n",
       "   '33591': 529,\n",
       "   '33592': 530,\n",
       "   '33593': 531,\n",
       "   '33599': 532,\n",
       "   '33611': 533,\n",
       "   '33612': 534,\n",
       "   '33613': 535,\n",
       "   '33614': 536,\n",
       "   '33619': 537,\n",
       "   '34110': 538,\n",
       "   '34121': 539,\n",
       "   '34122': 540,\n",
       "   '34123': 541,\n",
       "   '34210': 542,\n",
       "   '34221': 543,\n",
       "   '34222': 544,\n",
       "   '34223': 545,\n",
       "   '34224': 546,\n",
       "   '34229': 547,\n",
       "   '34310': 548,\n",
       "   '34321': 549,\n",
       "   '34322': 550,\n",
       "   '34323': 551,\n",
       "   '34331': 552,\n",
       "   '34332': 553,\n",
       "   '34341': 554,\n",
       "   '34342': 555,\n",
       "   '34343': 556,\n",
       "   '34391': 557,\n",
       "   '34399': 558,\n",
       "   '34401': 559,\n",
       "   '34409': 560,\n",
       "   '35110': 561,\n",
       "   '35121': 562,\n",
       "   '35122': 563,\n",
       "   '35123': 564,\n",
       "   '35129': 565,\n",
       "   '35140': 566,\n",
       "   '35211': 567,\n",
       "   '35212': 568,\n",
       "   '35213': 569,\n",
       "   '35214': 570,\n",
       "   '35215': 571,\n",
       "   '35219': 572,\n",
       "   '35220': 573,\n",
       "   '35231': 574,\n",
       "   '35232': 575,\n",
       "   '35239': 576,\n",
       "   '35290': 577,\n",
       "   '36100': 578,\n",
       "   '36201': 579,\n",
       "   '36202': 580,\n",
       "   '36203': 581,\n",
       "   '36204': 582,\n",
       "   '36205': 583,\n",
       "   '36206': 584,\n",
       "   '36209': 585,\n",
       "   '36910': 586,\n",
       "   '36991': 587,\n",
       "   '36999': 588,\n",
       "   '39910': 589,\n",
       "   '40000': 590,\n",
       "   '41101': 591,\n",
       "   '41102': 592,\n",
       "   '41109': 593,\n",
       "   '41201': 594,\n",
       "   '41202': 595,\n",
       "   '41310': 596,\n",
       "   '41320': 597,\n",
       "   '42111': 598,\n",
       "   '42112': 599,\n",
       "   '42113': 600,\n",
       "   '42119': 601,\n",
       "   '42131': 602,\n",
       "   '42132': 603,\n",
       "   '42141': 604,\n",
       "   '42149': 605,\n",
       "   '42210': 606,\n",
       "   '42230': 607,\n",
       "   '42241': 608,\n",
       "   '42242': 609,\n",
       "   '42243': 610,\n",
       "   '42244': 611,\n",
       "   '42245': 612,\n",
       "   '42246': 613,\n",
       "   '42247': 614,\n",
       "   '42249': 615,\n",
       "   '42290': 616,\n",
       "   '43111': 617,\n",
       "   '43112': 618,\n",
       "   '43113': 619,\n",
       "   '43114': 620,\n",
       "   '43115': 621,\n",
       "   '43116': 622,\n",
       "   '43119': 623,\n",
       "   '43121': 624,\n",
       "   '43122': 625,\n",
       "   '43123': 626,\n",
       "   '43129': 627,\n",
       "   '43141': 628,\n",
       "   '43142': 629,\n",
       "   '43151': 630,\n",
       "   '43159': 631,\n",
       "   '43211': 632,\n",
       "   '43212': 633,\n",
       "   '43219': 634,\n",
       "   '43221': 635,\n",
       "   '43222': 636,\n",
       "   '43229': 637,\n",
       "   '43231': 638,\n",
       "   '43232': 639,\n",
       "   '43233': 640,\n",
       "   '43239': 641,\n",
       "   '44110': 642,\n",
       "   '44121': 643,\n",
       "   '44122': 644,\n",
       "   '44123': 645,\n",
       "   '44129': 646,\n",
       "   '44170': 647,\n",
       "   '44191': 648,\n",
       "   '44199': 649,\n",
       "   '51111': 650,\n",
       "   '51112': 651,\n",
       "   '51121': 652,\n",
       "   '51122': 653,\n",
       "   '51129': 654,\n",
       "   '51131': 655,\n",
       "   '51132': 656,\n",
       "   '51201': 657,\n",
       "   '51202': 658,\n",
       "   '51311': 659,\n",
       "   '51312': 660,\n",
       "   '51313': 661,\n",
       "   '51321': 662,\n",
       "   '51322': 663,\n",
       "   '51330': 664,\n",
       "   '51390': 665,\n",
       "   '51411': 666,\n",
       "   '51412': 667,\n",
       "   '51419': 668,\n",
       "   '51421': 669,\n",
       "   '51422': 670,\n",
       "   '51423': 671,\n",
       "   '51491': 672,\n",
       "   '51492': 673,\n",
       "   '51499': 674,\n",
       "   '51501': 675,\n",
       "   '51502': 676,\n",
       "   '51503': 677,\n",
       "   '51504': 678,\n",
       "   '51505': 679,\n",
       "   '51509': 680,\n",
       "   '51701': 681,\n",
       "   '51702': 682,\n",
       "   '51910': 683,\n",
       "   '51931': 684,\n",
       "   '51932': 685,\n",
       "   '51941': 686,\n",
       "   '51942': 687,\n",
       "   '51943': 688,\n",
       "   '51944': 689,\n",
       "   '51949': 690,\n",
       "   '51950': 691,\n",
       "   '51991': 692,\n",
       "   '51999': 693,\n",
       "   '52110': 694,\n",
       "   '52120': 695,\n",
       "   '52130': 696,\n",
       "   '52190': 697,\n",
       "   '52201': 698,\n",
       "   '52202': 699,\n",
       "   '52301': 700,\n",
       "   '52302': 701,\n",
       "   '52303': 702,\n",
       "   '52309': 703,\n",
       "   '52411': 704,\n",
       "   '52419': 705,\n",
       "   '52421': 706,\n",
       "   '52422': 707,\n",
       "   '52440': 708,\n",
       "   '52491': 709,\n",
       "   '52492': 710,\n",
       "   '52499': 711,\n",
       "   '53111': 712,\n",
       "   '53112': 713,\n",
       "   '53113': 714,\n",
       "   '53114': 715,\n",
       "   '53115': 716,\n",
       "   '53120': 717,\n",
       "   '53201': 718,\n",
       "   '53202': 719,\n",
       "   '53203': 720,\n",
       "   '53209': 721,\n",
       "   '54110': 722,\n",
       "   '54121': 723,\n",
       "   '54122': 724,\n",
       "   '54123': 725,\n",
       "   '54130': 726,\n",
       "   '54141': 727,\n",
       "   '54142': 728,\n",
       "   '54143': 729,\n",
       "   '54144': 730,\n",
       "   '54150': 731,\n",
       "   '54191': 732,\n",
       "   '54192': 733,\n",
       "   '54193': 734,\n",
       "   '54194': 735,\n",
       "   '54199': 736,\n",
       "   '59000': 737,\n",
       "   '61110': 738,\n",
       "   '61131': 739,\n",
       "   '61132': 740,\n",
       "   '61133': 741,\n",
       "   '61210': 742,\n",
       "   '61221': 743,\n",
       "   '61222': 744,\n",
       "   '61900': 745,\n",
       "   '62211': 746,\n",
       "   '62212': 747,\n",
       "   '62219': 748,\n",
       "   '62220': 749,\n",
       "   '71000': 750,\n",
       "   '71120': 751,\n",
       "   '71130': 752,\n",
       "   '71141': 753,\n",
       "   '71142': 754,\n",
       "   '71149': 755,\n",
       "   '71151': 756,\n",
       "   '71152': 757,\n",
       "   '71191': 758,\n",
       "   '71192': 759,\n",
       "   '71199': 760,\n",
       "   '71210': 761,\n",
       "   '71220': 762,\n",
       "   '71230': 763,\n",
       "   '71241': 764,\n",
       "   '71242': 765,\n",
       "   '71249': 766,\n",
       "   '71250': 767,\n",
       "   '71261': 768,\n",
       "   '71262': 769,\n",
       "   '71263': 770,\n",
       "   '71271': 771,\n",
       "   '71272': 772,\n",
       "   '71290': 773,\n",
       "   '71311': 774,\n",
       "   '71312': 775,\n",
       "   '71321': 776,\n",
       "   '71322': 777,\n",
       "   '71323': 778,\n",
       "   '71324': 779,\n",
       "   '71329': 780,\n",
       "   '71331': 781,\n",
       "   '71332': 782,\n",
       "   '72000': 783,\n",
       "   '72110': 784,\n",
       "   '72120': 785,\n",
       "   '72130': 786,\n",
       "   '72140': 787,\n",
       "   '72150': 788,\n",
       "   '72210': 789,\n",
       "   '72221': 790,\n",
       "   '72222': 791,\n",
       "   '72229': 792,\n",
       "   '72240': 793,\n",
       "   '72310': 794,\n",
       "   '72320': 795,\n",
       "   '72330': 796,\n",
       "   '72340': 797,\n",
       "   '72350': 798,\n",
       "   '72391': 799,\n",
       "   '72392': 800,\n",
       "   '72399': 801,\n",
       "   '73000': 802,\n",
       "   '73111': 803,\n",
       "   '73112': 804,\n",
       "   '73113': 805,\n",
       "   '73119': 806,\n",
       "   '73120': 807,\n",
       "   '73130': 808,\n",
       "   '73140': 809,\n",
       "   '73150': 810,\n",
       "   '73160': 811,\n",
       "   '73210': 812,\n",
       "   '73220': 813,\n",
       "   '74001': 814,\n",
       "   '74002': 815,\n",
       "   '74110': 816,\n",
       "   '74121': 817,\n",
       "   '74122': 818,\n",
       "   '74123': 819,\n",
       "   '74131': 820,\n",
       "   '74132': 821,\n",
       "   '74133': 822,\n",
       "   '74211': 823,\n",
       "   '74212': 824,\n",
       "   '74221': 825,\n",
       "   '74222': 826,\n",
       "   '74223': 827,\n",
       "   '74224': 828,\n",
       "   '75000': 829,\n",
       "   '75110': 830,\n",
       "   '75121': 831,\n",
       "   '75122': 832,\n",
       "   '75150': 833,\n",
       "   '75190': 834,\n",
       "   '75210': 835,\n",
       "   '75220': 836,\n",
       "   '75290': 837,\n",
       "   '75310': 838,\n",
       "   '75320': 839,\n",
       "   '75340': 840,\n",
       "   '75361': 841,\n",
       "   '75362': 842,\n",
       "   '75363': 843,\n",
       "   '75369': 844,\n",
       "   '75390': 845,\n",
       "   '75410': 846,\n",
       "   '75440': 847,\n",
       "   '75490': 848,\n",
       "   '81000': 849,\n",
       "   '81130': 850,\n",
       "   '81140': 851,\n",
       "   '81210': 852,\n",
       "   '81220': 853,\n",
       "   '81230': 854,\n",
       "   '81240': 855,\n",
       "   '81251': 856,\n",
       "   '81252': 857,\n",
       "   '81259': 858,\n",
       "   '81311': 859,\n",
       "   '81312': 860,\n",
       "   '81320': 861,\n",
       "   '81390': 862,\n",
       "   '81410': 863,\n",
       "   '81420': 864,\n",
       "   '81430': 865,\n",
       "   '81501': 866,\n",
       "   '81502': 867,\n",
       "   '81509': 868,\n",
       "   '81601': 869,\n",
       "   '81602': 870,\n",
       "   '81603': 871,\n",
       "   '81604': 872,\n",
       "   '81609': 873,\n",
       "   '81700': 874,\n",
       "   '81811': 875,\n",
       "   '81812': 876,\n",
       "   '81819': 877,\n",
       "   '81821': 878,\n",
       "   '81822': 879,\n",
       "   '81829': 880,\n",
       "   '81830': 881,\n",
       "   '81841': 882,\n",
       "   '81842': 883,\n",
       "   '81849': 884,\n",
       "   '81890': 885,\n",
       "   '82000': 886,\n",
       "   '82110': 887,\n",
       "   '82121': 888,\n",
       "   '82122': 889,\n",
       "   '82123': 890,\n",
       "   '82131': 891,\n",
       "   '82132': 892,\n",
       "   '82139': 893,\n",
       "   '82190': 894,\n",
       "   '83000': 895,\n",
       "   '83110': 896,\n",
       "   '83121': 897,\n",
       "   '83129': 898,\n",
       "   '83211': 899,\n",
       "   '83212': 900,\n",
       "   '83221': 901,\n",
       "   '83222': 902,\n",
       "   '83223': 903,\n",
       "   '83224': 904,\n",
       "   '83225': 905,\n",
       "   '83226': 906,\n",
       "   '83229': 907,\n",
       "   '83311': 908,\n",
       "   '83312': 909,\n",
       "   '83321': 910,\n",
       "   '83322': 911,\n",
       "   '83323': 912,\n",
       "   '83324': 913,\n",
       "   '83329': 914,\n",
       "   '83421': 915,\n",
       "   '83422': 916,\n",
       "   '83423': 917,\n",
       "   '83424': 918,\n",
       "   '83429': 919,\n",
       "   '83431': 920,\n",
       "   '83432': 921,\n",
       "   '83439': 922,\n",
       "   '83441': 923,\n",
       "   '83449': 924,\n",
       "   '83491': 925,\n",
       "   '83492': 926,\n",
       "   '83499': 927,\n",
       "   '83501': 928,\n",
       "   '83502': 929,\n",
       "   '83509': 930,\n",
       "   '91000': 931,\n",
       "   '91121': 932,\n",
       "   '91122': 933,\n",
       "   '91129': 934,\n",
       "   '91131': 935,\n",
       "   '91132': 936,\n",
       "   '91133': 937,\n",
       "   '91151': 938,\n",
       "   '91152': 939,\n",
       "   '91153': 940,\n",
       "   '91154': 941,\n",
       "   '91161': 942,\n",
       "   '91162': 943,\n",
       "   '91210': 944,\n",
       "   '91220': 945,\n",
       "   '91230': 946,\n",
       "   '91291': 947,\n",
       "   '91292': 948,\n",
       "   '91293': 949,\n",
       "   '91299': 950,\n",
       "   '91300': 951,\n",
       "   '92141': 952,\n",
       "   '92142': 953,\n",
       "   '92149': 954,\n",
       "   '92190': 955,\n",
       "   '93100': 956,\n",
       "   '93201': 957,\n",
       "   '93209': 958,\n",
       "   '93310': 959,\n",
       "   '93331': 960,\n",
       "   '93332': 961,\n",
       "   '93333': 962,\n",
       "   '93334': 963,\n",
       "   '93335': 964,\n",
       "   '93336': 965,\n",
       "   '93337': 966,\n",
       "   '93339': 967,\n",
       "   '94101': 968,\n",
       "   '94102': 969,\n",
       "   '94103': 970,\n",
       "   '94104': 971,\n",
       "   '96000': 972,\n",
       "   '96111': 973,\n",
       "   '96112': 974,\n",
       "   '96113': 975,\n",
       "   '96119': 976,\n",
       "   '96211': 977,\n",
       "   '96212': 978,\n",
       "   '96213': 979,\n",
       "   '96251': 980,\n",
       "   '96252': 981,\n",
       "   '96253': 982,\n",
       "   '96254': 983,\n",
       "   '96255': 984,\n",
       "   '96256': 985,\n",
       "   '96257': 986,\n",
       "   '96259': 987,\n",
       "   '96261': 988,\n",
       "   '96262': 989,\n",
       "   '96269': 990,\n",
       "   '96271': 991,\n",
       "   '96272': 992,\n",
       "   '96291': 993,\n",
       "   '96292': 994,\n",
       "   '96293': 995,\n",
       "   '96299': 996},\n",
       "  'idx_ssoc': {0: '11110',\n",
       "   1: '11121',\n",
       "   2: '11122',\n",
       "   3: '11140',\n",
       "   4: '11150',\n",
       "   5: '11160',\n",
       "   6: '11201',\n",
       "   7: '11202',\n",
       "   8: '11203',\n",
       "   9: '12111',\n",
       "   10: '12112',\n",
       "   11: '12113',\n",
       "   12: '12121',\n",
       "   13: '12122',\n",
       "   14: '12123',\n",
       "   15: '12131',\n",
       "   16: '12132',\n",
       "   17: '12133',\n",
       "   18: '12191',\n",
       "   19: '12192',\n",
       "   20: '12193',\n",
       "   21: '12194',\n",
       "   22: '12195',\n",
       "   23: '12199',\n",
       "   24: '12211',\n",
       "   25: '12212',\n",
       "   26: '12213',\n",
       "   27: '12214',\n",
       "   28: '12215',\n",
       "   29: '12221',\n",
       "   30: '12222',\n",
       "   31: '12230',\n",
       "   32: '12241',\n",
       "   33: '12242',\n",
       "   34: '13100',\n",
       "   35: '13210',\n",
       "   36: '13230',\n",
       "   37: '13241',\n",
       "   38: '13242',\n",
       "   39: '13243',\n",
       "   40: '13244',\n",
       "   41: '13245',\n",
       "   42: '13291',\n",
       "   43: '13292',\n",
       "   44: '13299',\n",
       "   45: '13301',\n",
       "   46: '13302',\n",
       "   47: '13303',\n",
       "   48: '13304',\n",
       "   49: '13410',\n",
       "   50: '13420',\n",
       "   51: '13430',\n",
       "   52: '13441',\n",
       "   53: '13442',\n",
       "   54: '13451',\n",
       "   55: '13459',\n",
       "   56: '13461',\n",
       "   57: '13462',\n",
       "   58: '13463',\n",
       "   59: '13491',\n",
       "   60: '13492',\n",
       "   61: '13493',\n",
       "   62: '13499',\n",
       "   63: '14110',\n",
       "   64: '14121',\n",
       "   65: '14122',\n",
       "   66: '14123',\n",
       "   67: '14201',\n",
       "   68: '14202',\n",
       "   69: '14310',\n",
       "   70: '14321',\n",
       "   71: '14322',\n",
       "   72: '14323',\n",
       "   73: '14324',\n",
       "   74: '14325',\n",
       "   75: '14329',\n",
       "   76: '14330',\n",
       "   77: '14391',\n",
       "   78: '14392',\n",
       "   79: '14399',\n",
       "   80: '21110',\n",
       "   81: '21120',\n",
       "   82: '21130',\n",
       "   83: '21141',\n",
       "   84: '21142',\n",
       "   85: '21149',\n",
       "   86: '21211',\n",
       "   87: '21212',\n",
       "   88: '21213',\n",
       "   89: '21221',\n",
       "   90: '21222',\n",
       "   91: '21231',\n",
       "   92: '21239',\n",
       "   93: '21311',\n",
       "   94: '21312',\n",
       "   95: '21319',\n",
       "   96: '21321',\n",
       "   97: '21329',\n",
       "   98: '21331',\n",
       "   99: '21332',\n",
       "   100: '21339',\n",
       "   101: '21341',\n",
       "   102: '21342',\n",
       "   103: '21343',\n",
       "   104: '21344',\n",
       "   105: '21345',\n",
       "   106: '21346',\n",
       "   107: '21347',\n",
       "   108: '21349',\n",
       "   109: '21411',\n",
       "   110: '21412',\n",
       "   111: '21413',\n",
       "   112: '21414',\n",
       "   113: '21415',\n",
       "   114: '21421',\n",
       "   115: '21422',\n",
       "   116: '21430',\n",
       "   117: '21441',\n",
       "   118: '21442',\n",
       "   119: '21443',\n",
       "   120: '21444',\n",
       "   121: '21451',\n",
       "   122: '21452',\n",
       "   123: '21453',\n",
       "   124: '21454',\n",
       "   125: '21460',\n",
       "   126: '21471',\n",
       "   127: '21472',\n",
       "   128: '21473',\n",
       "   129: '21474',\n",
       "   130: '21475',\n",
       "   131: '21491',\n",
       "   132: '21492',\n",
       "   133: '21493',\n",
       "   134: '21494',\n",
       "   135: '21495',\n",
       "   136: '21496',\n",
       "   137: '21497',\n",
       "   138: '21498',\n",
       "   139: '21499',\n",
       "   140: '21511',\n",
       "   141: '21512',\n",
       "   142: '21513',\n",
       "   143: '21521',\n",
       "   144: '21522',\n",
       "   145: '21523',\n",
       "   146: '21524',\n",
       "   147: '21525',\n",
       "   148: '21526',\n",
       "   149: '21531',\n",
       "   150: '21532',\n",
       "   151: '21610',\n",
       "   152: '21621',\n",
       "   153: '21622',\n",
       "   154: '21631',\n",
       "   155: '21632',\n",
       "   156: '21641',\n",
       "   157: '21649',\n",
       "   158: '21651',\n",
       "   159: '21652',\n",
       "   160: '21659',\n",
       "   161: '21661',\n",
       "   162: '21662',\n",
       "   163: '21663',\n",
       "   164: '21664',\n",
       "   165: '21669',\n",
       "   166: '21670',\n",
       "   167: '21711',\n",
       "   168: '21712',\n",
       "   169: '21713',\n",
       "   170: '21714',\n",
       "   171: '21721',\n",
       "   172: '21722',\n",
       "   173: '21723',\n",
       "   174: '21724',\n",
       "   175: '21729',\n",
       "   176: '22110',\n",
       "   177: '22121',\n",
       "   178: '22122',\n",
       "   179: '22123',\n",
       "   180: '22124',\n",
       "   181: '22125',\n",
       "   182: '22126',\n",
       "   183: '22127',\n",
       "   184: '22128',\n",
       "   185: '22129',\n",
       "   186: '22131',\n",
       "   187: '22132',\n",
       "   188: '22133',\n",
       "   189: '22134',\n",
       "   190: '22135',\n",
       "   191: '22136',\n",
       "   192: '22137',\n",
       "   193: '22138',\n",
       "   194: '22139',\n",
       "   195: '22141',\n",
       "   196: '22142',\n",
       "   197: '22143',\n",
       "   198: '22144',\n",
       "   199: '22145',\n",
       "   200: '22146',\n",
       "   201: '22147',\n",
       "   202: '22148',\n",
       "   203: '22150',\n",
       "   204: '22200',\n",
       "   205: '22301',\n",
       "   206: '22302',\n",
       "   207: '22500',\n",
       "   208: '22611',\n",
       "   209: '22612',\n",
       "   210: '22621',\n",
       "   211: '22629',\n",
       "   212: '22631',\n",
       "   213: '22632',\n",
       "   214: '22639',\n",
       "   215: '22640',\n",
       "   216: '22651',\n",
       "   217: '22652',\n",
       "   218: '22661',\n",
       "   219: '22662',\n",
       "   220: '22670',\n",
       "   221: '22680',\n",
       "   222: '22691',\n",
       "   223: '22692',\n",
       "   224: '22693',\n",
       "   225: '22694',\n",
       "   226: '22699',\n",
       "   227: '23101',\n",
       "   228: '23102',\n",
       "   229: '23103',\n",
       "   230: '23109',\n",
       "   231: '23200',\n",
       "   232: '23300',\n",
       "   233: '23400',\n",
       "   234: '23500',\n",
       "   235: '23611',\n",
       "   236: '23612',\n",
       "   237: '23619',\n",
       "   238: '23621',\n",
       "   239: '23622',\n",
       "   240: '23629',\n",
       "   241: '23690',\n",
       "   242: '24111',\n",
       "   243: '24112',\n",
       "   244: '24113',\n",
       "   245: '24121',\n",
       "   246: '24122',\n",
       "   247: '24131',\n",
       "   248: '24132',\n",
       "   249: '24133',\n",
       "   250: '24134',\n",
       "   251: '24135',\n",
       "   252: '24139',\n",
       "   253: '24141',\n",
       "   254: '24142',\n",
       "   255: '24143',\n",
       "   256: '24149',\n",
       "   257: '24151',\n",
       "   258: '24152',\n",
       "   259: '24153',\n",
       "   260: '24154',\n",
       "   261: '24159',\n",
       "   262: '24160',\n",
       "   263: '24211',\n",
       "   264: '24212',\n",
       "   265: '24213',\n",
       "   266: '24220',\n",
       "   267: '24231',\n",
       "   268: '24232',\n",
       "   269: '24233',\n",
       "   270: '24234',\n",
       "   271: '24240',\n",
       "   272: '24251',\n",
       "   273: '24252',\n",
       "   274: '24291',\n",
       "   275: '24299',\n",
       "   276: '24311',\n",
       "   277: '24312',\n",
       "   278: '24313',\n",
       "   279: '24314',\n",
       "   280: '24315',\n",
       "   281: '24320',\n",
       "   282: '24331',\n",
       "   283: '24332',\n",
       "   284: '24333',\n",
       "   285: '24334',\n",
       "   286: '24339',\n",
       "   287: '24351',\n",
       "   288: '24352',\n",
       "   289: '24353',\n",
       "   290: '24361',\n",
       "   291: '24362',\n",
       "   292: '25111',\n",
       "   293: '25112',\n",
       "   294: '25113',\n",
       "   295: '25121',\n",
       "   296: '25122',\n",
       "   297: '25123',\n",
       "   298: '25140',\n",
       "   299: '25151',\n",
       "   300: '25152',\n",
       "   301: '25190',\n",
       "   302: '25211',\n",
       "   303: '25212',\n",
       "   304: '25220',\n",
       "   305: '25231',\n",
       "   306: '25232',\n",
       "   307: '25239',\n",
       "   308: '25241',\n",
       "   309: '25242',\n",
       "   310: '25243',\n",
       "   311: '25244',\n",
       "   312: '25245',\n",
       "   313: '25249',\n",
       "   314: '25291',\n",
       "   315: '25299',\n",
       "   316: '26111',\n",
       "   317: '26112',\n",
       "   318: '26119',\n",
       "   319: '26120',\n",
       "   320: '26191',\n",
       "   321: '26199',\n",
       "   322: '26211',\n",
       "   323: '26212',\n",
       "   324: '26213',\n",
       "   325: '26221',\n",
       "   326: '26229',\n",
       "   327: '26310',\n",
       "   328: '26321',\n",
       "   329: '26322',\n",
       "   330: '26331',\n",
       "   331: '26339',\n",
       "   332: '26341',\n",
       "   333: '26342',\n",
       "   334: '26343',\n",
       "   335: '26349',\n",
       "   336: '26351',\n",
       "   337: '26352',\n",
       "   338: '26353',\n",
       "   339: '26359',\n",
       "   340: '26361',\n",
       "   341: '26369',\n",
       "   342: '26371',\n",
       "   343: '26372',\n",
       "   344: '26373',\n",
       "   345: '26374',\n",
       "   346: '26375',\n",
       "   347: '26379',\n",
       "   348: '26411',\n",
       "   349: '26412',\n",
       "   350: '26413',\n",
       "   351: '26414',\n",
       "   352: '26415',\n",
       "   353: '26419',\n",
       "   354: '26421',\n",
       "   355: '26422',\n",
       "   356: '26431',\n",
       "   357: '26432',\n",
       "   358: '26511',\n",
       "   359: '26519',\n",
       "   360: '26521',\n",
       "   361: '26522',\n",
       "   362: '26523',\n",
       "   363: '26524',\n",
       "   364: '26529',\n",
       "   365: '26530',\n",
       "   366: '26541',\n",
       "   367: '26542',\n",
       "   368: '26543',\n",
       "   369: '26544',\n",
       "   370: '26549',\n",
       "   371: '26550',\n",
       "   372: '26561',\n",
       "   373: '26569',\n",
       "   374: '31001',\n",
       "   375: '31002',\n",
       "   376: '31003',\n",
       "   377: '31004',\n",
       "   378: '31005',\n",
       "   379: '31006',\n",
       "   380: '31009',\n",
       "   381: '31111',\n",
       "   382: '31112',\n",
       "   383: '31119',\n",
       "   384: '31121',\n",
       "   385: '31122',\n",
       "   386: '31123',\n",
       "   387: '31124',\n",
       "   388: '31129',\n",
       "   389: '31131',\n",
       "   390: '31132',\n",
       "   391: '31141',\n",
       "   392: '31142',\n",
       "   393: '31143',\n",
       "   394: '31144',\n",
       "   395: '31151',\n",
       "   396: '31152',\n",
       "   397: '31153',\n",
       "   398: '31161',\n",
       "   399: '31162',\n",
       "   400: '31163',\n",
       "   401: '31164',\n",
       "   402: '31171',\n",
       "   403: '31172',\n",
       "   404: '31173',\n",
       "   405: '31174',\n",
       "   406: '31175',\n",
       "   407: '31181',\n",
       "   408: '31182',\n",
       "   409: '31183',\n",
       "   410: '31184',\n",
       "   411: '31185',\n",
       "   412: '31189',\n",
       "   413: '31211',\n",
       "   414: '31212',\n",
       "   415: '31213',\n",
       "   416: '31214',\n",
       "   417: '31291',\n",
       "   418: '31292',\n",
       "   419: '31293',\n",
       "   420: '31294',\n",
       "   421: '31295',\n",
       "   422: '31299',\n",
       "   423: '31310',\n",
       "   424: '31321',\n",
       "   425: '31322',\n",
       "   426: '31323',\n",
       "   427: '31324',\n",
       "   428: '31325',\n",
       "   429: '31329',\n",
       "   430: '31331',\n",
       "   431: '31332',\n",
       "   432: '31341',\n",
       "   433: '31342',\n",
       "   434: '31350',\n",
       "   435: '31391',\n",
       "   436: '31392',\n",
       "   437: '31399',\n",
       "   438: '31411',\n",
       "   439: '31412',\n",
       "   440: '31419',\n",
       "   441: '31421',\n",
       "   442: '31422',\n",
       "   443: '31423',\n",
       "   444: '31510',\n",
       "   445: '31521',\n",
       "   446: '31522',\n",
       "   447: '31529',\n",
       "   448: '31540',\n",
       "   449: '31560',\n",
       "   450: '31571',\n",
       "   451: '31572',\n",
       "   452: '31573',\n",
       "   453: '31574',\n",
       "   454: '31579',\n",
       "   455: '31591',\n",
       "   456: '31592',\n",
       "   457: '31593',\n",
       "   458: '31594',\n",
       "   459: '31595',\n",
       "   460: '31596',\n",
       "   461: '31597',\n",
       "   462: '31599',\n",
       "   463: '31601',\n",
       "   464: '31602',\n",
       "   465: '31603',\n",
       "   466: '31711',\n",
       "   467: '31719',\n",
       "   468: '31720',\n",
       "   469: '32111',\n",
       "   470: '32112',\n",
       "   471: '32119',\n",
       "   472: '32120',\n",
       "   473: '32130',\n",
       "   474: '32141',\n",
       "   475: '32142',\n",
       "   476: '32143',\n",
       "   477: '32200',\n",
       "   478: '32300',\n",
       "   479: '32400',\n",
       "   480: '32510',\n",
       "   481: '32530',\n",
       "   482: '32540',\n",
       "   483: '32551',\n",
       "   484: '32559',\n",
       "   485: '32571',\n",
       "   486: '32572',\n",
       "   487: '32591',\n",
       "   488: '32599',\n",
       "   489: '33121',\n",
       "   490: '33129',\n",
       "   491: '33131',\n",
       "   492: '33132',\n",
       "   493: '33133',\n",
       "   494: '33151',\n",
       "   495: '33152',\n",
       "   496: '33153',\n",
       "   497: '33211',\n",
       "   498: '33219',\n",
       "   499: '33221',\n",
       "   500: '33222',\n",
       "   501: '33223',\n",
       "   502: '33224',\n",
       "   503: '33225',\n",
       "   504: '33229',\n",
       "   505: '33231',\n",
       "   506: '33232',\n",
       "   507: '33291',\n",
       "   508: '33299',\n",
       "   509: '33311',\n",
       "   510: '33312',\n",
       "   511: '33313',\n",
       "   512: '33320',\n",
       "   513: '33330',\n",
       "   514: '33340',\n",
       "   515: '33391',\n",
       "   516: '33392',\n",
       "   517: '33393',\n",
       "   518: '33394',\n",
       "   519: '33399',\n",
       "   520: '33461',\n",
       "   521: '33462',\n",
       "   522: '33491',\n",
       "   523: '33492',\n",
       "   524: '33493',\n",
       "   525: '33499',\n",
       "   526: '33510',\n",
       "   527: '33551',\n",
       "   528: '33552',\n",
       "   529: '33591',\n",
       "   530: '33592',\n",
       "   531: '33593',\n",
       "   532: '33599',\n",
       "   533: '33611',\n",
       "   534: '33612',\n",
       "   535: '33613',\n",
       "   536: '33614',\n",
       "   537: '33619',\n",
       "   538: '34110',\n",
       "   539: '34121',\n",
       "   540: '34122',\n",
       "   541: '34123',\n",
       "   542: '34210',\n",
       "   543: '34221',\n",
       "   544: '34222',\n",
       "   545: '34223',\n",
       "   546: '34224',\n",
       "   547: '34229',\n",
       "   548: '34310',\n",
       "   549: '34321',\n",
       "   550: '34322',\n",
       "   551: '34323',\n",
       "   552: '34331',\n",
       "   553: '34332',\n",
       "   554: '34341',\n",
       "   555: '34342',\n",
       "   556: '34343',\n",
       "   557: '34391',\n",
       "   558: '34399',\n",
       "   559: '34401',\n",
       "   560: '34409',\n",
       "   561: '35110',\n",
       "   562: '35121',\n",
       "   563: '35122',\n",
       "   564: '35123',\n",
       "   565: '35129',\n",
       "   566: '35140',\n",
       "   567: '35211',\n",
       "   568: '35212',\n",
       "   569: '35213',\n",
       "   570: '35214',\n",
       "   571: '35215',\n",
       "   572: '35219',\n",
       "   573: '35220',\n",
       "   574: '35231',\n",
       "   575: '35232',\n",
       "   576: '35239',\n",
       "   577: '35290',\n",
       "   578: '36100',\n",
       "   579: '36201',\n",
       "   580: '36202',\n",
       "   581: '36203',\n",
       "   582: '36204',\n",
       "   583: '36205',\n",
       "   584: '36206',\n",
       "   585: '36209',\n",
       "   586: '36910',\n",
       "   587: '36991',\n",
       "   588: '36999',\n",
       "   589: '39910',\n",
       "   590: '40000',\n",
       "   591: '41101',\n",
       "   592: '41102',\n",
       "   593: '41109',\n",
       "   594: '41201',\n",
       "   595: '41202',\n",
       "   596: '41310',\n",
       "   597: '41320',\n",
       "   598: '42111',\n",
       "   599: '42112',\n",
       "   600: '42113',\n",
       "   601: '42119',\n",
       "   602: '42131',\n",
       "   603: '42132',\n",
       "   604: '42141',\n",
       "   605: '42149',\n",
       "   606: '42210',\n",
       "   607: '42230',\n",
       "   608: '42241',\n",
       "   609: '42242',\n",
       "   610: '42243',\n",
       "   611: '42244',\n",
       "   612: '42245',\n",
       "   613: '42246',\n",
       "   614: '42247',\n",
       "   615: '42249',\n",
       "   616: '42290',\n",
       "   617: '43111',\n",
       "   618: '43112',\n",
       "   619: '43113',\n",
       "   620: '43114',\n",
       "   621: '43115',\n",
       "   622: '43116',\n",
       "   623: '43119',\n",
       "   624: '43121',\n",
       "   625: '43122',\n",
       "   626: '43123',\n",
       "   627: '43129',\n",
       "   628: '43141',\n",
       "   629: '43142',\n",
       "   630: '43151',\n",
       "   631: '43159',\n",
       "   632: '43211',\n",
       "   633: '43212',\n",
       "   634: '43219',\n",
       "   635: '43221',\n",
       "   636: '43222',\n",
       "   637: '43229',\n",
       "   638: '43231',\n",
       "   639: '43232',\n",
       "   640: '43233',\n",
       "   641: '43239',\n",
       "   642: '44110',\n",
       "   643: '44121',\n",
       "   644: '44122',\n",
       "   645: '44123',\n",
       "   646: '44129',\n",
       "   647: '44170',\n",
       "   648: '44191',\n",
       "   649: '44199',\n",
       "   650: '51111',\n",
       "   651: '51112',\n",
       "   652: '51121',\n",
       "   653: '51122',\n",
       "   654: '51129',\n",
       "   655: '51131',\n",
       "   656: '51132',\n",
       "   657: '51201',\n",
       "   658: '51202',\n",
       "   659: '51311',\n",
       "   660: '51312',\n",
       "   661: '51313',\n",
       "   662: '51321',\n",
       "   663: '51322',\n",
       "   664: '51330',\n",
       "   665: '51390',\n",
       "   666: '51411',\n",
       "   667: '51412',\n",
       "   668: '51419',\n",
       "   669: '51421',\n",
       "   670: '51422',\n",
       "   671: '51423',\n",
       "   672: '51491',\n",
       "   673: '51492',\n",
       "   674: '51499',\n",
       "   675: '51501',\n",
       "   676: '51502',\n",
       "   677: '51503',\n",
       "   678: '51504',\n",
       "   679: '51505',\n",
       "   680: '51509',\n",
       "   681: '51701',\n",
       "   682: '51702',\n",
       "   683: '51910',\n",
       "   684: '51931',\n",
       "   685: '51932',\n",
       "   686: '51941',\n",
       "   687: '51942',\n",
       "   688: '51943',\n",
       "   689: '51944',\n",
       "   690: '51949',\n",
       "   691: '51950',\n",
       "   692: '51991',\n",
       "   693: '51999',\n",
       "   694: '52110',\n",
       "   695: '52120',\n",
       "   696: '52130',\n",
       "   697: '52190',\n",
       "   698: '52201',\n",
       "   699: '52202',\n",
       "   700: '52301',\n",
       "   701: '52302',\n",
       "   702: '52303',\n",
       "   703: '52309',\n",
       "   704: '52411',\n",
       "   705: '52419',\n",
       "   706: '52421',\n",
       "   707: '52422',\n",
       "   708: '52440',\n",
       "   709: '52491',\n",
       "   710: '52492',\n",
       "   711: '52499',\n",
       "   712: '53111',\n",
       "   713: '53112',\n",
       "   714: '53113',\n",
       "   715: '53114',\n",
       "   716: '53115',\n",
       "   717: '53120',\n",
       "   718: '53201',\n",
       "   719: '53202',\n",
       "   720: '53203',\n",
       "   721: '53209',\n",
       "   722: '54110',\n",
       "   723: '54121',\n",
       "   724: '54122',\n",
       "   725: '54123',\n",
       "   726: '54130',\n",
       "   727: '54141',\n",
       "   728: '54142',\n",
       "   729: '54143',\n",
       "   730: '54144',\n",
       "   731: '54150',\n",
       "   732: '54191',\n",
       "   733: '54192',\n",
       "   734: '54193',\n",
       "   735: '54194',\n",
       "   736: '54199',\n",
       "   737: '59000',\n",
       "   738: '61110',\n",
       "   739: '61131',\n",
       "   740: '61132',\n",
       "   741: '61133',\n",
       "   742: '61210',\n",
       "   743: '61221',\n",
       "   744: '61222',\n",
       "   745: '61900',\n",
       "   746: '62211',\n",
       "   747: '62212',\n",
       "   748: '62219',\n",
       "   749: '62220',\n",
       "   750: '71000',\n",
       "   751: '71120',\n",
       "   752: '71130',\n",
       "   753: '71141',\n",
       "   754: '71142',\n",
       "   755: '71149',\n",
       "   756: '71151',\n",
       "   757: '71152',\n",
       "   758: '71191',\n",
       "   759: '71192',\n",
       "   760: '71199',\n",
       "   761: '71210',\n",
       "   762: '71220',\n",
       "   763: '71230',\n",
       "   764: '71241',\n",
       "   765: '71242',\n",
       "   766: '71249',\n",
       "   767: '71250',\n",
       "   768: '71261',\n",
       "   769: '71262',\n",
       "   770: '71263',\n",
       "   771: '71271',\n",
       "   772: '71272',\n",
       "   773: '71290',\n",
       "   774: '71311',\n",
       "   775: '71312',\n",
       "   776: '71321',\n",
       "   777: '71322',\n",
       "   778: '71323',\n",
       "   779: '71324',\n",
       "   780: '71329',\n",
       "   781: '71331',\n",
       "   782: '71332',\n",
       "   783: '72000',\n",
       "   784: '72110',\n",
       "   785: '72120',\n",
       "   786: '72130',\n",
       "   787: '72140',\n",
       "   788: '72150',\n",
       "   789: '72210',\n",
       "   790: '72221',\n",
       "   791: '72222',\n",
       "   792: '72229',\n",
       "   793: '72240',\n",
       "   794: '72310',\n",
       "   795: '72320',\n",
       "   796: '72330',\n",
       "   797: '72340',\n",
       "   798: '72350',\n",
       "   799: '72391',\n",
       "   800: '72392',\n",
       "   801: '72399',\n",
       "   802: '73000',\n",
       "   803: '73111',\n",
       "   804: '73112',\n",
       "   805: '73113',\n",
       "   806: '73119',\n",
       "   807: '73120',\n",
       "   808: '73130',\n",
       "   809: '73140',\n",
       "   810: '73150',\n",
       "   811: '73160',\n",
       "   812: '73210',\n",
       "   813: '73220',\n",
       "   814: '74001',\n",
       "   815: '74002',\n",
       "   816: '74110',\n",
       "   817: '74121',\n",
       "   818: '74122',\n",
       "   819: '74123',\n",
       "   820: '74131',\n",
       "   821: '74132',\n",
       "   822: '74133',\n",
       "   823: '74211',\n",
       "   824: '74212',\n",
       "   825: '74221',\n",
       "   826: '74222',\n",
       "   827: '74223',\n",
       "   828: '74224',\n",
       "   829: '75000',\n",
       "   830: '75110',\n",
       "   831: '75121',\n",
       "   832: '75122',\n",
       "   833: '75150',\n",
       "   834: '75190',\n",
       "   835: '75210',\n",
       "   836: '75220',\n",
       "   837: '75290',\n",
       "   838: '75310',\n",
       "   839: '75320',\n",
       "   840: '75340',\n",
       "   841: '75361',\n",
       "   842: '75362',\n",
       "   843: '75363',\n",
       "   844: '75369',\n",
       "   845: '75390',\n",
       "   846: '75410',\n",
       "   847: '75440',\n",
       "   848: '75490',\n",
       "   849: '81000',\n",
       "   850: '81130',\n",
       "   851: '81140',\n",
       "   852: '81210',\n",
       "   853: '81220',\n",
       "   854: '81230',\n",
       "   855: '81240',\n",
       "   856: '81251',\n",
       "   857: '81252',\n",
       "   858: '81259',\n",
       "   859: '81311',\n",
       "   860: '81312',\n",
       "   861: '81320',\n",
       "   862: '81390',\n",
       "   863: '81410',\n",
       "   864: '81420',\n",
       "   865: '81430',\n",
       "   866: '81501',\n",
       "   867: '81502',\n",
       "   868: '81509',\n",
       "   869: '81601',\n",
       "   870: '81602',\n",
       "   871: '81603',\n",
       "   872: '81604',\n",
       "   873: '81609',\n",
       "   874: '81700',\n",
       "   875: '81811',\n",
       "   876: '81812',\n",
       "   877: '81819',\n",
       "   878: '81821',\n",
       "   879: '81822',\n",
       "   880: '81829',\n",
       "   881: '81830',\n",
       "   882: '81841',\n",
       "   883: '81842',\n",
       "   884: '81849',\n",
       "   885: '81890',\n",
       "   886: '82000',\n",
       "   887: '82110',\n",
       "   888: '82121',\n",
       "   889: '82122',\n",
       "   890: '82123',\n",
       "   891: '82131',\n",
       "   892: '82132',\n",
       "   893: '82139',\n",
       "   894: '82190',\n",
       "   895: '83000',\n",
       "   896: '83110',\n",
       "   897: '83121',\n",
       "   898: '83129',\n",
       "   899: '83211',\n",
       "   900: '83212',\n",
       "   901: '83221',\n",
       "   902: '83222',\n",
       "   903: '83223',\n",
       "   904: '83224',\n",
       "   905: '83225',\n",
       "   906: '83226',\n",
       "   907: '83229',\n",
       "   908: '83311',\n",
       "   909: '83312',\n",
       "   910: '83321',\n",
       "   911: '83322',\n",
       "   912: '83323',\n",
       "   913: '83324',\n",
       "   914: '83329',\n",
       "   915: '83421',\n",
       "   916: '83422',\n",
       "   917: '83423',\n",
       "   918: '83424',\n",
       "   919: '83429',\n",
       "   920: '83431',\n",
       "   921: '83432',\n",
       "   922: '83439',\n",
       "   923: '83441',\n",
       "   924: '83449',\n",
       "   925: '83491',\n",
       "   926: '83492',\n",
       "   927: '83499',\n",
       "   928: '83501',\n",
       "   929: '83502',\n",
       "   930: '83509',\n",
       "   931: '91000',\n",
       "   932: '91121',\n",
       "   933: '91122',\n",
       "   934: '91129',\n",
       "   935: '91131',\n",
       "   936: '91132',\n",
       "   937: '91133',\n",
       "   938: '91151',\n",
       "   939: '91152',\n",
       "   940: '91153',\n",
       "   941: '91154',\n",
       "   942: '91161',\n",
       "   943: '91162',\n",
       "   944: '91210',\n",
       "   945: '91220',\n",
       "   946: '91230',\n",
       "   947: '91291',\n",
       "   948: '91292',\n",
       "   949: '91293',\n",
       "   950: '91299',\n",
       "   951: '91300',\n",
       "   952: '92141',\n",
       "   953: '92142',\n",
       "   954: '92149',\n",
       "   955: '92190',\n",
       "   956: '93100',\n",
       "   957: '93201',\n",
       "   958: '93209',\n",
       "   959: '93310',\n",
       "   960: '93331',\n",
       "   961: '93332',\n",
       "   962: '93333',\n",
       "   963: '93334',\n",
       "   964: '93335',\n",
       "   965: '93336',\n",
       "   966: '93337',\n",
       "   967: '93339',\n",
       "   968: '94101',\n",
       "   969: '94102',\n",
       "   970: '94103',\n",
       "   971: '94104',\n",
       "   972: '96000',\n",
       "   973: '96111',\n",
       "   974: '96112',\n",
       "   975: '96113',\n",
       "   976: '96119',\n",
       "   977: '96211',\n",
       "   978: '96212',\n",
       "   979: '96213',\n",
       "   980: '96251',\n",
       "   981: '96252',\n",
       "   982: '96253',\n",
       "   983: '96254',\n",
       "   984: '96255',\n",
       "   985: '96256',\n",
       "   986: '96257',\n",
       "   987: '96259',\n",
       "   988: '96261',\n",
       "   989: '96262',\n",
       "   990: '96269',\n",
       "   991: '96271',\n",
       "   992: '96272',\n",
       "   993: '96291',\n",
       "   994: '96292',\n",
       "   995: '96293',\n",
       "   996: '96299'}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fc5b-365c-426b-ae4a-4ec7456edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06838cc-934b-48f7-8154-0726ffef7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.data.Description[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets_1d': torch.tensor(self.data.SSOC_1D[index], dtype=torch.long),\n",
    "            'targets_2d': torch.tensor(self.data.SSOC_2D[index], dtype=torch.long),\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f0ba-0af3-4c04-8867-b4a08f086106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "training_set = Triage(train, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef8b3b-d1d4-4dda-b174-835feec76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ca96-b200-45ba-a7ab-13c2cf8e2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Stack 1: Predicting 1D SSOC (9)\n",
    "        self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "        # Stack 2: Predicting 2D SSOC (40 + 2 nec)\n",
    "        self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(777, 777), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(777, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 42)\n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Obtain the sentence embeddings from the DistilBERT model\n",
    "        embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = embeddings[0]\n",
    "        X = hidden_state[:, 0]\n",
    "        \n",
    "        # 1D Prediction\n",
    "        preds_1d = self.ssoc_1d_stack(X)\n",
    "        \n",
    "        # 2D Prediction\n",
    "        X = torch.cat((X, preds_1d), dim = 1)\n",
    "        preds_2d = self.ssoc_2d_stack(X)\n",
    "        \n",
    "        return preds_1d, preds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec19da7-4ecb-4eef-92ff-3bba58243d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb5c-f2ed-4635-8d82-c5b125ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss_fn\n",
    "# think of how to adjust the crossentropyloss function\n",
    "# change the targets upfront before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cd92-b91b-45ca-b4e2-fd83532500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ssoc(predicted, actual):\n",
    "    base_penalty = 10\n",
    "    penalty = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] != actual[i]:\n",
    "            penalty += base_penalty/(i+1)\n",
    "    return penalty\n",
    "\n",
    "def custom_loss_fn(top_probs_idx, targets, ssoc_level):\n",
    "          \n",
    "    if ssoc_level == '1d':\n",
    "          mapping = idx_ssoc1d\n",
    "    elif ssoc_level == '2d':\n",
    "          mapping = idx_ssoc2d\n",
    "          \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(top_probs_idx)):\n",
    "        predicted_ssoc = mapping[top_probs_idx[i].item()]\n",
    "        actual_ssoc = mapping[targets[i].item()]\n",
    "        loss += compare_ssoc(predicted_ssoc, actual_ssoc)\n",
    "        \n",
    "    return Variable(torch.tensor(float(loss)), requires_grad = True)\n",
    "\n",
    "# need to use Torch variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb2b8-516b-4e26-ab24-741e51189f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1 = Variable(torch.tensor([float(5), float(15)]), requires_grad = True)\n",
    "print(testing1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27724a-6699-4599-9ff3-47d6cc53f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40566ccf-4440-4b93-a379-7203ac10dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.tensor(float(1)), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badb587-3f24-4fc6-b4db-0df4f210129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18bd4c-cb70-44bc-b997-01838fbbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61414221-77ae-40e8-a916-b5161c227dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    # Set the NN to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(training_loader):\n",
    "        \n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "        targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Run the forward prop\n",
    "        preds_1d, preds_2d = model(ids, mask)\n",
    "        \n",
    "        # Find the indices of the top prediction\n",
    "        top_probs_1d, top_probs_idx_1d = torch.max(preds_1d.data, dim = 1)\n",
    "        top_probs_2d, top_probs_idx_2d = torch.max(preds_2d.data, dim = 1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss1 = loss_function(preds_1d, targets_1d)\n",
    "        loss2 = loss_function(preds_2d, targets_2d)\n",
    "        loss = loss1*5 + loss2\n",
    "        #print(f'Overall loss: {loss} = {loss1} + {loss2}')\n",
    "\n",
    "        # Deprecated\n",
    "        #loss = loss_function(preds_1d, targets_1d) + loss_function(preds_2d, targets_2d)\n",
    "        \n",
    "        # Add this batch's loss to the overall training loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        n_correct += calcuate_accu(top_probs_idx_2d, targets_2d)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets_2d.size(0)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 50 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 50 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822fb71-153f-4f25-8ed0-5bfc38b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70808b3-f08f-4df4-a07e-4274823285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1dd6-c7b5-4518-acc6-d16c7ef7910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d0c-a5b1-45e3-981f-df1f0f1c0c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf138-632a-4705-ad11-70e59c904cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
