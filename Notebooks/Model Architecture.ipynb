{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e984e1-4149-449d-8339-888305301829",
   "metadata": {},
   "source": [
    "Importing the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b570ba-9289-4103-8dd7-b29e154b1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 32,\n",
    "    'validation_batch_size': 32,\n",
    "    'epochs': 11,\n",
    "    'learning_rate': 0.001,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 4,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77a396c-5ec0-4827-969e-c32df888b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a002f18f-dfa4-405b-8ddc-82e8d94e9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoding = train.generate_encoding(SSOC_2020)\n",
    "encoded_data = train.encode_dataset(data, encoding, colnames)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "training_loader, validation_loader = train.prepare_data(encoded_data, tokenizer, colnames, parameters)\n",
    "model, loss_function, optimizer = train.prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9456a17-f3fe-467f-b605-3d97642d7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Models/sm-model1.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a322bac6-26ae-4970-99ac-13bc1e25b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "va_n_correct = 0\n",
    "va_loss = 0\n",
    "nb_va_steps = 0\n",
    "nb_va_examples = 0\n",
    "def calculate_accuracy(big_idx, targets):\n",
    "    n_correct = (big_idx == targets).sum().item()\n",
    "    return n_correct\n",
    "# Disable the calculation of gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(validation_loader):\n",
    "\n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "        mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "        # Run the forward prop\n",
    "        predictions = model(ids, mask)\n",
    "\n",
    "        # Iterate through each SSOC level\n",
    "        for ssoc_level, preds in predictions.items():\n",
    "\n",
    "            # Extract the correct target for the SSOC level\n",
    "            targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Compute the loss function using the predictions and the targets\n",
    "            level_loss = loss_function(preds, targets)\n",
    "\n",
    "            # Initialise the loss variable if this is the 1D level\n",
    "            # Else add to the loss variable\n",
    "            # Note the weights on each level\n",
    "            if ssoc_level == 'SSOC_1D':\n",
    "                loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "            else:\n",
    "                loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "        # Use the deepest level predictions to calculate accuracy\n",
    "        # Exploit the fact that the last preds object is the deepest level one\n",
    "        top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "        va_n_correct += calculate_accuracy(top_probs_idx, targets)\n",
    "\n",
    "        # Add this batch's loss to the overall training loss\n",
    "        va_loss += loss.item()\n",
    "\n",
    "        # Keep count for the batch steps and number of examples\n",
    "        nb_va_steps += 1\n",
    "        nb_va_examples += targets.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b8ec74-57cc-4cc8-bf1d-60334dba4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 61.933\n",
      "Validation: 0.56%\n"
     ]
    }
   ],
   "source": [
    "epoch_va_loss = va_loss / nb_va_steps\n",
    "epoch_va_accu = (va_n_correct * 100) / nb_va_examples\n",
    "print(f\"Validation: {epoch_va_loss:.3f}\")\n",
    "print(f\"Validation: {epoch_va_accu:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d456db-0dec-4b88-9821-9520b23bc5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5684"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)*.8/32/50*120/3600*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2af0b0-fa9f-4623-ab7f-a7d4b734f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 08 Oct 2021 - 23:38:59\n",
      "====================================================================\n",
      "> Epoch 1 started on: 08 Oct 2021 - 23:38:59\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 32.1588 \n",
      ">> Training Accuracy per 200 steps: 37.70%\n",
      ">> Batch of 200 took 7.24 mins\n",
      ">> Training Loss per 200 steps: 31.8466 \n",
      ">> Training Accuracy per 200 steps: 38.81%\n",
      ">> Batch of 200 took 7.11 mins\n",
      ">> Training Loss per 200 steps: 31.6486 \n",
      ">> Training Accuracy per 200 steps: 39.18%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 31.4667 \n",
      ">> Training Accuracy per 200 steps: 39.65%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 31.3973 \n",
      ">> Training Accuracy per 200 steps: 40.03%\n",
      ">> Batch of 200 took 7.12 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 1 Loss = \tTraining: 31.324  \tValidation: 28.819\n",
      "> Epoch 1 Accuracy = \tTraining: 40.15%  \tValidation: 45.22%\n",
      "> Epoch 1 took 47.48 mins\n",
      "====================================================================\n",
      "> Epoch 2 started on: 09 Oct 2021 - 00:26:28\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 30.2474 \n",
      ">> Training Accuracy per 200 steps: 42.53%\n",
      ">> Batch of 200 took 7.23 mins\n",
      ">> Training Loss per 200 steps: 30.3752 \n",
      ">> Training Accuracy per 200 steps: 42.61%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 30.5048 \n",
      ">> Training Accuracy per 200 steps: 42.65%\n",
      ">> Batch of 200 took 7.11 mins\n",
      ">> Training Loss per 200 steps: 30.3839 \n",
      ">> Training Accuracy per 200 steps: 42.73%\n",
      ">> Batch of 200 took 7.11 mins\n",
      ">> Training Loss per 200 steps: 30.3385 \n",
      ">> Training Accuracy per 200 steps: 42.88%\n",
      ">> Batch of 200 took 7.12 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 2 Loss = \tTraining: 30.270  \tValidation: 28.046\n",
      "> Epoch 2 Accuracy = \tTraining: 42.85%  \tValidation: 45.75%\n",
      "> Epoch 2 took 47.47 mins\n",
      "====================================================================\n",
      "> Epoch 3 started on: 09 Oct 2021 - 01:13:56\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 30.1208 \n",
      ">> Training Accuracy per 200 steps: 42.70%\n",
      ">> Batch of 200 took 7.24 mins\n",
      ">> Training Loss per 200 steps: 29.9466 \n",
      ">> Training Accuracy per 200 steps: 43.80%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 29.9266 \n",
      ">> Training Accuracy per 200 steps: 44.16%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 29.8385 \n",
      ">> Training Accuracy per 200 steps: 43.95%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 29.6933 \n",
      ">> Training Accuracy per 200 steps: 44.39%\n",
      ">> Batch of 200 took 7.12 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 3 Loss = \tTraining: 29.635  \tValidation: 27.901\n",
      "> Epoch 3 Accuracy = \tTraining: 44.39%  \tValidation: 48.63%\n",
      "> Epoch 3 took 47.51 mins\n",
      "====================================================================\n",
      "> Epoch 4 started on: 09 Oct 2021 - 02:01:27\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 29.4764 \n",
      ">> Training Accuracy per 200 steps: 44.67%\n",
      ">> Batch of 200 took 7.26 mins\n",
      ">> Training Loss per 200 steps: 29.4488 \n",
      ">> Training Accuracy per 200 steps: 44.89%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 29.4146 \n",
      ">> Training Accuracy per 200 steps: 45.14%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 29.4022 \n",
      ">> Training Accuracy per 200 steps: 45.01%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 29.3798 \n",
      ">> Training Accuracy per 200 steps: 45.20%\n",
      ">> Batch of 200 took 7.13 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 4 Loss = \tTraining: 29.409  \tValidation: 27.978\n",
      "> Epoch 4 Accuracy = \tTraining: 45.22%  \tValidation: 47.50%\n",
      "> Epoch 4 took 47.58 mins\n",
      "====================================================================\n",
      "> Epoch 5 started on: 09 Oct 2021 - 02:49:02\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 29.8984 \n",
      ">> Training Accuracy per 200 steps: 44.83%\n",
      ">> Batch of 200 took 7.24 mins\n",
      ">> Training Loss per 200 steps: 29.3511 \n",
      ">> Training Accuracy per 200 steps: 45.59%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 29.1998 \n",
      ">> Training Accuracy per 200 steps: 45.83%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 29.1531 \n",
      ">> Training Accuracy per 200 steps: 45.97%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 29.0520 \n",
      ">> Training Accuracy per 200 steps: 45.99%\n",
      ">> Batch of 200 took 7.12 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 5 Loss = \tTraining: 29.064  \tValidation: 27.544\n",
      "> Epoch 5 Accuracy = \tTraining: 45.94%  \tValidation: 48.02%\n",
      "> Epoch 5 took 47.51 mins\n",
      "====================================================================\n",
      "> Epoch 6 started on: 09 Oct 2021 - 03:36:32\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 29.1525 \n",
      ">> Training Accuracy per 200 steps: 45.80%\n",
      ">> Batch of 200 took 7.23 mins\n",
      ">> Training Loss per 200 steps: 28.4999 \n",
      ">> Training Accuracy per 200 steps: 46.80%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 28.3422 \n",
      ">> Training Accuracy per 200 steps: 46.93%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 28.5497 \n",
      ">> Training Accuracy per 200 steps: 46.73%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 28.6235 \n",
      ">> Training Accuracy per 200 steps: 46.84%\n",
      ">> Batch of 200 took 7.13 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 6 Loss = \tTraining: 28.611  \tValidation: 27.120\n",
      "> Epoch 6 Accuracy = \tTraining: 46.80%  \tValidation: 49.94%\n",
      "> Epoch 6 took 47.51 mins\n",
      "====================================================================\n",
      "> Epoch 7 started on: 09 Oct 2021 - 04:24:03\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 28.7199 \n",
      ">> Training Accuracy per 200 steps: 47.11%\n",
      ">> Batch of 200 took 7.26 mins\n",
      ">> Training Loss per 200 steps: 28.5122 \n",
      ">> Training Accuracy per 200 steps: 46.99%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 28.4278 \n",
      ">> Training Accuracy per 200 steps: 47.16%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 28.4349 \n",
      ">> Training Accuracy per 200 steps: 47.12%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 28.4256 \n",
      ">> Training Accuracy per 200 steps: 47.23%\n",
      ">> Batch of 200 took 7.13 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 7 Loss = \tTraining: 28.351  \tValidation: 27.314\n",
      "> Epoch 7 Accuracy = \tTraining: 47.26%  \tValidation: 50.80%\n",
      "> Epoch 7 took 47.60 mins\n",
      "====================================================================\n",
      "> Epoch 8 started on: 09 Oct 2021 - 05:11:39\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 28.6101 \n",
      ">> Training Accuracy per 200 steps: 46.95%\n",
      ">> Batch of 200 took 7.27 mins\n",
      ">> Training Loss per 200 steps: 28.6100 \n",
      ">> Training Accuracy per 200 steps: 46.91%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 28.5275 \n",
      ">> Training Accuracy per 200 steps: 47.35%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 28.4560 \n",
      ">> Training Accuracy per 200 steps: 47.53%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 28.4302 \n",
      ">> Training Accuracy per 200 steps: 47.40%\n",
      ">> Batch of 200 took 7.14 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 8 Loss = \tTraining: 28.398  \tValidation: 26.860\n",
      "> Epoch 8 Accuracy = \tTraining: 47.49%  \tValidation: 49.26%\n",
      "> Epoch 8 took 47.65 mins\n",
      "====================================================================\n",
      "> Epoch 9 started on: 09 Oct 2021 - 05:59:18\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 28.1063 \n",
      ">> Training Accuracy per 200 steps: 47.53%\n",
      ">> Batch of 200 took 7.26 mins\n",
      ">> Training Loss per 200 steps: 28.0796 \n",
      ">> Training Accuracy per 200 steps: 47.49%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 27.9592 \n",
      ">> Training Accuracy per 200 steps: 47.82%\n",
      ">> Batch of 200 took 7.14 mins\n",
      ">> Training Loss per 200 steps: 27.9889 \n",
      ">> Training Accuracy per 200 steps: 47.77%\n",
      ">> Batch of 200 took 7.15 mins\n",
      ">> Training Loss per 200 steps: 28.0674 \n",
      ">> Training Accuracy per 200 steps: 47.74%\n",
      ">> Batch of 200 took 7.16 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 9 Loss = \tTraining: 28.007  \tValidation: 26.318\n",
      "> Epoch 9 Accuracy = \tTraining: 47.74%  \tValidation: 51.25%\n",
      "> Epoch 9 took 47.70 mins\n",
      "====================================================================\n",
      "> Epoch 10 started on: 09 Oct 2021 - 06:47:01\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 27.8495 \n",
      ">> Training Accuracy per 200 steps: 48.92%\n",
      ">> Batch of 200 took 7.27 mins\n",
      ">> Training Loss per 200 steps: 27.8733 \n",
      ">> Training Accuracy per 200 steps: 48.61%\n",
      ">> Batch of 200 took 7.15 mins\n",
      ">> Training Loss per 200 steps: 27.9189 \n",
      ">> Training Accuracy per 200 steps: 48.42%\n",
      ">> Batch of 200 took 7.15 mins\n",
      ">> Training Loss per 200 steps: 27.9176 \n",
      ">> Training Accuracy per 200 steps: 48.47%\n",
      ">> Batch of 200 took 7.15 mins\n",
      ">> Training Loss per 200 steps: 27.9240 \n",
      ">> Training Accuracy per 200 steps: 48.20%\n",
      ">> Batch of 200 took 7.14 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 10 Loss = \tTraining: 27.898  \tValidation: 26.666\n",
      "> Epoch 10 Accuracy = \tTraining: 48.22%  \tValidation: 50.59%\n",
      "> Epoch 10 took 47.68 mins\n",
      "====================================================================\n",
      "> Epoch 11 started on: 09 Oct 2021 - 07:34:41\n",
      "--------------------------------------------------------------------\n",
      ">> Training Loss per 200 steps: 27.9160 \n",
      ">> Training Accuracy per 200 steps: 47.56%\n",
      ">> Batch of 200 took 7.28 mins\n",
      ">> Training Loss per 200 steps: 27.7275 \n",
      ">> Training Accuracy per 200 steps: 48.06%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 27.6895 \n",
      ">> Training Accuracy per 200 steps: 47.85%\n",
      ">> Batch of 200 took 7.12 mins\n",
      ">> Training Loss per 200 steps: 27.6936 \n",
      ">> Training Accuracy per 200 steps: 48.03%\n",
      ">> Batch of 200 took 7.13 mins\n",
      ">> Training Loss per 200 steps: 27.7450 \n",
      ">> Training Accuracy per 200 steps: 47.88%\n",
      ">> Batch of 200 took 7.13 mins\n",
      "--------------------------------------------------------------------\n",
      "> Epoch 11 Loss = \tTraining: 27.725  \tValidation: 26.148\n",
      "> Epoch 11 Accuracy = \tTraining: 47.87%  \tValidation: 50.65%\n",
      "> Epoch 11 took 47.60 mins\n",
      "====================================================================\n",
      "Training ended on: 09 Oct 2021 - 08:22:17\n",
      "Total training time: 8.72 hours\n"
     ]
    }
   ],
   "source": [
    "train.train_model(model, loss_function, optimizer, training_loader, validation_loader, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3167b7c-f782-4378-a8b9-437d26c48fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/autocoder-8oct-extralayer-12epoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993bc750-2bca-4ba3-b128-69532f890721",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20000/4067800170.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_20000/4067800170.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3d1a2-4c6b-44a8-a7bf-8727ced070fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b86904-1a0a-453a-a963-48402249b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"encoding.json\", 'w') as outfile:\n",
    "    json.dump(encoding, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8645c3-1ae1-48b1-9221-005085543ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b6acc3-ee89-4b7c-9807-6b8a98ed199b",
   "metadata": {},
   "source": [
    "## Analysis of the underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72164f17-e3fe-40bd-ab56-291a3d3faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[colnames['SSOC']].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ce07d-a215-461d-9047-508baed4a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train.generate_encoding(SSOC_2020)\n",
    "encoded_data = train.encode_dataset(data, encoding, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05435c85-2390-4ae7-a2d0-5c6a4d5bb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_1D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4315-d6a4-4702-b6bb-985c42e57ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data['SSOC_2D'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa63fd6-f7a9-408e-8104-80edfa29cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_encoding(reference_data, ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Generates encoding for SSOC to indices, as required by PyTorch\n",
    "    for multi-class classification, for the training data\n",
    "\n",
    "    Args:\n",
    "        reference_data: Pandas dataframe containing all SSOCs\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the SSOC to index mapping (for preparing the\n",
    "        dataset) and index to SSOC mapping (for interpreting the predictions),\n",
    "        for each SSOC level from 1D to 5D.\n",
    "    '''\n",
    "\n",
    "    # Initialise the dictionary object to store the encodings for each level\n",
    "    encoding = {}\n",
    "\n",
    "    # Iterate through each level from 1 to 5\n",
    "    for level in range(1, 6):\n",
    "\n",
    "        # Initialise a dictionary object to store the respective-way encodings\n",
    "        ssoc_idx_mapping = {}\n",
    "\n",
    "        # Slice the SSOC column by the level required, drop duplicates, and sort\n",
    "        ssocs = list(np.sort(reference_data[ssoc_colname].astype('str').str.slice(0, level).unique()))\n",
    "\n",
    "        # Iterate through each unique SSOC (at i-digit level) and add to dict\n",
    "        for i, ssoc in enumerate(ssocs):\n",
    "            ssoc_idx_mapping[ssoc] = i\n",
    "\n",
    "        # Add each level's encodings to the output dictionary\n",
    "        encoding[f'SSOC_{level}D'] = {\n",
    "\n",
    "            # Store the SSOC to index encoding\n",
    "            'ssoc_idx': ssoc_idx_mapping,\n",
    "            # Store the index to SSOC encoding\n",
    "            'idx_ssoc': {v: k for k, v in ssoc_idx_mapping.items()}\n",
    "        }\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def encode_dataset(data,\n",
    "                   encoding,\n",
    "                   ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Uses the generated encoding to encode the SSOCs at each\n",
    "    digit level.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe of the training data with the correct SSOC\n",
    "        encoding: Encoding for each SSOC level\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with each digit SSOC encoded correctly\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    encoded_data = copy.deepcopy(data)[~data[ssoc_colname].str.contains('X')]\n",
    "\n",
    "    # For each digit, encode the SSOC correctly\n",
    "    for ssoc_level, encodings in encoding.items():\n",
    "        encoded_data[ssoc_level] = encoded_data[ssoc_colname].astype('str').str.slice(0, int(ssoc_level[5])).replace(encodings['ssoc_idx'])\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "# Create a new Python class to handle the additional complexity\n",
    "class SSOC_Dataset(Dataset):\n",
    "\n",
    "    # Define the class attributes\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Define the iterable over the Dataset object \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Extract the text\n",
    "        text = self.data[colnames['job_description']][index]\n",
    "\n",
    "        # Pass in the data into the tokenizer\n",
    "        inputs = self.tokenizer(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        # Extract the IDs and attention mask\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Return all the outputs needed for training and evaluation\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'SSOC_1D': torch.tensor(self.data.SSOC_1D[index], dtype = torch.long),\n",
    "            'SSOC_2D': torch.tensor(self.data.SSOC_2D[index], dtype = torch.long),\n",
    "            'SSOC_3D': torch.tensor(self.data.SSOC_3D[index], dtype = torch.long),\n",
    "            'SSOC_4D': torch.tensor(self.data.SSOC_4D[index], dtype = torch.long),\n",
    "            'SSOC_5D': torch.tensor(self.data.SSOC_5D[index], dtype = torch.long),\n",
    "        } \n",
    "\n",
    "    # Define the length attribute\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d9967-0376-4345-b512-539480a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(encoded_data,\n",
    "                 colnames,\n",
    "                 parameters):\n",
    "    \n",
    "    # Split the dataset into training and validation\n",
    "    training_data, validation_data = train_test_split(encoded_data,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 2021)\n",
    "    training_data.reset_index(drop = True, inplace = True)\n",
    "    validation_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "    \n",
    "    # Creating the dataset and dataloader for the neural network\n",
    "    training_loader = DataLoader(SSOC_Dataset(training_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                 batch_size = parameters['training_batch_size'],\n",
    "                                 num_workers = parameters['num_workers'],\n",
    "                                 shuffle = True,\n",
    "                                 persistent_workers=True)\n",
    "    validation_loader = DataLoader(SSOC_Dataset(validation_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                   batch_size = parameters['training_batch_size'],\n",
    "                                   num_workers = parameters['num_workers'],\n",
    "                                   shuffle = True,\n",
    "                                   persistent_workers=True)\n",
    "    \n",
    "    return training_loader, validation_loader, tokenizer\n",
    "\n",
    "class HierarchicalSSOCClassifier(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(HierarchicalSSOCClassifier, self).__init__()\n",
    "            \n",
    "            self.l1 = DistilBertModel.from_pretrained(parameters['pretrained_model'])\n",
    "\n",
    "            # Generating dimensions\n",
    "            SSOC_1D_count = len(encoding['SSOC_1D']['ssoc_idx'].keys())\n",
    "            SSOC_2D_count = len(encoding['SSOC_2D']['ssoc_idx'].keys())\n",
    "            SSOC_3D_count = len(encoding['SSOC_3D']['ssoc_idx'].keys())\n",
    "            SSOC_4D_count = len(encoding['SSOC_4D']['ssoc_idx'].keys())\n",
    "            SSOC_5D_count = len(encoding['SSOC_5D']['ssoc_idx'].keys())            \n",
    "            \n",
    "            # Stack 1: Predicting 1D SSOC (9)\n",
    "            if parameters['max_level'] >= 1:\n",
    "                self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(768, 768), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(768, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_1D_count)\n",
    "                )\n",
    "\n",
    "            # Stack 2: Predicting 2D SSOC (42)\n",
    "            if parameters['max_level'] >= 2:\n",
    "                n_dims_2d = 768 + SSOC_1D_count\n",
    "                self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_dims_2d, n_dims_2d), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(n_dims_2d, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_2D_count)\n",
    "                )        \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "\n",
    "            # Obtain the sentence embeddings from the DistilBERT model\n",
    "            embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_state = embeddings[0]\n",
    "            X = hidden_state[:, 0]\n",
    "\n",
    "            predictions = {}\n",
    "            \n",
    "            # 1D Prediction\n",
    "            if parameters['max_level'] >= 1:\n",
    "                predictions['SSOC_1D'] = self.ssoc_1d_stack(X)\n",
    "\n",
    "            # 2D Prediction\n",
    "            if parameters['max_level'] >= 2:\n",
    "                X = torch.cat((X, predictions['SSOC_1D']), dim = 1)\n",
    "                predictions['SSOC_2D'] = self.ssoc_2d_stack(X)\n",
    "\n",
    "            return {f'SSOC_{i}D': predictions[f'SSOC_{i}D'] for i in range(1, parameters['max_level'] + 1)}\n",
    "\n",
    "def prepare_model(encoding, parameters):\n",
    "        \n",
    "    model = HierarchicalSSOCClassifier()\n",
    "    model.to(parameters['device'])\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr = parameters['learning_rate'])\n",
    "    \n",
    "    return model, loss_function, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd14b71-6820-4f26-a338-04d2590df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training started on:\", current_time)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Set the NN to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over each batch\n",
    "        for batch, data in enumerate(training_loader):\n",
    "\n",
    "            # Extract the data\n",
    "            ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "            mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Run the forward prop\n",
    "            predictions = model(ids, mask)\n",
    "\n",
    "            # Iterate through each SSOC level\n",
    "            for ssoc_level, preds in predictions.items():\n",
    "\n",
    "                # Extract the correct target for the SSOC level\n",
    "                targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "                # Compute the loss function using the predictions and the targets\n",
    "                level_loss = loss_function(preds, targets)\n",
    "\n",
    "                # Initialise the loss variable if this is the 1D level\n",
    "                # Else add to the loss variable\n",
    "                # Note the weights on each level\n",
    "                if ssoc_level == 'SSOC_1D':\n",
    "                    loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "                else:\n",
    "                    loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "            # Use the deepest level predictions to calculate accuracy\n",
    "            top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "            n_correct += calculate_accu(top_probs_idx, targets)\n",
    "\n",
    "            # Calculate the loss\n",
    "    #         targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "    #         targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "    #         loss1 = loss_function(preds_1d, targets_1d)\n",
    "    #         loss2 = loss_function(preds_2d, targets_2d)\n",
    "    #         loss = loss1*5 + loss2\n",
    "\n",
    "            # Add this batch's loss to the overall training loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 500 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples \n",
    "                print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "                print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "                print(f\"Batch of 500 took {(time.time() - batch_start_time)/60:.2f} mins\")\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "        print(f\"Epoch training time: {(time.time() - epoch_start_time)/60:.2f} mins\")\n",
    "\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} mins\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training ended on:\", current_time)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f8869-10b0-4ec8-9a37-977c4882278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 4,\n",
    "    'validation_batch_size': 2,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 1e-05,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 0,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0da49c-bf7e-4e30-827b-11da01be9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')\n",
    "encoding = generate_encoding(SSOC_2020)\n",
    "encoded_data = encode_dataset(data[0:10000], encoding)\n",
    "training_loader, validation_loader = prepare_data(encoded_data, colnames, parameters)\n",
    "model, loss_function, optimizer = prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d4b13-379e-4a1a-8f57-6342b26063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa083cd5-ccad-457b-9ebe-d95ad39b6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "10000*.8/4/50*45/3600*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb40d7-95ea-4ca4-bc1c-2334cd9a953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, loss_function, optimizer, parameters['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a796d-c742-49c5-9057-5fe0fdc7bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184151f8-6e52-4dfd-8d7d-71516af33210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ddf71-b7a8-4291-aa38-0d1b489eb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6e0dc-4f0f-42e6-a568-45b55d2278b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data[other_data['Cleaned_Description'] == 'Duties and Responsibilities: Implementation of Sage 300 ERP (Financials, Distribution, Project) Providing Pre & Post-Sales Consulting. Perform Business requirement analysis and provide professional advises. Install, Implement, Train and Support users on Sage 300 ERP Software. On-site and Back-end support on ERP Software. Diploma or Degree in Accountancy/Business Admin/Computer Science, Information Systems. Good Knowledge in MSSQL Server, MS Excel, Crystal Report and Visual Basic. Good analytical and problem-solving skills are essential. Good interpersonal and communication skills. Must have Sage 300 ERP Software. At least 4 - 5 years of working experience in relevant field. Must be able to work in Singapore and travel to other country.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41208371-6984-4640-adb4-84aaa54e6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279f1f3-d29c-435d-b572-0c999f27e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = data[10000:]\n",
    "test_data = other_data[other_data['SSOC 2020'] == data[0:10000]['SSOC 2020'].sample().values[0]].sample()\n",
    "test_target = test_data['SSOC 2020'].values[0]\n",
    "text = test_data['Cleaned_Description'].values[0]\n",
    "print(test_data['SSOC 2020'].values[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed846f1-5e88-4c4c-af28-69f4ec950f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, tokenizer, text, target, parameters):\n",
    "    tokenized = tokenizer(\n",
    "        text = text,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    test_ids = torch.tensor([tokenized['input_ids']], dtype = torch.long)\n",
    "    test_mask = torch.tensor([tokenized['attention_mask']], dtype = torch.long)\n",
    "    \n",
    "    model.eval()\n",
    "    preds = model(test_ids, test_mask)\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    predicted_1D = encoding['SSOC_1D']['idx_ssoc'][np.argmax(preds[\"SSOC_1D\"].detach().numpy())]\n",
    "    predicted_1D_proba = np.max(m(preds['SSOC_1D']).detach().numpy())\n",
    "    predicted_2D = encoding['SSOC_2D']['idx_ssoc'][np.argmax(preds[\"SSOC_2D\"].detach().numpy())]\n",
    "    predicted_2D_proba = np.max(m(preds['SSOC_2D']).detach().numpy())\n",
    "    \n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Model predicted 1D: {predicted_1D} ({predicted_1D_proba*100:.2f}%)\")\n",
    "    print(f\"Model predicted 2D: {predicted_2D} ({predicted_2D_proba*100:.2f}%)\")\n",
    "    \n",
    "generate_prediction(model1, tokenizer, text, test_target, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602728de-d8bc-4a4e-9eb1-0a09731df744",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/autocoder-v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47315b-d1c4-4352-9c93-1e17afa470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = HierarchicalSSOCClassifier()\n",
    "model1.load_state_dict(torch.load('Models/autocoder-v1.pt'))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939c615-0f03-4e83-8481-9ef4fd819d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\n",
    "    text = text,\n",
    "    text_pair = None,\n",
    "    add_special_tokens = True,\n",
    "    max_length = parameters['sequence_max_length'],\n",
    "    pad_to_max_length = True,\n",
    "    return_token_type_ids = True,\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3dd47-526d-4c11-a294-ffedab5a056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = torch.tensor([tokenized['input_ids']], dtype = torch.long)\n",
    "test_mask = torch.tensor([tokenized['attention_mask']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf396e12-b574-484c-b190-9b744790d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_ids, test_mask)\n",
    "targets = torch.tensor([encoding['SSOC_1D']['ssoc_idx']['2']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ad2d7-f060-4bf2-85d7-b0647fd085e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(preds[\"SSOC_1D\"], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dce3d3-1e01-4c4e-8f1b-aa7f0b9c1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['SSOC_1D']['idx_ssoc'][np.argmax(preds[\"SSOC_1D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0b4ea-6595-4604-85f0-c842dc16f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['SSOC_2D']['idx_ssoc'][np.argmax(preds[\"SSOC_2D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340a569-14e6-4610-879c-b69c78503a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "m(preds['SSOC_2D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39977202-1d8b-4511-b0de-dba63e556810",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Assist with installation, configuration and set-up of new IT accounts & IT equipment for new users. Liaising with vendors for procurement, logistic and maintenance of IT equipment. Managing & troubleshooting of office IT equipment & systems. Analyze, monitor and resolve application and system failures and provide operational support. Perform, review and enhance business and IT systems & processes for enhanced improvement for the company.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fc5b-365c-426b-ae4a-4ec7456edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06838cc-934b-48f7-8154-0726ffef7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.data.Description[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets_1d': torch.tensor(self.data.SSOC_1D[index], dtype=torch.long),\n",
    "            'targets_2d': torch.tensor(self.data.SSOC_2D[index], dtype=torch.long),\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f0ba-0af3-4c04-8867-b4a08f086106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "training_set = Triage(train, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef8b3b-d1d4-4dda-b174-835feec76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ca96-b200-45ba-a7ab-13c2cf8e2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Stack 1: Predicting 1D SSOC (9)\n",
    "        self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "        # Stack 2: Predicting 2D SSOC (40 + 2 nec)\n",
    "        self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(777, 777), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(777, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 42)\n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Obtain the sentence embeddings from the DistilBERT model\n",
    "        embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = embeddings[0]\n",
    "        X = hidden_state[:, 0]\n",
    "        \n",
    "        # 1D Prediction\n",
    "        preds_1d = self.ssoc_1d_stack(X)\n",
    "        \n",
    "        # 2D Prediction\n",
    "        X = torch.cat((X, preds_1d), dim = 1)\n",
    "        preds_2d = self.ssoc_2d_stack(X)\n",
    "        \n",
    "        return preds_1d, preds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec19da7-4ecb-4eef-92ff-3bba58243d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb5c-f2ed-4635-8d82-c5b125ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss_fn\n",
    "# think of how to adjust the crossentropyloss function\n",
    "# change the targets upfront before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cd92-b91b-45ca-b4e2-fd83532500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ssoc(predicted, actual):\n",
    "    base_penalty = 10\n",
    "    penalty = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] != actual[i]:\n",
    "            penalty += base_penalty/(i+1)\n",
    "    return penalty\n",
    "\n",
    "def custom_loss_fn(top_probs_idx, targets, ssoc_level):\n",
    "          \n",
    "    if ssoc_level == '1d':\n",
    "          mapping = idx_ssoc1d\n",
    "    elif ssoc_level == '2d':\n",
    "          mapping = idx_ssoc2d\n",
    "          \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(top_probs_idx)):\n",
    "        predicted_ssoc = mapping[top_probs_idx[i].item()]\n",
    "        actual_ssoc = mapping[targets[i].item()]\n",
    "        loss += compare_ssoc(predicted_ssoc, actual_ssoc)\n",
    "        \n",
    "    return Variable(torch.tensor(float(loss)), requires_grad = True)\n",
    "\n",
    "# need to use Torch variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb2b8-516b-4e26-ab24-741e51189f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1 = Variable(torch.tensor([float(5), float(15)]), requires_grad = True)\n",
    "print(testing1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27724a-6699-4599-9ff3-47d6cc53f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40566ccf-4440-4b93-a379-7203ac10dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.tensor(float(1)), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badb587-3f24-4fc6-b4db-0df4f210129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18bd4c-cb70-44bc-b997-01838fbbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61414221-77ae-40e8-a916-b5161c227dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    # Set the NN to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(training_loader):\n",
    "        \n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "        targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Run the forward prop\n",
    "        preds_1d, preds_2d = model(ids, mask)\n",
    "        \n",
    "        # Find the indices of the top prediction\n",
    "        top_probs_1d, top_probs_idx_1d = torch.max(preds_1d.data, dim = 1)\n",
    "        top_probs_2d, top_probs_idx_2d = torch.max(preds_2d.data, dim = 1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss1 = loss_function(preds_1d, targets_1d)\n",
    "        loss2 = loss_function(preds_2d, targets_2d)\n",
    "        loss = loss1*5 + loss2\n",
    "        #print(f'Overall loss: {loss} = {loss1} + {loss2}')\n",
    "\n",
    "        # Deprecated\n",
    "        #loss = loss_function(preds_1d, targets_1d) + loss_function(preds_2d, targets_2d)\n",
    "        \n",
    "        # Add this batch's loss to the overall training loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        n_correct += calcuate_accu(top_probs_idx_2d, targets_2d)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets_2d.size(0)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 50 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 50 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822fb71-153f-4f25-8ed0-5bfc38b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70808b3-f08f-4df4-a07e-4274823285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1dd6-c7b5-4518-acc6-d16c7ef7910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d0c-a5b1-45e3-981f-df1f0f1c0c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf138-632a-4705-ad11-70e59c904cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
