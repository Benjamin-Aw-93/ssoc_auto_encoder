{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e984e1-4149-449d-8339-888305301829",
   "metadata": {},
   "source": [
    "Importing the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa63fd6-f7a9-408e-8104-80edfa29cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_encoding(reference_data, ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Generates encoding for SSOC to indices, as required by PyTorch\n",
    "    for multi-class classification, for the training data\n",
    "\n",
    "    Args:\n",
    "        reference_data: Pandas dataframe containing all SSOCs\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the SSOC to index mapping (for preparing the\n",
    "        dataset) and index to SSOC mapping (for interpreting the predictions),\n",
    "        for each SSOC level from 1D to 5D.\n",
    "    '''\n",
    "\n",
    "    # Initialise the dictionary object to store the encodings for each level\n",
    "    encoding = {}\n",
    "\n",
    "    # Iterate through each level from 1 to 5\n",
    "    for level in range(1, 6):\n",
    "\n",
    "        # Initialise a dictionary object to store the respective-way encodings\n",
    "        ssoc_idx_mapping = {}\n",
    "\n",
    "        # Slice the SSOC column by the level required, drop duplicates, and sort\n",
    "        ssocs = list(np.sort(reference_data[ssoc_colname].astype('str').str.slice(0, level).unique()))\n",
    "\n",
    "        # Iterate through each unique SSOC (at i-digit level) and add to dict\n",
    "        for i, ssoc in enumerate(ssocs):\n",
    "            ssoc_idx_mapping[ssoc] = i\n",
    "\n",
    "        # Add each level's encodings to the output dictionary\n",
    "        encoding[f'SSOC_{level}D'] = {\n",
    "\n",
    "            # Store the SSOC to index encoding\n",
    "            'ssoc_idx': ssoc_idx_mapping,\n",
    "            # Store the index to SSOC encoding\n",
    "            'idx_ssoc': {v: k for k, v in ssoc_idx_mapping.items()}\n",
    "        }\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def encode_dataset(data,\n",
    "                   encoding,\n",
    "                   ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Uses the generated encoding to encode the SSOCs at each\n",
    "    digit level.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe of the training data with the correct SSOC\n",
    "        encoding: Encoding for each SSOC level\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with each digit SSOC encoded correctly\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    encoded_data = copy.deepcopy(data)[~data[ssoc_colname].str.contains('X')]\n",
    "\n",
    "    # For each digit, encode the SSOC correctly\n",
    "    for ssoc_level, encodings in encoding.items():\n",
    "        encoded_data[ssoc_level] = encoded_data[ssoc_colname].astype('str').str.slice(0, int(ssoc_level[5])).replace(encodings['ssoc_idx'])\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "# Create a new Python class to handle the additional complexity\n",
    "class SSOC_Dataset(Dataset):\n",
    "\n",
    "    # Define the class attributes\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Define the iterable over the Dataset object \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Extract the text\n",
    "        text = self.data[colnames['job_description']][index]\n",
    "\n",
    "        # Pass in the data into the tokenizer\n",
    "        inputs = self.tokenizer(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        # Extract the IDs and attention mask\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Return all the outputs needed for training and evaluation\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'SSOC_1D': torch.tensor(self.data.SSOC_1D[index], dtype = torch.long),\n",
    "            'SSOC_2D': torch.tensor(self.data.SSOC_2D[index], dtype = torch.long),\n",
    "            'SSOC_3D': torch.tensor(self.data.SSOC_3D[index], dtype = torch.long),\n",
    "            'SSOC_4D': torch.tensor(self.data.SSOC_4D[index], dtype = torch.long),\n",
    "            'SSOC_5D': torch.tensor(self.data.SSOC_5D[index], dtype = torch.long),\n",
    "        } \n",
    "\n",
    "    # Define the length attribute\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44d9967-0376-4345-b512-539480a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(encoded_data,\n",
    "                 colnames,\n",
    "                 parameters):\n",
    "    \n",
    "    # Split the dataset into training and testing\n",
    "    training_data, testing_data = train_test_split(encoded_data,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 2021)\n",
    "    training_data.reset_index(drop = True, inplace = True)\n",
    "    testing_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "    \n",
    "    # Creating the dataset and dataloader for the neural network\n",
    "    training_loader = DataLoader(SSOC_Dataset(training_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                 batch_size = parameters['training_batch_size'],\n",
    "                                 num_workers = parameters['num_workers'],\n",
    "                                 shuffle = True)\n",
    "    testing_loader = DataLoader(SSOC_Dataset(testing_data, tokenizer, parameters['sequence_max_length']),\n",
    "                                batch_size = parameters['training_batch_size'],\n",
    "                                num_workers = parameters['num_workers'],\n",
    "                                shuffle = True)\n",
    "    \n",
    "    return training_loader, testing_loader, tokenizer\n",
    "\n",
    "def prepare_model(encoding, parameters):\n",
    "    \n",
    "    class HierarchicalSSOCClassifier(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(HierarchicalSSOCClassifier, self).__init__()\n",
    "            \n",
    "            self.l1 = DistilBertModel.from_pretrained(parameters['pretrained_model'])\n",
    "\n",
    "            # Generating dimensions\n",
    "            SSOC_1D_count = len(encoding['SSOC_1D']['ssoc_idx'].keys())\n",
    "            SSOC_2D_count = len(encoding['SSOC_2D']['ssoc_idx'].keys())\n",
    "            SSOC_3D_count = len(encoding['SSOC_3D']['ssoc_idx'].keys())\n",
    "            SSOC_4D_count = len(encoding['SSOC_4D']['ssoc_idx'].keys())\n",
    "            SSOC_5D_count = len(encoding['SSOC_5D']['ssoc_idx'].keys())            \n",
    "            \n",
    "            # Stack 1: Predicting 1D SSOC (9)\n",
    "            if parameters['max_level'] >= 1:\n",
    "                self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(768, 768), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(768, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_1D_count)\n",
    "                )\n",
    "\n",
    "            # Stack 2: Predicting 2D SSOC (42)\n",
    "            if parameters['max_level'] >= 2:\n",
    "                n_dims_2d = 768 + SSOC_1D_count\n",
    "                self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_dims_2d, n_dims_2d), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(n_dims_2d, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_2D_count)\n",
    "                )        \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "\n",
    "            # Obtain the sentence embeddings from the DistilBERT model\n",
    "            embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_state = embeddings[0]\n",
    "            X = hidden_state[:, 0]\n",
    "\n",
    "            predictions = {}\n",
    "            \n",
    "            # 1D Prediction\n",
    "            if parameters['max_level'] >= 1:\n",
    "                predictions['SSOC_1D'] = self.ssoc_1d_stack(X)\n",
    "\n",
    "            # 2D Prediction\n",
    "            if parameters['max_level'] >= 2:\n",
    "                X = torch.cat((X, predictions['SSOC_1D']), dim = 1)\n",
    "                predictions['SSOC_2D'] = self.ssoc_2d_stack(X)\n",
    "\n",
    "            return {f'SSOC_{i}D': predictions[f'SSOC_{i}D'] for i in range(1, parameters['max_level'] + 1)}\n",
    "        \n",
    "    model = HierarchicalSSOCClassifier()\n",
    "    model.to(parameters['device'])\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr = parameters['learning_rate'])\n",
    "    \n",
    "    return model, loss_function, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd14b71-6820-4f26-a338-04d2590df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training started on:\", current_time)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Set the NN to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over each batch\n",
    "        for batch, data in enumerate(training_loader):\n",
    "\n",
    "            # Extract the data\n",
    "            ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "            mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Run the forward prop\n",
    "            predictions = model(ids, mask)\n",
    "\n",
    "            # Iterate through each SSOC level\n",
    "            for ssoc_level, preds in predictions.items():\n",
    "\n",
    "                # Extract the correct target for the SSOC level\n",
    "                targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "                # Compute the loss function using the predictions and the targets\n",
    "                level_loss = loss_function(preds, targets)\n",
    "\n",
    "                # Initialise the loss variable if this is the 1D level\n",
    "                # Else add to the loss variable\n",
    "                # Note the weights on each level\n",
    "                if ssoc_level == 'SSOC_1D':\n",
    "                    loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "                else:\n",
    "                    loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "            # Use the deepest level predictions to calculate accuracy\n",
    "            top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "            n_correct += calculate_accu(top_probs_idx, targets)\n",
    "\n",
    "            # Calculate the loss\n",
    "    #         targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "    #         targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "    #         loss1 = loss_function(preds_1d, targets_1d)\n",
    "    #         loss2 = loss_function(preds_2d, targets_2d)\n",
    "    #         loss = loss1*5 + loss2\n",
    "\n",
    "            # Add this batch's loss to the overall training loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 500 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples \n",
    "                print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "                print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "                print(f\"Batch of 500 took {(time.time() - batch_start_time)/60:.2f} mins\")\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "        print(f\"Epoch training time: {(time.time() - epoch_start_time)/60:.2f} mins\")\n",
    "\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} mins\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training ended on:\", current_time)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9f8869-10b0-4ec8-9a37-977c4882278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 4,\n",
    "    'validation_batch_size': 2,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 1e-05,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 0,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0da49c-bf7e-4e30-827b-11da01be9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoding = generate_encoding(SSOC_2020)\n",
    "encoded_data = encode_dataset(data[0:10000], encoding)\n",
    "training_loader, testing_loader = prepare_data(encoded_data, colnames, parameters)\n",
    "model, loss_function, optimizer = prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa083cd5-ccad-457b-9ebe-d95ad39b6896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000*.8/4/50*45/3600*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43fb40d7-95ea-4ca4-bc1c-2334cd9a953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started on: 04 Oct 2021 - 11:06:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 52.67226482391357\n",
      "Training Accuracy per 500 steps: 12.9\n",
      "Batch of 500 took 7.33 mins\n",
      "Training Loss per 500 steps: 47.7736167011261\n",
      "Training Accuracy per 500 steps: 17.225\n",
      "Batch of 500 took 7.47 mins\n",
      "Training Loss per 500 steps: 44.36316791343689\n",
      "Training Accuracy per 500 steps: 20.666666666666668\n",
      "Batch of 500 took 7.50 mins\n",
      "Training Loss per 500 steps: 41.82485381841659\n",
      "Training Accuracy per 500 steps: 24.01550193774222\n",
      "Batch of 500 took 7.42 mins\n",
      "The Total Accuracy for Epoch 0: 24.01550193774222\n",
      "Training Loss Epoch: 41.82485381841659\n",
      "Training Accuracy Epoch: 24.01550193774222\n",
      "Epoch training time: 29.72 mins\n",
      "Total training time: 29.72 mins\n",
      "Training ended on: 04 Oct 2021 - 11:36:24\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_function, optimizer, parameters['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a796d-c742-49c5-9057-5fe0fdc7bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184151f8-6e52-4dfd-8d7d-71516af33210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalSSOCClassifier(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ssoc_1d_stack): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       "  (ssoc_2d_stack): Sequential(\n",
       "    (0): Linear(in_features=777, out_features=777, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=777, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ddf71-b7a8-4291-aa38-0d1b489eb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dc6e0dc-4f0f-42e6-a568-45b55d2278b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>SSOC 2020</th>\n",
       "      <th>Cleaned_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41352</th>\n",
       "      <td>&lt;p&gt;Duties and Responsibilities:&lt;/p&gt;\\n&lt;ul&gt;\\n  &lt;...</td>\n",
       "      <td>24212</td>\n",
       "      <td>Duties and Responsibilities: Implementation of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Description SSOC 2020  \\\n",
       "41352  <p>Duties and Responsibilities:</p>\\n<ul>\\n  <...     24212   \n",
       "\n",
       "                                     Cleaned_Description  \n",
       "41352  Duties and Responsibilities: Implementation of...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_data[other_data['Cleaned_Description'] == 'Duties and Responsibilities: Implementation of Sage 300 ERP (Financials, Distribution, Project) Providing Pre & Post-Sales Consulting. Perform Business requirement analysis and provide professional advises. Install, Implement, Train and Support users on Sage 300 ERP Software. On-site and Back-end support on ERP Software. Diploma or Degree in Accountancy/Business Admin/Computer Science, Information Systems. Good Knowledge in MSSQL Server, MS Excel, Crystal Report and Visual Basic. Good analytical and problem-solving skills are essential. Good interpersonal and communication skills. Must have Sage 300 ERP Software. At least 4 - 5 years of working experience in relevant field. Must be able to work in Singapore and travel to other country.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41208371-6984-4640-adb4-84aaa54e6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7279f1f3-d29c-435d-b572-0c999f27e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25190\n",
      "The candidate will be part of one of the teams in charge of building and maintaining the payment system of the bank. The team follows agile methodology of software development. Main responsibilities o Study, evaluate and provide feedback on functional specifications o Provide detailed estimation for development of solutions o Implement solutions using TDD o Perform non-functional testing o Prepare documentation for design and implementation o Support applications in production - analyze and resolve issues as they arise. Requirements: Profile. At least 5 years in Java/J2EE development. At least 2 years of experience in big data development on Hadoop platform. Basic understanding of finance and investment banking. Java/J2EE, Spring, Spring-Boot, RESTFUL Webservices. Solution design using proven patterns, awareness of anti-patterns, performance benchmarking. Spark, Spark Streaming, HDFS, Kafka, HBase, Phoenix. Knowledge on technologies like Storm, Hive, Sqoop, Flume, Apache Ignite, RPC is an advantage. Camel, Spring Integration, JMS/Websphere MQ. Relational databases – Oracle/MSSQL tools - Maven/Gradle, Jenkins/Team City, Cucumber. Source management – SVN/GIT, TDD using Junit, DBUnit, Jira/QC.\n"
     ]
    }
   ],
   "source": [
    "other_data = data[10000:]\n",
    "test_data = other_data[other_data['SSOC 2020'] == data[0:10000]['SSOC 2020'].sample().values[0]].sample()\n",
    "test_target = test_data['SSOC 2020'].values[0]\n",
    "text = test_data['Cleaned_Description'].values[0]\n",
    "print(test_data['SSOC 2020'].values[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ed846f1-5e88-4c4c-af28-69f4ec950f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 25190\n",
      "Model predicted 1D: 2 (92.87%)\n",
      "Model predicted 2D: 25 (51.22%)\n"
     ]
    }
   ],
   "source": [
    "def generate_prediction(model, tokenizer, text, target, parameters):\n",
    "    tokenized = tokenizer(\n",
    "        text = text,\n",
    "        text_pair = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = parameters['sequence_max_length'],\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    test_ids = torch.tensor([tokenized['input_ids']], dtype = torch.long)\n",
    "    test_mask = torch.tensor([tokenized['attention_mask']], dtype = torch.long)\n",
    "    \n",
    "    model.eval()\n",
    "    preds = model(test_ids, test_mask)\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    predicted_1D = encoding['SSOC_1D']['idx_ssoc'][np.argmax(preds[\"SSOC_1D\"].detach().numpy())]\n",
    "    predicted_1D_proba = np.max(m(preds['SSOC_1D']).detach().numpy())\n",
    "    predicted_2D = encoding['SSOC_2D']['idx_ssoc'][np.argmax(preds[\"SSOC_2D\"].detach().numpy())]\n",
    "    predicted_2D_proba = np.max(m(preds['SSOC_2D']).detach().numpy())\n",
    "    \n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Model predicted 1D: {predicted_1D} ({predicted_1D_proba*100:.2f}%)\")\n",
    "    print(f\"Model predicted 2D: {predicted_2D} ({predicted_2D_proba*100:.2f}%)\")\n",
    "    \n",
    "generate_prediction(model, tokenizer, text, test_target, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5939c615-0f03-4e83-8481-9ef4fd819d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\n",
    "    text = text,\n",
    "    text_pair = None,\n",
    "    add_special_tokens = True,\n",
    "    max_length = parameters['sequence_max_length'],\n",
    "    pad_to_max_length = True,\n",
    "    return_token_type_ids = True,\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fa3dd47-526d-4c11-a294-ffedab5a056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = torch.tensor([tokenized['input_ids']], dtype = torch.long)\n",
    "test_mask = torch.tensor([tokenized['attention_mask']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf396e12-b574-484c-b190-9b744790d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_ids, test_mask)\n",
    "targets = torch.tensor([encoding['SSOC_1D']['ssoc_idx']['2']], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b7ad2d7-f060-4bf2-85d7-b0647fd085e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4207, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(preds[\"SSOC_1D\"], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00dce3d3-1e01-4c4e-8f1b-aa7f0b9c1157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['SSOC_1D']['idx_ssoc'][np.argmax(preds[\"SSOC_1D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63f0b4ea-6595-4604-85f0-c842dc16f822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['SSOC_2D']['idx_ssoc'][np.argmax(preds[\"SSOC_2D\"].detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d340a569-14e6-4610-879c-b69c78503a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0113, 0.0476, 0.0236, 0.0071, 0.1295, 0.0208, 0.0028, 0.1043, 0.1157,\n",
       "         0.0129, 0.1394, 0.0068, 0.0941, 0.0169, 0.0048, 0.0224, 0.0091, 0.0019,\n",
       "         0.0459, 0.0055, 0.0158, 0.0037, 0.0143, 0.0206, 0.0037, 0.0068, 0.0021,\n",
       "         0.0037, 0.0033, 0.0050, 0.0041, 0.0201, 0.0019, 0.0056, 0.0017, 0.0044,\n",
       "         0.0223, 0.0110, 0.0028, 0.0061, 0.0056, 0.0130]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "m(preds['SSOC_2D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39977202-1d8b-4511-b0de-dba63e556810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assist with installation, configuration and set-up of new IT accounts & IT equipment for new users. Liaising with vendors for procurement, logistic and maintenance of IT equipment. Managing & troubleshooting of office IT equipment & systems. Analyze, monitor and resolve application and system failures and provide operational support. Perform, review and enhance business and IT systems & processes for enhanced improvement for the company.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Assist with installation, configuration and set-up of new IT accounts & IT equipment for new users. Liaising with vendors for procurement, logistic and maintenance of IT equipment. Managing & troubleshooting of office IT equipment & systems. Analyze, monitor and resolve application and system failures and provide operational support. Perform, review and enhance business and IT systems & processes for enhanced improvement for the company.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fc5b-365c-426b-ae4a-4ec7456edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06838cc-934b-48f7-8154-0726ffef7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.data.Description[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets_1d': torch.tensor(self.data.SSOC_1D[index], dtype=torch.long),\n",
    "            'targets_2d': torch.tensor(self.data.SSOC_2D[index], dtype=torch.long),\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f0ba-0af3-4c04-8867-b4a08f086106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "training_set = Triage(train, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef8b3b-d1d4-4dda-b174-835feec76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ca96-b200-45ba-a7ab-13c2cf8e2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Stack 1: Predicting 1D SSOC (9)\n",
    "        self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "        # Stack 2: Predicting 2D SSOC (40 + 2 nec)\n",
    "        self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(777, 777), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(777, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 42)\n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Obtain the sentence embeddings from the DistilBERT model\n",
    "        embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = embeddings[0]\n",
    "        X = hidden_state[:, 0]\n",
    "        \n",
    "        # 1D Prediction\n",
    "        preds_1d = self.ssoc_1d_stack(X)\n",
    "        \n",
    "        # 2D Prediction\n",
    "        X = torch.cat((X, preds_1d), dim = 1)\n",
    "        preds_2d = self.ssoc_2d_stack(X)\n",
    "        \n",
    "        return preds_1d, preds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec19da7-4ecb-4eef-92ff-3bba58243d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb5c-f2ed-4635-8d82-c5b125ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss_fn\n",
    "# think of how to adjust the crossentropyloss function\n",
    "# change the targets upfront before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cd92-b91b-45ca-b4e2-fd83532500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ssoc(predicted, actual):\n",
    "    base_penalty = 10\n",
    "    penalty = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] != actual[i]:\n",
    "            penalty += base_penalty/(i+1)\n",
    "    return penalty\n",
    "\n",
    "def custom_loss_fn(top_probs_idx, targets, ssoc_level):\n",
    "          \n",
    "    if ssoc_level == '1d':\n",
    "          mapping = idx_ssoc1d\n",
    "    elif ssoc_level == '2d':\n",
    "          mapping = idx_ssoc2d\n",
    "          \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(top_probs_idx)):\n",
    "        predicted_ssoc = mapping[top_probs_idx[i].item()]\n",
    "        actual_ssoc = mapping[targets[i].item()]\n",
    "        loss += compare_ssoc(predicted_ssoc, actual_ssoc)\n",
    "        \n",
    "    return Variable(torch.tensor(float(loss)), requires_grad = True)\n",
    "\n",
    "# need to use Torch variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb2b8-516b-4e26-ab24-741e51189f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1 = Variable(torch.tensor([float(5), float(15)]), requires_grad = True)\n",
    "print(testing1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27724a-6699-4599-9ff3-47d6cc53f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40566ccf-4440-4b93-a379-7203ac10dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.tensor(float(1)), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badb587-3f24-4fc6-b4db-0df4f210129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18bd4c-cb70-44bc-b997-01838fbbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61414221-77ae-40e8-a916-b5161c227dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    # Set the NN to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(training_loader):\n",
    "        \n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "        targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Run the forward prop\n",
    "        preds_1d, preds_2d = model(ids, mask)\n",
    "        \n",
    "        # Find the indices of the top prediction\n",
    "        top_probs_1d, top_probs_idx_1d = torch.max(preds_1d.data, dim = 1)\n",
    "        top_probs_2d, top_probs_idx_2d = torch.max(preds_2d.data, dim = 1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss1 = loss_function(preds_1d, targets_1d)\n",
    "        loss2 = loss_function(preds_2d, targets_2d)\n",
    "        loss = loss1*5 + loss2\n",
    "        #print(f'Overall loss: {loss} = {loss1} + {loss2}')\n",
    "\n",
    "        # Deprecated\n",
    "        #loss = loss_function(preds_1d, targets_1d) + loss_function(preds_2d, targets_2d)\n",
    "        \n",
    "        # Add this batch's loss to the overall training loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        n_correct += calcuate_accu(top_probs_idx_2d, targets_2d)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets_2d.size(0)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 50 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 50 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822fb71-153f-4f25-8ed0-5bfc38b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70808b3-f08f-4df4-a07e-4274823285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1dd6-c7b5-4518-acc6-d16c7ef7910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d0c-a5b1-45e3-981f-df1f0f1c0c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf138-632a-4705-ad11-70e59c904cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
