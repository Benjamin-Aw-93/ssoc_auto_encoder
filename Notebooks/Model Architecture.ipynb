{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8775370-8109-40f3-a0d2-a4195063b60d",
   "metadata": {},
   "source": [
    "## Developing Hierarchical Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e984e1-4149-449d-8339-888305301829",
   "metadata": {},
   "source": [
    "Importing the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8297f1dd-799b-416d-9059-e0cee822df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092ef1f-94bd-49bc-938f-72adc267438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Enable debugging while on GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c5ffcb-3f02-4c89-98e5-fb95c9126f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssoc_autocoder import processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a95825-82a9-4bd2-ac98-3c25e88e1eb5",
   "metadata": {},
   "source": [
    "Importing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aadf5-d4de-45a2-a38b-fdbedc4f6e7c",
   "metadata": {},
   "source": [
    "Use a custom function to encode the category correctly as PyTorch requires (as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa63fd6-f7a9-408e-8104-80edfa29cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_encoding(reference_data, ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Generates encoding for SSOC to indices, as required by PyTorch\n",
    "    for multi-class classification, for the training data\n",
    "\n",
    "    Args:\n",
    "        reference_data: Pandas dataframe containing all SSOCs\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the SSOC to index mapping (for preparing the\n",
    "        dataset) and index to SSOC mapping (for interpreting the predictions),\n",
    "        for each SSOC level from 1D to 5D.\n",
    "    '''\n",
    "\n",
    "    # Initialise the dictionary object to store the encodings for each level\n",
    "    encoding = {}\n",
    "\n",
    "    # Iterate through each level from 1 to 5\n",
    "    for level in range(1, 6):\n",
    "\n",
    "        # Initialise a dictionary object to store the respective-way encodings\n",
    "        ssoc_idx_mapping = {}\n",
    "\n",
    "        # Slice the SSOC column by the level required, drop duplicates, and sort\n",
    "        ssocs = list(np.sort(reference_data[ssoc_colname].astype('str').str.slice(0, level).unique()))\n",
    "\n",
    "        # Iterate through each unique SSOC (at i-digit level) and add to dict\n",
    "        for i, ssoc in enumerate(ssocs):\n",
    "            ssoc_idx_mapping[ssoc] = i\n",
    "\n",
    "        # Add each level's encodings to the output dictionary\n",
    "        encoding[f'SSOC_{level}D'] = {\n",
    "\n",
    "            # Store the SSOC to index encoding\n",
    "            'ssoc_idx': ssoc_idx_mapping,\n",
    "            # Store the index to SSOC encoding\n",
    "            'idx_ssoc': {v: k for k, v in ssoc_idx_mapping.items()}\n",
    "        }\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def encode_dataset(data,\n",
    "                   encoding,\n",
    "                   ssoc_colname = 'SSOC 2020'):\n",
    "\n",
    "    '''\n",
    "    Uses the generated encoding to encode the SSOCs at each\n",
    "    digit level.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe of the training data with the correct SSOC\n",
    "        encoding: Encoding for each SSOC level\n",
    "        ssoc_colname: Name of the SSOC column\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe with each digit SSOC encoded correctly\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    encoded_data = copy.deepcopy(data)[~data[ssoc_colname].str.contains('X')]\n",
    "\n",
    "    # For each digit, encode the SSOC correctly\n",
    "    for ssoc_level, encodings in encoding.items():\n",
    "        encoded_data[ssoc_level] = encoded_data[ssoc_colname].astype('str').str.slice(0, int(ssoc_level[5])).replace(encodings['ssoc_idx'])\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "# Create a new Python class to handle the additional complexity\n",
    "class SSOC_Dataset(Dataset):\n",
    "\n",
    "    # Define the class attributes\n",
    "    def __init__(self, dataframe, encoding, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Define the iterable over the Dataset object \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Extract the text\n",
    "        text = self.data[colnames['job_description']][index]\n",
    "\n",
    "        # Pass in the data into the tokenizer\n",
    "        inputs = self.tokenizer(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        # Extract the IDs and attention mask\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Return all the outputs needed for training and evaluation\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'SSOC_1D': torch.tensor(self.data.SSOC_1D[index], dtype = torch.long),\n",
    "            'SSOC_2D': torch.tensor(self.data.SSOC_2D[index], dtype = torch.long),\n",
    "            'SSOC_3D': torch.tensor(self.data.SSOC_3D[index], dtype = torch.long),\n",
    "            'SSOC_4D': torch.tensor(self.data.SSOC_4D[index], dtype = torch.long),\n",
    "            'SSOC_5D': torch.tensor(self.data.SSOC_5D[index], dtype = torch.long),\n",
    "        } \n",
    "\n",
    "    # Define the length attribute\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44d9967-0376-4345-b512-539480a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(encoded_data,\n",
    "                 colnames,\n",
    "                 parameters):\n",
    "    \n",
    "    # Split the dataset into training and testing\n",
    "    training_set_number = int(len(encoded_data)*0.8)\n",
    "    testing_set_number = len(encoded_data) - int(len(encoded_data)*0.8)\n",
    "    training_data, testing_data = train_test_split(encoded_data,\n",
    "                                                   test_size = 0.8,\n",
    "                                                   random_state = 2021)\n",
    "    training_data.reset_index(drop = True, inplace = True)\n",
    "    testing_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(parameters['pretrained_model'])\n",
    "    \n",
    "    # Creating the dataset and dataloader for the neural network\n",
    "    training_loader = DataLoader(SSOC_Dataset(training_data, encoding, tokenizer, parameters['sequence_max_length']),\n",
    "                                 batch_size = parameters['training_batch_size'],\n",
    "                                 num_workers = parameters['num_workers'],\n",
    "                                 shuffle = True)\n",
    "    testing_loader = DataLoader(SSOC_Dataset(testing_data, encoding, tokenizer, parameters['sequence_max_length']),\n",
    "                                batch_size = parameters['training_batch_size'],\n",
    "                                num_workers = parameters['num_workers'],\n",
    "                                shuffle = True)\n",
    "    \n",
    "    return training_loader, testing_loader\n",
    "\n",
    "def prepare_model(encoding, parameters):\n",
    "    \n",
    "    class HierarchicalSSOCClassifier(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(HierarchicalSSOCClassifier, self).__init__()\n",
    "            \n",
    "            self.l1 = DistilBertModel.from_pretrained(parameters['pretrained_model'])\n",
    "\n",
    "            # Generating dimensions\n",
    "            SSOC_1D_count = len(encoding['SSOC_1D']['ssoc_idx'].keys())\n",
    "            SSOC_2D_count = len(encoding['SSOC_2D']['ssoc_idx'].keys())\n",
    "            SSOC_3D_count = len(encoding['SSOC_3D']['ssoc_idx'].keys())\n",
    "            SSOC_4D_count = len(encoding['SSOC_4D']['ssoc_idx'].keys())\n",
    "            SSOC_5D_count = len(encoding['SSOC_5D']['ssoc_idx'].keys())            \n",
    "            \n",
    "            # Stack 1: Predicting 1D SSOC (9)\n",
    "            if parameters['max_level'] >= 1:\n",
    "                self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(768, 768), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(768, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_1D_count)\n",
    "                )\n",
    "\n",
    "            # Stack 2: Predicting 2D SSOC (42)\n",
    "            if parameters['max_level'] >= 2:\n",
    "                n_dims_2d = 768 + SSOC_1D_count\n",
    "                self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_dims_2d, n_dims_2d), \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(n_dims_2d, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(0.3),\n",
    "                    torch.nn.Linear(128, SSOC_2D_count)\n",
    "                )        \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "\n",
    "            # Obtain the sentence embeddings from the DistilBERT model\n",
    "            embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_state = embeddings[0]\n",
    "            X = hidden_state[:, 0]\n",
    "\n",
    "            predictions = {}\n",
    "            \n",
    "            # 1D Prediction\n",
    "            if parameters['max_level'] >= 1:\n",
    "                predictions['SSOC_1D'] = self.ssoc_1d_stack(X)\n",
    "\n",
    "            # 2D Prediction\n",
    "            if parameters['max_level'] >= 2:\n",
    "                X = torch.cat((X, predictions['SSOC_1D']), dim = 1)\n",
    "                predictions['SSOC_2D'] = self.ssoc_2d_stack(X)\n",
    "\n",
    "            return {f'SSOC_{i}D': predictions[f'SSOC_{i}D'] for i in range(1, parameters['max_level'] + 1)}\n",
    "        \n",
    "    model = HierarchicalSSOCClassifier()\n",
    "    model.to(parameters['device'])\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr = parameters['learning_rate'])\n",
    "    \n",
    "    return model, loss_function, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd14b71-6820-4f26-a338-04d2590df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training started on:\", current_time)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Set the NN to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over each batch\n",
    "        for batch, data in enumerate(training_loader):\n",
    "\n",
    "            # Extract the data\n",
    "            ids = data['ids'].to(parameters['device'], dtype = torch.long)\n",
    "            mask = data['mask'].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "            # Run the forward prop\n",
    "            predictions = model(ids, mask)\n",
    "\n",
    "            # Iterate through each SSOC level\n",
    "            for ssoc_level, preds in predictions.items():\n",
    "\n",
    "                # Extract the correct target for the SSOC level\n",
    "                targets = data[ssoc_level].to(parameters['device'], dtype = torch.long)\n",
    "\n",
    "                # Compute the loss function using the predictions and the targets\n",
    "                level_loss = loss_function(preds, targets)\n",
    "\n",
    "                # Initialise the loss variable if this is the 1D level\n",
    "                # Else add to the loss variable\n",
    "                # Note the weights on each level\n",
    "                if ssoc_level == 'SSOC_1D':\n",
    "                    loss = level_loss * parameters['loss_weights'][ssoc_level]\n",
    "                else:\n",
    "                    loss += level_loss * parameters['loss_weights'][ssoc_level]\n",
    "\n",
    "            # Use the deepest level predictions to calculate accuracy\n",
    "            top_probs, top_probs_idx = torch.max(preds.data, dim = 1)\n",
    "            n_correct += calculate_accu(top_probs_idx, targets)\n",
    "\n",
    "            # Calculate the loss\n",
    "    #         targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "    #         targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "    #         loss1 = loss_function(preds_1d, targets_1d)\n",
    "    #         loss2 = loss_function(preds_2d, targets_2d)\n",
    "    #         loss = loss1*5 + loss2\n",
    "\n",
    "            # Add this batch's loss to the overall training loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 500 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples \n",
    "                print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "                print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "                print(f\"Batch of 500 took {(time.time() - batch_start_time)/60:.2f} mins\")\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "        print(f\"Epoch training time: {(time.time() - epoch_start_time)/60:.2f} mins\")\n",
    "\n",
    "    print(f\"Total training time: {(time.time() - start_time)/60:.2f} mins\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%d %b %Y - %H:%M:%S\")\n",
    "    print(\"Training ended on:\", current_time)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9f8869-10b0-4ec8-9a37-977c4882278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = {\n",
    "    'SSOC': 'SSOC 2020',\n",
    "    'job_description': 'Cleaned_Description'\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'sequence_max_length': 512,\n",
    "    'max_level': 2,\n",
    "    'training_batch_size': 4,\n",
    "    'validation_batch_size': 2,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 1e-05,\n",
    "    'pretrained_model': 'distilbert-base-uncased',\n",
    "    'num_workers': 0,\n",
    "    'loss_weights': {\n",
    "        'SSOC_1D': 20,\n",
    "        'SSOC_2D': 5,\n",
    "        'SSOC_3D': 3,\n",
    "        'SSOC_4D': 2,\n",
    "        'SSOC_5D': 1\n",
    "    },\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0da49c-bf7e-4e30-827b-11da01be9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/Processed/Training/train_full.csv')\n",
    "SSOC_2020 = pd.read_csv('Data/Processed/Training/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9587b798-77c2-47e6-a7d4-ea38b8959b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoding = generate_encoding(SSOC_2020)\n",
    "encoded_data = encode_dataset(data[0:5000], encoding)\n",
    "training_loader, testing_loader = prepare_data(encoded_data, colnames, parameters)\n",
    "model, loss_function, optimizer = prepare_model(encoding, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa083cd5-ccad-457b-9ebe-d95ad39b6896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.67775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)/4/50*45/3600*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb40d7-95ea-4ca4-bc1c-2334cd9a953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, loss_function, optimizer, parameters['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a796d-c742-49c5-9057-5fe0fdc7bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39977202-1d8b-4511-b0de-dba63e556810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4fc5b-365c-426b-ae4a-4ec7456edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06838cc-934b-48f7-8154-0726ffef7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.data.Description[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets_1d': torch.tensor(self.data.SSOC_1D[index], dtype=torch.long),\n",
    "            'targets_2d': torch.tensor(self.data.SSOC_2D[index], dtype=torch.long),\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f0ba-0af3-4c04-8867-b4a08f086106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "training_set = Triage(train, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef8b3b-d1d4-4dda-b174-835feec76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ca96-b200-45ba-a7ab-13c2cf8e2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Stack 1: Predicting 1D SSOC (9)\n",
    "        self.ssoc_1d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(768, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "        # Stack 2: Predicting 2D SSOC (40 + 2 nec)\n",
    "        self.ssoc_2d_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(777, 777), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(777, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 42)\n",
    "        )        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Obtain the sentence embeddings from the DistilBERT model\n",
    "        embeddings = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = embeddings[0]\n",
    "        X = hidden_state[:, 0]\n",
    "        \n",
    "        # 1D Prediction\n",
    "        preds_1d = self.ssoc_1d_stack(X)\n",
    "        \n",
    "        # 2D Prediction\n",
    "        X = torch.cat((X, preds_1d), dim = 1)\n",
    "        preds_2d = self.ssoc_2d_stack(X)\n",
    "        \n",
    "        return preds_1d, preds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec19da7-4ecb-4eef-92ff-3bba58243d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb5c-f2ed-4635-8d82-c5b125ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss_fn\n",
    "# think of how to adjust the crossentropyloss function\n",
    "# change the targets upfront before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cd92-b91b-45ca-b4e2-fd83532500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ssoc(predicted, actual):\n",
    "    base_penalty = 10\n",
    "    penalty = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] != actual[i]:\n",
    "            penalty += base_penalty/(i+1)\n",
    "    return penalty\n",
    "\n",
    "def custom_loss_fn(top_probs_idx, targets, ssoc_level):\n",
    "          \n",
    "    if ssoc_level == '1d':\n",
    "          mapping = idx_ssoc1d\n",
    "    elif ssoc_level == '2d':\n",
    "          mapping = idx_ssoc2d\n",
    "          \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(top_probs_idx)):\n",
    "        predicted_ssoc = mapping[top_probs_idx[i].item()]\n",
    "        actual_ssoc = mapping[targets[i].item()]\n",
    "        loss += compare_ssoc(predicted_ssoc, actual_ssoc)\n",
    "        \n",
    "    return Variable(torch.tensor(float(loss)), requires_grad = True)\n",
    "\n",
    "# need to use Torch variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb2b8-516b-4e26-ab24-741e51189f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1 = Variable(torch.tensor([float(5), float(15)]), requires_grad = True)\n",
    "print(testing1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27724a-6699-4599-9ff3-47d6cc53f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d85028-3b41-411e-8eed-36eb9d5cd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea7f7b-d7e6-4d3b-92c1-34ca1f5b14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40566ccf-4440-4b93-a379-7203ac10dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.tensor(float(1)), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badb587-3f24-4fc6-b4db-0df4f210129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18bd4c-cb70-44bc-b997-01838fbbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61414221-77ae-40e8-a916-b5161c227dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    # Set the NN to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over each batch\n",
    "    for batch, data in enumerate(training_loader):\n",
    "        \n",
    "        # Extract the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets_1d = data['targets_1d'].to(device, dtype = torch.long)\n",
    "        targets_2d = data['targets_2d'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Run the forward prop\n",
    "        preds_1d, preds_2d = model(ids, mask)\n",
    "        \n",
    "        # Find the indices of the top prediction\n",
    "        top_probs_1d, top_probs_idx_1d = torch.max(preds_1d.data, dim = 1)\n",
    "        top_probs_2d, top_probs_idx_2d = torch.max(preds_2d.data, dim = 1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss1 = loss_function(preds_1d, targets_1d)\n",
    "        loss2 = loss_function(preds_2d, targets_2d)\n",
    "        loss = loss1*5 + loss2\n",
    "        #print(f'Overall loss: {loss} = {loss1} + {loss2}')\n",
    "\n",
    "        # Deprecated\n",
    "        #loss = loss_function(preds_1d, targets_1d) + loss_function(preds_2d, targets_2d)\n",
    "        \n",
    "        # Add this batch's loss to the overall training loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        n_correct += calcuate_accu(top_probs_idx_2d, targets_2d)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets_2d.size(0)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 50 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 50 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822fb71-153f-4f25-8ed0-5bfc38b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70808b3-f08f-4df4-a07e-4274823285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f1dd6-c7b5-4518-acc6-d16c7ef7910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d0c-a5b1-45e3-981f-df1f0f1c0c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf138-632a-4705-ad11-70e59c904cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
