{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd8e7ca-ffd2-4497-b610-ff2a3a94c113",
   "metadata": {},
   "source": [
    "## Model training on AWS Sagemaker\n",
    "\n",
    "**Author:** Shaun Khoo  \n",
    "**Date:** 15 Oct 2021  \n",
    "**Context:** Training on the local computer is taking too much time, would be much faster if we could train our models on AWS Sagemaker instead  \n",
    "**Objective:** Develop code that will help us train our model directly on AWS Sagemaker   \n",
    "\n",
    "**Note:** Referencing [this notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32000b95-981e-45b9-96c0-cfec4d827d40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A) Importing the required libraries\n",
    "\n",
    "Note that your AWS credentials need to be set up on the AWS CLI first before this can work seamlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5124a4d-1dec-469e-91fc-a1afca2a073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc471fd8-ff0c-42fa-938a-70821b010a10",
   "metadata": {},
   "source": [
    "The code below returns your IAM user (as an Amazon Resource Number or `Arn`). Make sure the code runs below - this ensures you are logged in correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6655e1-f6ed-4873-ae90-e9c18ebfc200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UserId': 'AIDAYUZMQUYGUNJ2VXD2E',\n",
       " 'Account': '594409465357',\n",
       " 'Arn': 'arn:aws:iam::594409465357:user/shaunkhoo',\n",
       " 'ResponseMetadata': {'RequestId': '314e2f55-26ea-4a35-8e43-b1d9e587238b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '314e2f55-26ea-4a35-8e43-b1d9e587238b',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '406',\n",
       "   'date': 'Wed, 29 Dec 2021 06:31:34 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts = boto3.client('sts')\n",
    "sts.get_caller_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9e89f-3e2e-41bd-9a90-f88c3722da65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Changing the working directory to the top-level folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f739ac6-a13a-4163-8def-b111e04da800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5abbf4-fb8f-44b9-9175-370e1a1653da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B) Setting up Sagemaker and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f557f-80e4-4670-aff4-ce168963b4a7",
   "metadata": {},
   "source": [
    "Initialising the Sagemaker session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef99523-7d47-459b-8ee3-92aa3fd67359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f725b1-c745-4544-b046-a40c8f3a37e8",
   "metadata": {},
   "source": [
    "Obtaining the default bucket for our Sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45350138-5e57-4dd8-8cd1-579f373422e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: sagemaker-us-east-1-594409465357\n"
     ]
    }
   ],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Bucket Name: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef0418-9b07-4202-847e-fe801b6896b9",
   "metadata": {},
   "source": [
    "Set the prefix for where you want to store your data / model files in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adda58e6-764f-4cde-95ad-6dab605c3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Sagemaker/ssoc-autocoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2fa33-bf22-4a92-a49e-7827e8f3637d",
   "metadata": {},
   "source": [
    "Run the code below to retrieve Sagemaker's execution role. Note that the role we have set up is called `mom-aws`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984e14c6-b545-4ffd-8700-477eb89d400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name shaunkhoo to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Role: arn:aws:iam::594409465357:role/mom-aws\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName = 'mom-aws')['Role']['Arn']\n",
    "print(f\"Execution Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c38ca9-0e94-4466-ad0b-f1d55b945dff",
   "metadata": {},
   "source": [
    "Upload the raw data to the S3 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e958a02-e261-42eb-840e-c92e5c4f1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path = \"Data/Train/pre-training-sample1000.txt\", \n",
    "                                       bucket = bucket, \n",
    "                                       key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb93f0e8-ed80-4cc3-ad40-df38bac2d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs stored in: s3://sagemaker-us-east-1-594409465357/Sagemaker/ssoc-autocoder/pre-training-sample1000.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs stored in: {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863e84d-a704-4978-b867-742eec062c07",
   "metadata": {},
   "source": [
    "#### C) Language modelling (or pretraining) on Sagemaker\n",
    "\n",
    "Running masked language modelling to finetune the DistilBERT model on MCF data to improve downstream classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ff523-1d8d-49ac-bb8f-ec68839fe52d",
   "metadata": {},
   "source": [
    "Define the hyperparameters that need to be passed onto the masked language modelling script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b222b53e-2954-4994-8803-8c9a2bac99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_parameters = {\n",
    "    'model_name_or_path': 'distilbert-base-uncased',\n",
    "    'train_file': \"pre-training-full.txt\",\n",
    "    'line_by_line': True,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'logging_steps': 500,\n",
    "    'save_strategy': 'no',\n",
    "    'overwrite_output_dir': True,\n",
    "    'output_dir': '20211228_test'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2f8c6-ccc9-4c35-8254-12ee6c009c1c",
   "metadata": {},
   "source": [
    "Create the estimator object and run it on the full pretraining text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238df6a3-4451-4a6b-9565-cb6b8b9c2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_estimator = PyTorch(\n",
    "    entry_point = \"run_mlm_aws.py\",\n",
    "    role = role,\n",
    "    framework_version = \"1.8.1\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.g4dn.xlarge\",\n",
    "    source_dir = \"ssoc_autocoder\",\n",
    "    py_version = \"py3\",\n",
    "    env = {'SAGEMAKER_REQUIREMENTS': '../Notebooks/aws-training-requirements/pretrain/requirements.txt'},\n",
    "    hyperparameters = mlm_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35b996-0f7d-4868-9658-080241d56f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-29 06:32:02 Starting - Starting the training job...\n",
      "2021-12-29 06:32:05 Starting - Launching requested ML instancesProfilerReport-1640759519: InProgress\n",
      "...\n",
      "2021-12-29 06:33:05 Starting - Preparing the instances for training......\n",
      "2021-12-29 06:34:20 Downloading - Downloading input data......\n",
      "2021-12-29 06:35:24 Training - Downloading the training image..................\n",
      "2021-12-29 06:38:51 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-29 06:38:51,732 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-29 06:38:51,755 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-29 06:38:54,780 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-29 06:38:55,150 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==1.16.1\n",
      "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.61.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.16.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.16.1->-r requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-1.16.1 huggingface-hub-0.2.1 regex-2021.11.10 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.10.0 xxhash-2.0.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-12-29 06:39:02,084 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"do_train\": true,\n",
      "        \"overwrite_output_dir\": true,\n",
      "        \"save_strategy\": \"no\",\n",
      "        \"do_eval\": true,\n",
      "        \"train_file\": \"pre-training-full.txt\",\n",
      "        \"line_by_line\": true,\n",
      "        \"output_dir\": \"20211228_test\",\n",
      "        \"logging_steps\": 500,\n",
      "        \"model_name_or_path\": \"distilbert-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-12-29-06-31-56-789\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-12-29-06-31-56-789/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_mlm_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_mlm_aws.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"line_by_line\":true,\"logging_steps\":500,\"model_name_or_path\":\"distilbert-base-uncased\",\"output_dir\":\"20211228_test\",\"overwrite_output_dir\":true,\"save_strategy\":\"no\",\"train_file\":\"pre-training-full.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_mlm_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_mlm_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-12-29-06-31-56-789/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"line_by_line\":true,\"logging_steps\":500,\"model_name_or_path\":\"distilbert-base-uncased\",\"output_dir\":\"20211228_test\",\"overwrite_output_dir\":true,\"save_strategy\":\"no\",\"train_file\":\"pre-training-full.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-12-29-06-31-56-789\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-12-29-06-31-56-789/source/sourcedir.tar.gz\",\"module_name\":\"run_mlm_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_mlm_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--evaluation_strategy\",\"epoch\",\"--line_by_line\",\"True\",\"--logging_steps\",\"500\",\"--model_name_or_path\",\"distilbert-base-uncased\",\"--output_dir\",\"20211228_test\",\"--overwrite_output_dir\",\"True\",\"--save_strategy\",\"no\",\"--train_file\",\"pre-training-full.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_OVERWRITE_OUTPUT_DIR=true\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=no\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=pre-training-full.txt\u001b[0m\n",
      "\u001b[34mSM_HP_LINE_BY_LINE=true\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=20211228_test\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_mlm_aws.py --do_eval True --do_train True --evaluation_strategy epoch --line_by_line True --logging_steps 500 --model_name_or_path distilbert-base-uncased --output_dir 20211228_test --overwrite_output_dir True --save_strategy no --train_file pre-training-full.txt\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=20211228_test/runs/Dec29_06-39-06_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/20211228_test,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=32,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=16,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=20211228_test,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=20211228_test,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:06 - WARNING - datasets.builder - Using custom data configuration default-1e0885708b3e267d\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:06 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:06 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:10 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:10 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:10 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - WARNING - datasets.builder - Using custom data configuration default-1e0885708b3e267d\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - WARNING - datasets.builder - Using custom data configuration default-1e0885708b3e267d\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-12-29 06:39:16,408 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjk5o_nb2\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-12-29 06:39:16,434 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-12-29 06:39:16,435 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-12-29 06:39:16,435 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-12-29 06:39:16,435 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-12-29 06:39:16,462 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnhifdm0u\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-12-29 06:39:16,486 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-12-29 06:39:16,486 >> creating metadata file for /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-12-29 06:39:16,512 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-12-29 06:39:16,512 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-12-29 06:39:16,564 >> https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpatmbv9gz\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-12-29 06:39:16,596 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-12-29 06:39:16,596 >> creating metadata file for /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-12-29 06:39:16,622 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu2agc35_\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-12-29 06:39:16,659 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-12-29 06:39:16,659 >> creating metadata file for /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-12-29 06:39:16,735 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-12-29 06:39:16,735 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-12-29 06:39:16,735 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-12-29 06:39:16,735 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-12-29 06:39:16,735 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-12-29 06:39:16,762 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-12-29 06:39:16,762 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-12-29 06:39:16,818 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzsminflj\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-12-29 06:39:19,710 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-12-29 06:39:19,710 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1279] 2021-12-29 06:39:19,711 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1524] 2021-12-29 06:39:20,668 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1533] 2021-12-29 06:39:20,668 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m{'text': \"In 2009 MoneySmart was founded with the simple aim to help Singaporeans make better financial decisions. Today, we're Singapore's #1 personal finance portal, with fast expansion into the rest of the region. To continue charting new territory, our amazing team of product experience designers and researchers is seeking a like-minded maker to join our crew. Together, we're creating products and services that are useful, usable and desirable for end-users while constantly innovating to impact the category from the ground, up. Your mission as an Associate Product Experience Researcher at MoneySmart is to design best-in-class user experiences to aid consumers in reaching their personal finance goals. The role requires you to work closely with the MoneySmart product, design, technology, marketing and commercial teams to develop digital products and services for the personal finance sector. In a Nutshell: You will be working in a team of product experience designers, product managers, and engineers to actively define the product strategy. You will be hyper-focused on the end user, able to see your insights' impact on their overall experience. You are expected to suggest methodology for eliciting feedback on concepts to finished products, and synthesise results in a fast-paced environment.\"}\u001b[0m\n",
      "\u001b[34m{'text': 'The School of Mechanical and Aerospace Engineering (MAE) boasts comprehensive state-of-the-art facilities and a faculty comprising more than 130 professors, making it one of the largest mechanical engineering schools in the world. Faculty members are drawn from renowned universities worldwide, providing a wealth of collective expertise in traditional mechanical and aerospace engineering, and in specialities including manufacturing, mechatronics, innovative design, nanotechnology, and biomedical and computational applications. A Research Associate (RA) positions are available for a research project working on the investigating and understanding the new paradigm for high impact research work. Singapore has made great strides in establishing R&D as a major pillar in the transformation of its economy. Understanding the new paradigm of high quality research work in Singapore is of paramount importance to the progress and sustainability of research in Singapore. Key Responsibilities: Conduct literature searches and write literature reviews. Collect data on publications, collaborations and their impact. Identify factors that contribute to the conducting of high quality research work resulting in high impact collaborations and publications. Assist in writing research reports.'}\u001b[0m\n",
      "\u001b[34m12/29/2021 06:39:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-7ec6285a062985ac.arrow\u001b[0m\n",
      "\u001b[34m12/29/2021 06:45:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1e0885708b3e267d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-b9e967e6b1db87aa.arrow\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:521] 2021-12-29 06:45:26,826 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2021-12-29 06:45:27,017 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2021-12-29 06:45:27,017 >>   Num examples = 838103\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2021-12-29 06:45:27,017 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-12-29 06:45:27,017 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-12-29 06:45:27,017 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-12-29 06:45:27,017 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-12-29 06:45:27,017 >>   Total optimization steps = 52382\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.237 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.334 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.335 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.335 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.336 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:27.336 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.024 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.025 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.026 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.027 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.028 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:vocab_transform.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:vocab_transform.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:vocab_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:vocab_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:591] name:vocab_projector.bias count_params:30522\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:593] Total Trainable Params: 66985530\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.029 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-12-29 06:45:28.030 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m{'loss': 2.6135, 'learning_rate': 4.9522736817990916e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m{'loss': 2.37, 'learning_rate': 4.904547363598182e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m{'loss': 2.2644, 'learning_rate': 4.856821045397274e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m{'loss': 2.1973, 'learning_rate': 4.809094727196366e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m{'loss': 2.1411, 'learning_rate': 4.761368408995457e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m{'loss': 2.1014, 'learning_rate': 4.713642090794548e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m{'loss': 2.0749, 'learning_rate': 4.665915772593639e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m{'loss': 2.0439, 'learning_rate': 4.6181894543927306e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m{'loss': 2.0073, 'learning_rate': 4.570463136191822e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m{'loss': 2.001, 'learning_rate': 4.522736817990913e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 1.977, 'learning_rate': 4.475010499790005e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 1.9627, 'learning_rate': 4.427284181589096e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m{'loss': 1.9403, 'learning_rate': 4.379557863388187e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m{'loss': 1.9135, 'learning_rate': 4.331831545187278e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m{'loss': 1.9069, 'learning_rate': 4.2841052269863695e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m{'loss': 1.9054, 'learning_rate': 4.236378908785461e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8867, 'learning_rate': 4.188652590584552e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8774, 'learning_rate': 4.1409262723836436e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8781, 'learning_rate': 4.093199954182735e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8551, 'learning_rate': 4.0454736359818264e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8354, 'learning_rate': 3.997747317780917e-05, 'epoch': 0.2}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mlm_estimator.fit({\"training\": 's3://sagemaker-us-east-1-594409465357/Sagemaker/ssoc-autocoder/pre-training-full.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33407fc9-1ead-42b5-94de-424bfe621125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb91c1-065b-47cd-93f6-0c02ac86df9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "395b4560-1646-4707-9864-4e4a8dd5f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"train_aws.py\",\n",
    "    role = role,\n",
    "    framework_version = \"1.8.1\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.g4dn.xlarge\",\n",
    "    source_dir = \"ssoc_autocoder\",\n",
    "    py_version = \"py3\",\n",
    "    env = env,\n",
    "    #use_spot_instances = True,\n",
    "    #max_run = ,\n",
    "    #max_wait = 600,\n",
    "    hyperparameters = {\"epochs\": 1, \"tied\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a5c2bf6-6b9b-4370-a4b4-340681f1bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 07:40:29 Starting - Starting the training job...\n",
      "2021-10-11 07:40:55 Starting - Launching requested ML instancesProfilerReport-1633938053: InProgress\n",
      "......\n",
      "2021-10-11 07:42:04 Starting - Preparing the instances for training.........\n",
      "2021-10-11 07:43:40 Downloading - Downloading input data...\n",
      "2021-10-11 07:44:16 Training - Downloading the training image......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,655 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,679 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,689 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:30,459 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:38,001 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-07-40-51-031\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-07-40-51-031\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mLoading pretrained model...\u001b[0m\n",
      "\u001b[34mModel loaded successfully!\u001b[0m\n",
      "\u001b[34mTraining started on: 11 Oct 2021 - 07:49:00\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 1 started on: 11 Oct 2021 - 07:49:00\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.742 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.940 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.940 algo-1:31 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.941 algo-1:31 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.942 algo-1:31 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.942 algo-1:31 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.0.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.0.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.3.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.3.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.6.weight count_params:98304\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.6.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.9.weight count_params:1152\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.9.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.0.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.0.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.3.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.3.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.6.weight count_params:198912\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.6.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.9.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.9.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.12.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.12.bias count_params:42\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.0.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.0.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.3.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.3.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.6.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.6.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.9.weight count_params:419328\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.9.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.12.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.12.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.15.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.15.bias count_params:144\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.0.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.0.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.3.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.3.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.6.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.6.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.9.weight count_params:739584\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.9.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.12.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.12.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.15.weight count_params:211456\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.15.bias count_params:413\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.0.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.0.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.3.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.3.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.6.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.6.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.9.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.9.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.12.weight count_params:1371872\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.12.bias count_params:997\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:593] Total Trainable Params: 18413009\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.527 algo-1:31 INFO hook.py:488] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 07:49:37 Training - Training image download completed. Training in progress.\u001b[34m>> Training Loss per 200 steps: 52.4779 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.22%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.41 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 52.1093 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.52%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.41 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 52.1961 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.54%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 51.8572 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.72%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 51.7501 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.76%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 1 Loss = #011Training: 51.699  #011Validation: 49.237\u001b[0m\n",
      "\u001b[34m> Epoch 1 Accuracy = #011Training: 13.87%  #011Validation: 13.88%\u001b[0m\n",
      "\u001b[34m> Epoch 1 took 22.73 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 2 started on: 11 Oct 2021 - 08:11:44\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.8340 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.67%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.46 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.7493 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.82%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5619 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.88%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.3199 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.64%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.1403 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.63%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 2 Loss = #011Training: 50.198  #011Validation: 46.922\u001b[0m\n",
      "\u001b[34m> Epoch 2 Accuracy = #011Training: 14.68%  #011Validation: 15.65%\u001b[0m\n",
      "\u001b[34m> Epoch 2 took 22.83 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 3 started on: 11 Oct 2021 - 08:34:33\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.1103 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.12%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.46 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.5551 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.70%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3934 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.77%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3123 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.93%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30468/4121110664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"None\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1653\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1654\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[0;32m   3675\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3677\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3679\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc28f8c-2dbf-48d8-b3ae-e710536bc2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 08:58:49 Starting - Starting the training job...\n",
      "2021-10-11 08:59:13 Starting - Launching requested ML instancesProfilerReport-1633942752: InProgress\n",
      "...\n",
      "2021-10-11 08:59:53 Starting - Preparing the instances for training.........\n",
      "2021-10-11 09:01:37 Downloading - Downloading input data\n",
      "2021-10-11 09:01:37 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:49,253 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:49,273 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:50,703 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:51,289 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:57,087 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-08-59-10-592\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-08-59-10-592\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mLoading pretrained model...\u001b[0m\n",
      "\u001b[34mModel loaded successfully!\u001b[0m\n",
      "\u001b[34mTraining started on: 11 Oct 2021 - 09:06:14\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 1 started on: 11 Oct 2021 - 09:06:14\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.252 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.438 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.439 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.439 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.440 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.440 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.938 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.0.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.0.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.3.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.3.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.6.weight count_params:98304\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.6.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.9.weight count_params:1152\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.9.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.0.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.0.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.3.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.3.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.6.weight count_params:198912\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.6.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.9.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.9.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.12.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.12.bias count_params:42\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.0.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.0.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.3.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.3.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.6.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.6.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.9.weight count_params:419328\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.9.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.12.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.12.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.15.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.15.bias count_params:144\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.0.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.0.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.3.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.3.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.6.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.6.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.9.weight count_params:739584\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.9.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.12.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.12.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.15.weight count_params:211456\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.15.bias count_params:413\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.0.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.0.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.3.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.3.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.6.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.6.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.9.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.9.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.12.weight count_params:1371872\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.12.bias count_params:997\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.942 algo-1:32 INFO hook.py:593] Total Trainable Params: 18413009\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.942 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.944 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 09:06:39 Training - Training image download completed. Training in progress.\u001b[34m>> Training Loss per 200 steps: 51.5119 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.88%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.78 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.9504 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.49%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.6563 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.76%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5482 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.81%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5025 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.72%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.84 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 1 Loss = #011Training: 50.465  #011Validation: 48.061\u001b[0m\n",
      "\u001b[34m> Epoch 1 Accuracy = #011Training: 14.78%  #011Validation: 16.75%\u001b[0m\n",
      "\u001b[34m> Epoch 1 took 12.20 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 2 started on: 11 Oct 2021 - 09:18:26\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3656 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.27%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3617 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.02%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3198 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.08%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3620 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.04%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.2366 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.17%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 2 Loss = #011Training: 49.235  #011Validation: 47.097\u001b[0m\n",
      "\u001b[34m> Epoch 2 Accuracy = #011Training: 16.18%  #011Validation: 17.04%\u001b[0m\n",
      "\u001b[34m> Epoch 2 took 12.34 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 3 started on: 11 Oct 2021 - 09:30:46\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6243 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.70%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6804 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.00%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.7788 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.58%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6815 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.65%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.4011 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.73%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 3 Loss = #011Training: 48.296  #011Validation: 46.215\u001b[0m\n",
      "\u001b[34m> Epoch 3 Accuracy = #011Training: 16.69%  #011Validation: 17.29%\u001b[0m\n",
      "\u001b[34m> Epoch 3 took 12.35 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 4 started on: 11 Oct 2021 - 09:43:08\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.8530 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.02%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.9214 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.91%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7115 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.36%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7667 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.63%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7688 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.57%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 4 Loss = #011Training: 47.671  #011Validation: 45.670\u001b[0m\n",
      "\u001b[34m> Epoch 4 Accuracy = #011Training: 17.64%  #011Validation: 18.36%\u001b[0m\n",
      "\u001b[34m> Epoch 4 took 12.36 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 5 started on: 11 Oct 2021 - 09:55:29\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.0637 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.91%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.6514 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.97%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.80 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.4380 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 18.07%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.67 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.4385 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.99%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.69 mins\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5fe04-b2e1-47a0-a15d-803b94ca102f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
