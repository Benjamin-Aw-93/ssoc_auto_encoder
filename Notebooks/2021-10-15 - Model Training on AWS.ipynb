{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd8e7ca-ffd2-4497-b610-ff2a3a94c113",
   "metadata": {},
   "source": [
    "## Model training on AWS Sagemaker\n",
    "\n",
    "**Author:** Shaun Khoo  \n",
    "**Date:** 15 Oct 2021  \n",
    "**Context:** Training on the local computer is taking too much time, would be much faster if we could train our models on AWS Sagemaker instead  \n",
    "**Objective:** Develop code that will help us train our model directly on AWS Sagemaker   \n",
    "\n",
    "**Note:** Referencing [this notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32000b95-981e-45b9-96c0-cfec4d827d40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A) Importing the required libraries\n",
    "\n",
    "Note that your AWS credentials need to be set up on the AWS CLI first before this can work seamlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5124a4d-1dec-469e-91fc-a1afca2a073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc471fd8-ff0c-42fa-938a-70821b010a10",
   "metadata": {},
   "source": [
    "The code below returns your IAM user (as an Amazon Resource Number or `Arn`). Make sure the code runs below - this ensures you are logged in correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6655e1-f6ed-4873-ae90-e9c18ebfc200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UserId': 'AIDAYUZMQUYGUNJ2VXD2E',\n",
       " 'Account': '594409465357',\n",
       " 'Arn': 'arn:aws:iam::594409465357:user/shaunkhoo',\n",
       " 'ResponseMetadata': {'RequestId': '36cedfbf-9749-4965-957b-b4c1c7d89981',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '36cedfbf-9749-4965-957b-b4c1c7d89981',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '406',\n",
       "   'date': 'Sat, 08 Jan 2022 00:45:52 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts = boto3.client('sts')\n",
    "sts.get_caller_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9e89f-3e2e-41bd-9a90-f88c3722da65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Changing the working directory to the top-level folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f739ac6-a13a-4163-8def-b111e04da800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5abbf4-fb8f-44b9-9175-370e1a1653da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B) Setting up Sagemaker and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f557f-80e4-4670-aff4-ce168963b4a7",
   "metadata": {},
   "source": [
    "Initialising the Sagemaker session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef99523-7d47-459b-8ee3-92aa3fd67359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f725b1-c745-4544-b046-a40c8f3a37e8",
   "metadata": {},
   "source": [
    "Obtaining the default bucket for our Sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45350138-5e57-4dd8-8cd1-579f373422e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: sagemaker-us-east-1-594409465357\n"
     ]
    }
   ],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Bucket Name: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef0418-9b07-4202-847e-fe801b6896b9",
   "metadata": {},
   "source": [
    "Set the prefix for where you want to store your data / model files in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adda58e6-764f-4cde-95ad-6dab605c3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Sagemaker/ssoc-autocoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2fa33-bf22-4a92-a49e-7827e8f3637d",
   "metadata": {},
   "source": [
    "Run the code below to retrieve Sagemaker's execution role. Note that the role we have set up is called `mom-aws`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984e14c6-b545-4ffd-8700-477eb89d400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name shaunkhoo to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Role: arn:aws:iam::594409465357:role/mom-aws\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName = 'mom-aws')['Role']['Arn']\n",
    "print(f\"Execution Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c38ca9-0e94-4466-ad0b-f1d55b945dff",
   "metadata": {},
   "source": [
    "Upload the raw data to the S3 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e958a02-e261-42eb-840e-c92e5c4f1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path = \"Data/Train/pre-training-sample1000.txt\", \n",
    "                                       bucket = bucket, \n",
    "                                       key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb93f0e8-ed80-4cc3-ad40-df38bac2d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs stored in: s3://sagemaker-us-east-1-594409465357/Sagemaker/ssoc-autocoder/pre-training-sample1000.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs stored in: {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863e84d-a704-4978-b867-742eec062c07",
   "metadata": {},
   "source": [
    "#### C) Language modelling (or pretraining) on Sagemaker\n",
    "\n",
    "Running masked language modelling to finetune the DistilBERT model on MCF data to improve downstream classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ff523-1d8d-49ac-bb8f-ec68839fe52d",
   "metadata": {},
   "source": [
    "Define the hyperparameters that need to be passed onto the masked language modelling script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b222b53e-2954-4994-8803-8c9a2bac99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_parameters = {\n",
    "    'model_name_or_path': 'mcf-pretrained-5epoch',#'distilbert-base-uncased',\n",
    "    'train_file': \"pre-training-full.txt\",\n",
    "    'line_by_line': True,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'logging_steps': 500,\n",
    "    'save_strategy': 'epoch',\n",
    "    'overwrite_output_dir': True,\n",
    "    'output_dir': '20211228_test'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2f8c6-ccc9-4c35-8254-12ee6c009c1c",
   "metadata": {},
   "source": [
    "Create the estimator object and run it on the full pretraining text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238df6a3-4451-4a6b-9565-cb6b8b9c2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_estimator = PyTorch(\n",
    "    entry_point = \"run_mlm_aws.py\",\n",
    "    role = role,\n",
    "    framework_version = \"1.8.1\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.g4dn.xlarge\",\n",
    "    source_dir = \"ssoc_autocoder\",\n",
    "    max_run = 432000,\n",
    "    py_version = \"py3\",\n",
    "    env = {'SAGEMAKER_REQUIREMENTS': 'C:\\\\Users\\\\shaun\\\\PycharmProjects\\\\ssoc-autocoder\\\\ssoc_autocoder\\\\requirements.txt'},\n",
    "    hyperparameters = mlm_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab35b996-0f7d-4868-9658-080241d56f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-08 00:50:45 Starting - Starting the training job...\n",
      "2022-01-08 00:50:47 Starting - Launching requested ML instancesProfilerReport-1641603042: InProgress\n",
      "...\n",
      "2022-01-08 00:51:48 Starting - Preparing the instances for training......\n",
      "2022-01-08 00:53:00 Downloading - Downloading input data......\n",
      "2022-01-08 00:54:00 Training - Downloading the training image..................\n",
      "2022-01-08 00:57:42 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-08 00:57:35,440 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-08 00:57:35,461 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-08 00:57:35,467 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-08 00:57:35,826 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==1.16.1\n",
      "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.61.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.16.1->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets==1.16.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.16.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.16.1->-r requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-1.16.1 huggingface-hub-0.2.1 regex-2021.11.10 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.10.0 xxhash-2.0.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-01-08 00:57:42,807 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"do_train\": true,\n",
      "        \"overwrite_output_dir\": true,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"do_eval\": true,\n",
      "        \"train_file\": \"pre-training-full.txt\",\n",
      "        \"line_by_line\": true,\n",
      "        \"output_dir\": \"20211228_test\",\n",
      "        \"logging_steps\": 500,\n",
      "        \"model_name_or_path\": \"mcf-pretrained-5epoch\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-01-08-00-50-39-417\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2022-01-08-00-50-39-417/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_mlm_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_mlm_aws.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"line_by_line\":true,\"logging_steps\":500,\"model_name_or_path\":\"mcf-pretrained-5epoch\",\"output_dir\":\"20211228_test\",\"overwrite_output_dir\":true,\"save_strategy\":\"epoch\",\"train_file\":\"pre-training-full.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_mlm_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_mlm_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2022-01-08-00-50-39-417/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"line_by_line\":true,\"logging_steps\":500,\"model_name_or_path\":\"mcf-pretrained-5epoch\",\"output_dir\":\"20211228_test\",\"overwrite_output_dir\":true,\"save_strategy\":\"epoch\",\"train_file\":\"pre-training-full.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-01-08-00-50-39-417\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2022-01-08-00-50-39-417/source/sourcedir.tar.gz\",\"module_name\":\"run_mlm_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_mlm_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--evaluation_strategy\",\"epoch\",\"--line_by_line\",\"True\",\"--logging_steps\",\"500\",\"--model_name_or_path\",\"mcf-pretrained-5epoch\",\"--output_dir\",\"20211228_test\",\"--overwrite_output_dir\",\"True\",\"--save_strategy\",\"epoch\",\"--train_file\",\"pre-training-full.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_OVERWRITE_OUTPUT_DIR=true\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=pre-training-full.txt\u001b[0m\n",
      "\u001b[34mSM_HP_LINE_BY_LINE=true\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=20211228_test\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=mcf-pretrained-5epoch\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_mlm_aws.py --do_eval True --do_train True --evaluation_strategy epoch --line_by_line True --logging_steps 500 --model_name_or_path mcf-pretrained-5epoch --output_dir 20211228_test --overwrite_output_dir True --save_strategy epoch --train_file pre-training-full.txt\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=20211228_test/runs/Jan08_00-57-46_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=2,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/20211228_test,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=32,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=16,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=20211228_test,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=20211228_test,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:46 - WARNING - datasets.builder - Using custom data configuration default-9ae880389d702d26\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:46 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:46 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:50 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:50 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:50 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - WARNING - datasets.builder - Using custom data configuration default-9ae880389d702d26\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.builder - Overwrite dataset info from restored data version.\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - WARNING - datasets.builder - Using custom data configuration default-9ae880389d702d26\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.builder - Overwrite dataset info from restored data version.\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:559] 2022-01-08 00:57:56,542 >> loading configuration file /opt/ml/input/data/training/mcf-pretrained-5epoch/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-08 00:57:56,542 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/training/mcf-pretrained-3epoch\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_auto.py:332] 2022-01-08 00:57:56,542 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:559] 2022-01-08 00:57:56,543 >> loading configuration file /opt/ml/input/data/training/mcf-pretrained-5epoch/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-08 00:57:56,543 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/training/mcf-pretrained-3epoch\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1669] 2022-01-08 00:57:56,544 >> Didn't find file /opt/ml/input/data/training/mcf-pretrained-5epoch/tokenizer.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1669] 2022-01-08 00:57:56,544 >> Didn't find file /opt/ml/input/data/training/mcf-pretrained-5epoch/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1669] 2022-01-08 00:57:56,544 >> Didn't find file /opt/ml/input/data/training/mcf-pretrained-5epoch/special_tokens_map.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1669] 2022-01-08 00:57:56,544 >> Didn't find file /opt/ml/input/data/training/mcf-pretrained-5epoch/tokenizer_config.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1737] 2022-01-08 00:57:56,544 >> loading file /opt/ml/input/data/training/mcf-pretrained-5epoch/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1737] 2022-01-08 00:57:56,544 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1737] 2022-01-08 00:57:56,544 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1737] 2022-01-08 00:57:56,544 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1737] 2022-01-08 00:57:56,545 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:559] 2022-01-08 00:57:56,545 >> loading configuration file /opt/ml/input/data/training/mcf-pretrained-5epoch/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-08 00:57:56,545 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/training/mcf-pretrained-3epoch\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:559] 2022-01-08 00:57:56,571 >> loading configuration file /opt/ml/input/data/training/mcf-pretrained-5epoch/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-08 00:57:56,571 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/training/mcf-pretrained-3epoch\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1277] 2022-01-08 00:57:56,594 >> loading weights file /opt/ml/input/data/training/mcf-pretrained-5epoch/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1524] 2022-01-08 00:57:57,767 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1533] 2022-01-08 00:57:57,767 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at /opt/ml/input/data/training/mcf-pretrained-5epoch.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:57 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 512 instead. You can change that default value by passing --max_seq_length xxx.\u001b[0m\n",
      "\u001b[34m{'text': \"In 2009 MoneySmart was founded with the simple aim to help Singaporeans make better financial decisions. Today, we're Singapore's #1 personal finance portal, with fast expansion into the rest of the region. To continue charting new territory, our amazing team of product experience designers and researchers is seeking a like-minded maker to join our crew. Together, we're creating products and services that are useful, usable and desirable for end-users while constantly innovating to impact the category from the ground, up. Your mission as an Associate Product Experience Researcher at MoneySmart is to design best-in-class user experiences to aid consumers in reaching their personal finance goals. The role requires you to work closely with the MoneySmart product, design, technology, marketing and commercial teams to develop digital products and services for the personal finance sector. In a Nutshell: You will be working in a team of product experience designers, product managers, and engineers to actively define the product strategy. You will be hyper-focused on the end user, able to see your insights' impact on their overall experience. You are expected to suggest methodology for eliciting feedback on concepts to finished products, and synthesise results in a fast-paced environment.\"}\u001b[0m\n",
      "\u001b[34m{'text': 'The School of Mechanical and Aerospace Engineering (MAE) boasts comprehensive state-of-the-art facilities and a faculty comprising more than 130 professors, making it one of the largest mechanical engineering schools in the world. Faculty members are drawn from renowned universities worldwide, providing a wealth of collective expertise in traditional mechanical and aerospace engineering, and in specialities including manufacturing, mechatronics, innovative design, nanotechnology, and biomedical and computational applications. A Research Associate (RA) positions are available for a research project working on the investigating and understanding the new paradigm for high impact research work. Singapore has made great strides in establishing R&D as a major pillar in the transformation of its economy. Understanding the new paradigm of high quality research work in Singapore is of paramount importance to the progress and sustainability of research in Singapore. Key Responsibilities: Conduct literature searches and write literature reviews. Collect data on publications, collaborations and their impact. Identify factors that contribute to the conducting of high quality research work resulting in high impact collaborations and publications. Assist in writing research reports.'}\u001b[0m\n",
      "\u001b[34m01/08/2022 00:57:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-8fd97e9bc9e376f3.arrow\u001b[0m\n",
      "\u001b[34m01/08/2022 01:03:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9ae880389d702d26/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-b9167b4b147a3702.arrow\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:521] 2022-01-08 01:04:15,108 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2022-01-08 01:04:15,308 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2022-01-08 01:04:15,308 >>   Num examples = 838103\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2022-01-08 01:04:15,308 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2022-01-08 01:04:15,308 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2022-01-08 01:04:15,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2022-01-08 01:04:15,308 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2022-01-08 01:04:15,308 >>   Total optimization steps = 104764\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.517 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.616 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.617 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.617 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.617 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:15.618 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.327 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.328 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.329 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.330 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.331 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:vocab_transform.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.332 algo-1:32 INFO hook.py:591] name:vocab_transform.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:591] name:vocab_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:591] name:vocab_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:591] name:vocab_projector.bias count_params:30522\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:593] Total Trainable Params: 66985530\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-01-08 01:04:16.333 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m{'loss': 1.0659, 'learning_rate': 4.9761368408995456e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0727, 'learning_rate': 4.9522736817990916e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0749, 'learning_rate': 4.928410522698637e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0813, 'learning_rate': 4.904547363598182e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m{'loss': 1.075, 'learning_rate': 4.880684204497728e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0752, 'learning_rate': 4.856821045397274e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0844, 'learning_rate': 4.83295788629682e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0851, 'learning_rate': 4.809094727196366e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0751, 'learning_rate': 4.785231568095911e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0924, 'learning_rate': 4.761368408995457e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0902, 'learning_rate': 4.7375052498950025e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0923, 'learning_rate': 4.713642090794548e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0999, 'learning_rate': 4.689778931694094e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0788, 'learning_rate': 4.665915772593639e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0892, 'learning_rate': 4.6420526134931845e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0983, 'learning_rate': 4.6181894543927306e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m{'loss': 1.093, 'learning_rate': 4.594326295292276e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1025, 'learning_rate': 4.570463136191822e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1124, 'learning_rate': 4.546599977091367e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1014, 'learning_rate': 4.522736817990913e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0889, 'learning_rate': 4.4988736588904587e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1062, 'learning_rate': 4.475010499790005e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1, 'learning_rate': 4.45114734068955e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1128, 'learning_rate': 4.427284181589096e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0988, 'learning_rate': 4.4034210224886414e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1097, 'learning_rate': 4.379557863388187e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1134, 'learning_rate': 4.355694704287733e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1113, 'learning_rate': 4.331831545187278e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1116, 'learning_rate': 4.307968386086824e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1066, 'learning_rate': 4.2841052269863695e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1228, 'learning_rate': 4.260242067885915e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m{'loss': 1.101, 'learning_rate': 4.236378908785461e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1142, 'learning_rate': 4.212515749685006e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1188, 'learning_rate': 4.188652590584552e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m{'loss': 1.111, 'learning_rate': 4.164789431484098e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1225, 'learning_rate': 4.1409262723836436e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1044, 'learning_rate': 4.117063113283189e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1125, 'learning_rate': 4.093199954182735e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1217, 'learning_rate': 4.0693367950822803e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1128, 'learning_rate': 4.0454736359818264e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1214, 'learning_rate': 4.021610476881372e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1337, 'learning_rate': 3.997747317780917e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1276, 'learning_rate': 3.973884158680463e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1242, 'learning_rate': 3.9500209995800084e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1173, 'learning_rate': 3.926157840479554e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1299, 'learning_rate': 3.9022946813791e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1145, 'learning_rate': 3.878431522278645e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1244, 'learning_rate': 3.854568363178191e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1405, 'learning_rate': 3.830705204077737e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1422, 'learning_rate': 3.8068420449772826e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m{'loss': 1.132, 'learning_rate': 3.7829788858768286e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m{'loss': 1.131, 'learning_rate': 3.759115726776374e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1266, 'learning_rate': 3.735252567675919e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1292, 'learning_rate': 3.711389408575465e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1394, 'learning_rate': 3.687526249475011e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1391, 'learning_rate': 3.663663090374556e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1465, 'learning_rate': 3.639799931274102e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1465, 'learning_rate': 3.6159367721736474e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1537, 'learning_rate': 3.592073613073193e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1448, 'learning_rate': 3.568210453972739e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1471, 'learning_rate': 3.544347294872285e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1444, 'learning_rate': 3.52048413577183e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1644, 'learning_rate': 3.496620976671376e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1647, 'learning_rate': 3.4727578175709215e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1578, 'learning_rate': 3.4488946584704675e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1628, 'learning_rate': 3.425031499370013e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m{'loss': 1.168, 'learning_rate': 3.401168340269558e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1526, 'learning_rate': 3.377305181169104e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1625, 'learning_rate': 3.3534420220686496e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1516, 'learning_rate': 3.329578862968195e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1598, 'learning_rate': 3.305715703867741e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1514, 'learning_rate': 3.281852544767286e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1663, 'learning_rate': 3.2579893856668324e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1739, 'learning_rate': 3.234126226566378e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1597, 'learning_rate': 3.210263067465924e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1695, 'learning_rate': 3.18639990836547e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1777, 'learning_rate': 3.162536749265015e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1783, 'learning_rate': 3.1386735901645605e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1814, 'learning_rate': 3.1148104310641065e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m{'loss': 1.173, 'learning_rate': 3.090947271963652e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1863, 'learning_rate': 3.067084112863197e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1807, 'learning_rate': 3.0432209537627432e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1766, 'learning_rate': 3.0193577946622885e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1817, 'learning_rate': 2.9954946355618346e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1852, 'learning_rate': 2.97163147646138e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1814, 'learning_rate': 2.9477683173609256e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m{'loss': 1.187, 'learning_rate': 2.9239051582604716e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1934, 'learning_rate': 2.900041999160017e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1905, 'learning_rate': 2.8761788400595623e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2003, 'learning_rate': 2.8523156809591084e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1963, 'learning_rate': 2.8284525218586537e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1972, 'learning_rate': 2.8045893627581994e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1906, 'learning_rate': 2.7807262036577454e-05, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1997, 'learning_rate': 2.7568630445572908e-05, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2067, 'learning_rate': 2.7329998854568368e-05, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2012, 'learning_rate': 2.709136726356382e-05, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2061, 'learning_rate': 2.6852735672559275e-05, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1922, 'learning_rate': 2.6614104081554735e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2062, 'learning_rate': 2.6375472490550192e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2079, 'learning_rate': 2.6136840899545646e-05, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2204, 'learning_rate': 2.5898209308541106e-05, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2121, 'learning_rate': 2.565957771753656e-05, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2271, 'learning_rate': 2.5420946126532013e-05, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2033, 'learning_rate': 2.5182314535527473e-05, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:521] 2022-01-08 17:48:06,226 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2181] 2022-01-08 17:48:06,237 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2183] 2022-01-08 17:48:06,238 >>   Num examples = 44111\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2186] 2022-01-08 17:48:06,238 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.4939054250717163, 'eval_runtime': 1101.0895, 'eval_samples_per_second': 40.061, 'eval_steps_per_second': 1.252, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2022-01-08 18:06:27,328 >> Saving model checkpoint to /opt/ml/model/20211228_test/checkpoint-52382\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2022-01-08 18:06:27,328 >> Configuration saved in /opt/ml/model/20211228_test/checkpoint-52382/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2022-01-08 18:06:27,776 >> Model weights saved in /opt/ml/model/20211228_test/checkpoint-52382/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2022-01-08 18:06:27,777 >> tokenizer config file saved in /opt/ml/model/20211228_test/checkpoint-52382/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2022-01-08 18:06:27,777 >> Special tokens file saved in /opt/ml/model/20211228_test/checkpoint-52382/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 1.2223, 'learning_rate': 2.4943682944522926e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2035, 'learning_rate': 2.4705051353518387e-05, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2069, 'learning_rate': 2.4466419762513844e-05, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2159, 'learning_rate': 2.4227788171509297e-05, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2233, 'learning_rate': 2.3989156580504754e-05, 'epoch': 1.04}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2072, 'learning_rate': 2.375052498950021e-05, 'epoch': 1.05}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2252, 'learning_rate': 2.3511893398495668e-05, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2021, 'learning_rate': 2.3273261807491125e-05, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m{'loss': 1.218, 'learning_rate': 2.303463021648658e-05, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2194, 'learning_rate': 2.279599862548204e-05, 'epoch': 1.09}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2258, 'learning_rate': 2.2557367034477495e-05, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2118, 'learning_rate': 2.231873544347295e-05, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2059, 'learning_rate': 2.2080103852468406e-05, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2145, 'learning_rate': 2.1841472261463862e-05, 'epoch': 1.13}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2109, 'learning_rate': 2.160284067045932e-05, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2188, 'learning_rate': 2.1364209079454776e-05, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m{'loss': 1.235, 'learning_rate': 2.1125577488450233e-05, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2257, 'learning_rate': 2.088694589744569e-05, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2116, 'learning_rate': 2.0648314306441143e-05, 'epoch': 1.17}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2121, 'learning_rate': 2.04096827154366e-05, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2329, 'learning_rate': 2.0171051124432057e-05, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2271, 'learning_rate': 1.9932419533427514e-05, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2322, 'learning_rate': 1.969378794242297e-05, 'epoch': 1.21}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2216, 'learning_rate': 1.9455156351418428e-05, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2266, 'learning_rate': 1.9216524760413885e-05, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2335, 'learning_rate': 1.8977893169409338e-05, 'epoch': 1.24}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2139, 'learning_rate': 1.8739261578404795e-05, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m{'loss': 1.226, 'learning_rate': 1.8500629987400252e-05, 'epoch': 1.26}\u001b[0m\n",
      "\u001b[34m{'loss': 1.225, 'learning_rate': 1.826199839639571e-05, 'epoch': 1.27}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2216, 'learning_rate': 1.8023366805391166e-05, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2224, 'learning_rate': 1.7784735214386622e-05, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2375, 'learning_rate': 1.754610362338208e-05, 'epoch': 1.3}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2405, 'learning_rate': 1.7307472032377536e-05, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2251, 'learning_rate': 1.706884044137299e-05, 'epoch': 1.32}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2354, 'learning_rate': 1.6830208850368447e-05, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2453, 'learning_rate': 1.6591577259363907e-05, 'epoch': 1.34}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2425, 'learning_rate': 1.635294566835936e-05, 'epoch': 1.35}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2375, 'learning_rate': 1.6114314077354817e-05, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2349, 'learning_rate': 1.5875682486350274e-05, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2335, 'learning_rate': 1.563705089534573e-05, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2513, 'learning_rate': 1.5398419304341184e-05, 'epoch': 1.38}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2365, 'learning_rate': 1.5159787713336643e-05, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2389, 'learning_rate': 1.49211561223321e-05, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2379, 'learning_rate': 1.4682524531327557e-05, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2352, 'learning_rate': 1.4443892940323012e-05, 'epoch': 1.42}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2388, 'learning_rate': 1.4205261349318469e-05, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2538, 'learning_rate': 1.3966629758313926e-05, 'epoch': 1.44}\u001b[0m\n",
      "\u001b[34m{'loss': 1.253, 'learning_rate': 1.372799816730938e-05, 'epoch': 1.45}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2598, 'learning_rate': 1.3489366576304838e-05, 'epoch': 1.46}\u001b[0m\n",
      "\u001b[34m{'loss': 1.244, 'learning_rate': 1.3250734985300295e-05, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2489, 'learning_rate': 1.3012103394295751e-05, 'epoch': 1.48}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2408, 'learning_rate': 1.2773471803291207e-05, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2453, 'learning_rate': 1.2534840212286663e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2289, 'learning_rate': 1.229620862128212e-05, 'epoch': 1.51}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2398, 'learning_rate': 1.2057577030277577e-05, 'epoch': 1.52}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2609, 'learning_rate': 1.1818945439273034e-05, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m{'loss': 1.244, 'learning_rate': 1.158031384826849e-05, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2503, 'learning_rate': 1.1341682257263946e-05, 'epoch': 1.55}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2619, 'learning_rate': 1.1103050666259403e-05, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2681, 'learning_rate': 1.0864419075254858e-05, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2559, 'learning_rate': 1.0625787484250315e-05, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2587, 'learning_rate': 1.0387155893245772e-05, 'epoch': 1.58}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2663, 'learning_rate': 1.0148524302241229e-05, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2495, 'learning_rate': 9.909892711236684e-06, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2487, 'learning_rate': 9.671261120232143e-06, 'epoch': 1.61}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2586, 'learning_rate': 9.432629529227598e-06, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[34m{'loss': 1.252, 'learning_rate': 9.193997938223055e-06, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m{'loss': 1.255, 'learning_rate': 8.955366347218511e-06, 'epoch': 1.64}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2604, 'learning_rate': 8.716734756213967e-06, 'epoch': 1.65}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2624, 'learning_rate': 8.478103165209424e-06, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2574, 'learning_rate': 8.239471574204879e-06, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2688, 'learning_rate': 8.000839983200337e-06, 'epoch': 1.68}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2586, 'learning_rate': 7.762208392195792e-06, 'epoch': 1.69}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2691, 'learning_rate': 7.523576801191249e-06, 'epoch': 1.7}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2772, 'learning_rate': 7.284945210186705e-06, 'epoch': 1.71}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2699, 'learning_rate': 7.046313619182163e-06, 'epoch': 1.72}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2613, 'learning_rate': 6.807682028177618e-06, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2701, 'learning_rate': 6.569050437173076e-06, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2758, 'learning_rate': 6.330418846168531e-06, 'epoch': 1.75}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2683, 'learning_rate': 6.091787255163988e-06, 'epoch': 1.76}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2735, 'learning_rate': 5.853155664159445e-06, 'epoch': 1.77}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2869, 'learning_rate': 5.614524073154901e-06, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2648, 'learning_rate': 5.375892482150358e-06, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2717, 'learning_rate': 5.137260891145814e-06, 'epoch': 1.79}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2733, 'learning_rate': 4.89862930014127e-06, 'epoch': 1.8}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2815, 'learning_rate': 4.659997709136727e-06, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2768, 'learning_rate': 4.421366118132183e-06, 'epoch': 1.82}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2969, 'learning_rate': 4.18273452712764e-06, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2777, 'learning_rate': 3.944102936123096e-06, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2746, 'learning_rate': 3.7054713451185525e-06, 'epoch': 1.85}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2861, 'learning_rate': 3.466839754114009e-06, 'epoch': 1.86}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2863, 'learning_rate': 3.2282081631094654e-06, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2919, 'learning_rate': 2.9895765721049214e-06, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2875, 'learning_rate': 2.7509449811003783e-06, 'epoch': 1.89}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2796, 'learning_rate': 2.5123133900958347e-06, 'epoch': 1.9}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2855, 'learning_rate': 2.273681799091291e-06, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2835, 'learning_rate': 2.035050208086747e-06, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m{'loss': 1.289, 'learning_rate': 1.7964186170822039e-06, 'epoch': 1.93}\u001b[0m\n",
      "\u001b[34m{'loss': 1.286, 'learning_rate': 1.5577870260776603e-06, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2862, 'learning_rate': 1.3191554350731168e-06, 'epoch': 1.95}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3027, 'learning_rate': 1.0805238440685732e-06, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2932, 'learning_rate': 8.418922530640297e-07, 'epoch': 1.97}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3016, 'learning_rate': 6.032606620594861e-07, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3026, 'learning_rate': 3.6462907105494255e-07, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m{'loss': 1.293, 'learning_rate': 1.25997480050399e-07, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:521] 2022-01-09 11:02:56,018 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2181] 2022-01-09 11:02:56,027 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2183] 2022-01-09 11:02:56,027 >>   Num examples = 44111\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2186] 2022-01-09 11:02:56,027 >>   Batch size = 32\u001b[0m\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidSignatureException) when calling the DescribeLogStreams operation: Signature expired: 20220109T110957Z is now earlier than 20220109T124605Z (20220109T125105Z - 5 min.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19912/2308911798.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlm_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m's3://sagemaker-us-east-1-594409465357/Sagemaker/ssoc-autocoder'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"None\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1653\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1654\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[0;32m   3662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3663\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3664\u001b[1;33m             _flush_log_streams(\n\u001b[0m\u001b[0;32m   3665\u001b[0m                 \u001b[0mstream_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3666\u001b[0m                 \u001b[0minstance_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36m_flush_log_streams\u001b[1;34m(stream_names, instance_count, client, log_group, job_name, positions, dot, color_wrap)\u001b[0m\n\u001b[0;32m   4707\u001b[0m         \u001b[1;31m# may be dynamic until we have a stream for every instance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4708\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4709\u001b[1;33m             streams = client.describe_log_streams(\n\u001b[0m\u001b[0;32m   4710\u001b[0m                 \u001b[0mlogGroupName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4711\u001b[0m                 \u001b[0mlogStreamNamePrefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (InvalidSignatureException) when calling the DescribeLogStreams operation: Signature expired: 20220109T110957Z is now earlier than 20220109T124605Z (20220109T125105Z - 5 min.)"
     ]
    }
   ],
   "source": [
    "mlm_estimator.fit({\"training\": 's3://sagemaker-us-east-1-594409465357/Sagemaker/ssoc-autocoder'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f2c57-e372-4a87-95bf-2ec944cbac9e",
   "metadata": {},
   "source": [
    "#### D) Finetuning classification on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "395b4560-1646-4707-9864-4e4a8dd5f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"train_aws.py\",\n",
    "    role = role,\n",
    "    framework_version = \"1.8.1\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.g4dn.xlarge\",\n",
    "    source_dir = \"ssoc_autocoder\",\n",
    "    py_version = \"py3\",\n",
    "    env = env,\n",
    "    #use_spot_instances = True,\n",
    "    #max_run = ,\n",
    "    #max_wait = 600,\n",
    "    hyperparameters = {\"epochs\": 4, \"tied\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a5c2bf6-6b9b-4370-a4b4-340681f1bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 07:40:29 Starting - Starting the training job...\n",
      "2021-10-11 07:40:55 Starting - Launching requested ML instancesProfilerReport-1633938053: InProgress\n",
      "......\n",
      "2021-10-11 07:42:04 Starting - Preparing the instances for training.........\n",
      "2021-10-11 07:43:40 Downloading - Downloading input data...\n",
      "2021-10-11 07:44:16 Training - Downloading the training image......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,655 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,679 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:29,689 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:30,459 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 07:48:38,001 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-07-40-51-031\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-07-40-51-031\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-07-40-51-031/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mLoading pretrained model...\u001b[0m\n",
      "\u001b[34mModel loaded successfully!\u001b[0m\n",
      "\u001b[34mTraining started on: 11 Oct 2021 - 07:49:00\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 1 started on: 11 Oct 2021 - 07:49:00\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.742 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.940 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.940 algo-1:31 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.941 algo-1:31 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.942 algo-1:31 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:00.942 algo-1:31 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.0.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.0.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.516 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.3.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.3.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.6.weight count_params:98304\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.6.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.9.weight count_params:1152\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_1d_stack.9.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.0.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.0.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.3.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.3.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.6.weight count_params:198912\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.517 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.6.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.9.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.9.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.12.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_2d_stack.12.bias count_params:42\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.0.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.0.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.3.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.3.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.518 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.6.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.6.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.9.weight count_params:419328\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.9.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.519 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.12.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.12.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.15.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_3d_stack.15.bias count_params:144\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.0.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.0.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.3.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.3.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.520 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.6.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.6.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.9.weight count_params:739584\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.9.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.12.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.12.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.15.weight count_params:211456\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_4d_stack.15.bias count_params:413\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.0.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.0.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.3.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.521 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.3.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.6.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.6.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.9.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.9.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.12.weight count_params:1371872\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:591] name:ssoc_5d_stack.12.bias count_params:997\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:593] Total Trainable Params: 18413009\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.522 algo-1:31 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-11 07:49:01.527 algo-1:31 INFO hook.py:488] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 07:49:37 Training - Training image download completed. Training in progress.\u001b[34m>> Training Loss per 200 steps: 52.4779 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.22%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.41 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 52.1093 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.52%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.41 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 52.1961 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.54%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 51.8572 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.72%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 51.7501 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.76%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 1 Loss = #011Training: 51.699  #011Validation: 49.237\u001b[0m\n",
      "\u001b[34m> Epoch 1 Accuracy = #011Training: 13.87%  #011Validation: 13.88%\u001b[0m\n",
      "\u001b[34m> Epoch 1 took 22.73 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 2 started on: 11 Oct 2021 - 08:11:44\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.8340 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.67%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.46 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.7493 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.82%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5619 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.88%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.3199 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.64%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.1403 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.63%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 2 Loss = #011Training: 50.198  #011Validation: 46.922\u001b[0m\n",
      "\u001b[34m> Epoch 2 Accuracy = #011Training: 14.68%  #011Validation: 15.65%\u001b[0m\n",
      "\u001b[34m> Epoch 2 took 22.83 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 3 started on: 11 Oct 2021 - 08:34:33\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.1103 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.12%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.46 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.5551 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.70%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3934 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.77%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3123 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.93%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.45 mins\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30468/4121110664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"None\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1653\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1654\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shaun\\pycharmprojects\\ssoc-autocoder\\venv\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[0;32m   3675\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3677\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3679\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc28f8c-2dbf-48d8-b3ae-e710536bc2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 08:58:49 Starting - Starting the training job...\n",
      "2021-10-11 08:59:13 Starting - Launching requested ML instancesProfilerReport-1633942752: InProgress\n",
      "...\n",
      "2021-10-11 08:59:53 Starting - Preparing the instances for training.........\n",
      "2021-10-11 09:01:37 Downloading - Downloading input data\n",
      "2021-10-11 09:01:37 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:49,253 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:49,273 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:50,703 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:51,289 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.10.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.51.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.10.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.10.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.10.0->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.10.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.10.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 09:05:57,087 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-08-59-10-592\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-08-59-10-592\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-08-59-10-592/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.46)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mLoading pretrained model...\u001b[0m\n",
      "\u001b[34mModel loaded successfully!\u001b[0m\n",
      "\u001b[34mTraining started on: 11 Oct 2021 - 09:06:14\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 1 started on: 11 Oct 2021 - 09:06:14\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.252 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.438 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.439 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.439 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.440 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.440 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.938 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.0.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.0.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.3.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.3.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.6.weight count_params:98304\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.6.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.9.weight count_params:1152\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_1d_stack.9.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.0.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.0.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.3.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.3.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.6.weight count_params:198912\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.6.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.939 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.9.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.9.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.12.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_2d_stack.12.bias count_params:42\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.0.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.0.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.3.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.3.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.6.weight count_params:670761\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.6.bias count_params:819\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.9.weight count_params:419328\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.9.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.12.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.12.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.15.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_3d_stack.15.bias count_params:144\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.0.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.0.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.940 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.3.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.3.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.6.weight count_params:927369\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.6.bias count_params:963\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.9.weight count_params:739584\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.9.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.12.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.12.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.15.weight count_params:211456\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_4d_stack.15.bias count_params:413\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.0.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.0.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.3.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.3.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.6.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.6.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.9.weight count_params:1893376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.9.bias count_params:1376\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.12.weight count_params:1371872\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.941 algo-1:32 INFO hook.py:591] name:ssoc_5d_stack.12.bias count_params:997\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.942 algo-1:32 INFO hook.py:593] Total Trainable Params: 18413009\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.942 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-11 09:06:15.944 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 09:06:39 Training - Training image download completed. Training in progress.\u001b[34m>> Training Loss per 200 steps: 51.5119 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 13.88%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.78 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.9504 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.49%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.6563 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.76%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5482 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.81%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 50.5025 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 14.72%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.84 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 1 Loss = #011Training: 50.465  #011Validation: 48.061\u001b[0m\n",
      "\u001b[34m> Epoch 1 Accuracy = #011Training: 14.78%  #011Validation: 16.75%\u001b[0m\n",
      "\u001b[34m> Epoch 1 took 12.20 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 2 started on: 11 Oct 2021 - 09:18:26\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3656 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.27%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.85 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3617 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.02%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3198 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.08%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.3620 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.04%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 49.2366 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.17%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 2 Loss = #011Training: 49.235  #011Validation: 47.097\u001b[0m\n",
      "\u001b[34m> Epoch 2 Accuracy = #011Training: 16.18%  #011Validation: 17.04%\u001b[0m\n",
      "\u001b[34m> Epoch 2 took 12.34 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 3 started on: 11 Oct 2021 - 09:30:46\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6243 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.70%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6804 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.00%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.7788 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.58%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.6815 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.65%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.4011 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.73%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 3 Loss = #011Training: 48.296  #011Validation: 46.215\u001b[0m\n",
      "\u001b[34m> Epoch 3 Accuracy = #011Training: 16.69%  #011Validation: 17.29%\u001b[0m\n",
      "\u001b[34m> Epoch 3 took 12.35 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 4 started on: 11 Oct 2021 - 09:43:08\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.8530 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.02%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.9214 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 16.91%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7115 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.36%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7667 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.63%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.7688 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.57%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.86 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 4 Loss = #011Training: 47.671  #011Validation: 45.670\u001b[0m\n",
      "\u001b[34m> Epoch 4 Accuracy = #011Training: 17.64%  #011Validation: 18.36%\u001b[0m\n",
      "\u001b[34m> Epoch 4 took 12.36 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 5 started on: 11 Oct 2021 - 09:55:29\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 48.0637 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.91%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.87 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.6514 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.97%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.80 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.4380 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 18.07%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.67 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 47.4385 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 17.99%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 1.69 mins\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5fe04-b2e1-47a0-a15d-803b94ca102f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
