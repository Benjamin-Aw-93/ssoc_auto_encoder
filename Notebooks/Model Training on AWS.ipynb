{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd8e7ca-ffd2-4497-b610-ff2a3a94c113",
   "metadata": {},
   "source": [
    "## Model training on AWS Sagemaker\n",
    "\n",
    "Referencing [this notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5124a4d-1dec-469e-91fc-a1afca2a073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef99523-7d47-459b-8ee3-92aa3fd67359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45350138-5e57-4dd8-8cd1-579f373422e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: sagemaker-us-east-1-594409465357\n"
     ]
    }
   ],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Bucket Name: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984e14c6-b545-4ffd-8700-477eb89d400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name shaunkhoo to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Role: arn:aws:iam::594409465357:role/mom-aws\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName = 'mom-aws')['Role']['Arn']\n",
    "print(f\"Execution Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e958a02-e261-42eb-840e-c92e5c4f1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path = \"Data/Processed/Training/train-aws\", \n",
    "                                       bucket = bucket, \n",
    "                                       key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb93f0e8-ed80-4cc3-ad40-df38bac2d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs stored in: s3://sagemaker-us-east-1-594409465357/sagemaker/ssoc-autocoder\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs stored in: {inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e1c5ba5-aa5a-40f9-b481-841a0c61a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    'SAGEMAKER_REQUIREMENTS': '../Notebooks/requirements.txt', # path relative to `source_dir` below.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "395b4560-1646-4707-9864-4e4a8dd5f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"train_aws.py\",\n",
    "    role = role,\n",
    "    framework_version = \"1.8.1\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.p2.xlarge\",\n",
    "    source_dir = \"ssoc_autocoder\",\n",
    "    py_version = \"py3\",\n",
    "    env = env,\n",
    "    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "    #                            bptt, dropout, tied, seed, log_interval\n",
    "    hyperparameters = {\"epochs\": 1, \"tied\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a5c2bf6-6b9b-4370-a4b4-340681f1bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 03:28:39 Starting - Starting the training job...\n",
      "2021-10-11 03:29:02 Starting - Launching requested ML instancesProfilerReport-1633922942: InProgress\n",
      "......\n",
      "2021-10-11 03:30:03 Starting - Preparing the instances for training......\n",
      "2021-10-11 03:31:27 Downloading - Downloading input data...\n",
      "2021-10-11 03:32:03 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 03:36:16,415 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 03:36:16,440 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 03:36:16,452 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 03:36:16,960 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-03-29-00-285\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-03-29-00-285/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-03-29-00-285/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-03-29-00-285\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-03-29-00-285/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\u001b[0m\n",
      "\u001b[34mTraining started on: 11 Oct 2021 - 03:36:45\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34m> Epoch 1 started on: 11 Oct 2021 - 03:36:45\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.450 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.676 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.676 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.677 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.678 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:46.678 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.373 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.0.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.374 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.0.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.374 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.3.weight count_params:98304\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.374 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.3.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.6.weight count_params:1152\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_1d_stack.6.bias count_params:9\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.0.weight count_params:603729\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.0.bias count_params:777\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.3.weight count_params:198912\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.3.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.6.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.6.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.375 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.9.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.376 algo-1:26 INFO hook.py:591] name:ssoc_2d_stack.9.bias count_params:42\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.376 algo-1:26 INFO hook.py:593] Total Trainable Params: 1532173\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.376 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-11 03:36:47.381 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 03:37:25 Training - Training image download completed. Training in progress.\u001b[34m>> Training Loss per 200 steps: 42.2173 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 21.45%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.43 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 39.0838 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 26.39%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.37 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 37.2470 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 29.13%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.37 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 36.2228 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 31.14%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.37 mins\u001b[0m\n",
      "\u001b[34m>> Training Loss per 200 steps: 35.4946 \u001b[0m\n",
      "\u001b[34m>> Training Accuracy per 200 steps: 32.35%\u001b[0m\n",
      "\u001b[34m>> Batch of 200 took 3.37 mins\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m> Epoch 1 Loss = #011Training: 35.285  #011Validation: 31.234\u001b[0m\n",
      "\u001b[34m> Epoch 1 Accuracy = #011Training: 32.70%  #011Validation: 39.74%\u001b[0m\n",
      "\u001b[34m> Epoch 1 took 22.44 mins\u001b[0m\n",
      "\u001b[34m====================================================================\u001b[0m\n",
      "\u001b[34mTraining ended on: 11 Oct 2021 - 03:59:12\u001b[0m\n",
      "\u001b[34mTotal training time: 0.37 hours\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 24.0MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 26.7kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 455k/455k [00:00<00:00, 26.6MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 461kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 3.38M/256M [00:00<00:07, 35.4MB/s]#015Downloading:   3%|▎         | 7.50M/256M [00:00<00:06, 37.4MB/s]#015Downloading:   5%|▍         | 11.5M/256M [00:00<00:06, 38.8MB/s]#015Downloading:   6%|▌         | 15.5M/256M [00:00<00:06, 39.6MB/s]#015Downloading:   8%|▊         | 20.6M/256M [00:00<00:05, 42.8MB/s]#015Downloading:  10%|▉         | 25.2M/256M [00:00<00:05, 44.5MB/s]#015Downloading:  12%|█▏        | 29.7M/256M [00:00<00:05, 45.3MB/s]#015Downloading:  13%|█▎        | 33.8M/256M [00:00<00:05, 44.0MB/s]#015Downloading:  15%|█▍        | 37.9M/256M [00:00<00:06, 37.3MB/s]#015Downloading:  16%|█▌        | 41.5M/256M [00:01<00:07, 30.3MB/s]#015Downloading:  17%|█▋        | 44.7M/256M [00:01<00:07, 30.6MB/s]#015Downloading:  19%|█▊        | 47.8M/256M [00:01<00:07, 30.6MB/s]#015Downloading:  20%|█▉        | 50.8M/256M [00:01<00:07, 30.3MB/s]#015Downloading:  22%|██▏       | 55.5M/256M [00:01<00:06, 34.2MB/s]#015Downloading:  23%|██▎       | 59.7M/256M [00:01<00:05, 36.8MB/s]#015Downloading:  25%|██▍       | 63.5M/256M [00:01<00:06, 32.0MB/s]#015Downloading:  26%|██▌       | 66.8M/256M [00:02<00:08, 23.5MB/s]#015Downloading:  28%|██▊       | 70.4M/256M [00:02<00:07, 26.6MB/s]#015Downloading:  29%|██▊       | 73.4M/256M [00:02<00:07, 25.5MB/s]#015Downloading:  30%|███       | 76.7M/256M [00:02<00:06, 27.6MB/s]#015Downloading:  31%|███       | 79.6M/256M [00:02<00:06, 28.3MB/s]#015Downloading:  32%|███▏      | 82.5M/256M [00:02<00:06, 27.9MB/s]#015Downloading:  33%|███▎      | 85.3M/256M [00:02<00:06, 28.0MB/s]#015Downloading:  35%|███▌      | 90.2M/256M [00:02<00:05, 32.5MB/s]#015Downloading:  37%|███▋      | 95.4M/256M [00:02<00:04, 36.9MB/s]#015Downloading:  39%|███▉      | 99.7M/256M [00:03<00:04, 39.0MB/s]#015Downloading:  41%|████      | 104M/256M [00:03<00:04, 36.6MB/s] #015Downloading:  42%|████▏     | 107M/256M [00:03<00:04, 34.6MB/s]#015Downloading:  43%|████▎     | 111M/256M [00:03<00:04, 34.0MB/s]#015Downloading:  45%|████▌     | 115M/256M [00:03<00:04, 36.3MB/s]#015Downloading:  47%|████▋     | 119M/256M [00:03<00:03, 38.6MB/s]#015Downloading:  48%|████▊     | 123M/256M [00:03<00:03, 35.9MB/s]#015Downloading:  50%|█████     | 128M/256M [00:03<00:03, 39.8MB/s]#015Downloading:  52%|█████▏    | 133M/256M [00:03<00:03, 42.2MB/s]#015Downloading:  54%|█████▍    | 137M/256M [00:04<00:02, 43.1MB/s]#015Downloading:  55%|█████▌    | 142M/256M [00:04<00:02, 39.8MB/s]#015Downloading:  57%|█████▋    | 146M/256M [00:04<00:03, 33.4MB/s]#015Downloading:  58%|█████▊    | 149M/256M [00:04<00:03, 34.8MB/s]#015Downloading:  60%|█████▉    | 153M/256M [00:04<00:03, 31.6MB/s]#015Downloading:  61%|██████    | 156M/256M [00:04<00:03, 29.2MB/s]#015Downloading:  63%|██████▎   | 160M/256M [00:04<00:03, 32.5MB/s]#015Downloading:  64%|██████▍   | 165M/256M [00:04<00:02, 35.5MB/s]#015Downloading:  66%|██████▋   | 169M/256M [00:04<00:02, 39.0MB/s]#015Downloading:  68%|██████▊   | 175M/256M [00:05<00:01, 43.0MB/s]#015Downloading:  70%|███████   | 179M/256M [00:05<00:01, 41.5MB/s]#015Downloading:  72%|███████▏  | 184M/256M [00:05<00:01, 42.8MB/s]#015Downloading:  74%|███████▎  | 188M/256M [00:05<00:01, 44.1MB/s]#015Downloading:  75%|███████▌  | 193M/256M [00:05<00:01, 45.6MB/s]#015Downloading:  77%|███████▋  | 197M/256M [00:05<00:01, 37.2MB/s]#015Downloading:  79%|███████▊  | 201M/256M [00:05<00:01, 33.8MB/s]#015Downloading:  80%|████████  | 205M/256M [00:05<00:01, 33.5MB/s]#015Downloading:  81%|████████▏ | 208M/256M [00:06<00:01, 34.7MB/s]#015Downloading:  83%|████████▎ | 213M/256M [00:06<00:01, 38.2MB/s]#015Downloading:  85%|████████▌ | 217M/256M [00:06<00:00, 40.2MB/s]#015Downloading:  87%|████████▋ | 223M/256M [00:06<00:00, 43.7MB/s]#015Downloading:  89%|████████▉ | 227M/256M [00:06<00:00, 43.5MB/s]#015Downloading:  91%|█████████ | 231M/256M [00:06<00:00, 36.6MB/s]#015Downloading:  92%|█████████▏| 235M/256M [00:06<00:00, 30.9MB/s]#015Downloading:  94%|█████████▍| 240M/256M [00:06<00:00, 34.5MB/s]#015Downloading:  95%|█████████▌| 243M/256M [00:07<00:00, 34.1MB/s]#015Downloading:  97%|█████████▋| 247M/256M [00:07<00:00, 33.8MB/s]#015Downloading:  98%|█████████▊| 250M/256M [00:07<00:00, 32.2MB/s]#015Downloading:  99%|█████████▉| 254M/256M [00:07<00:00, 33.5MB/s]#015Downloading: 100%|██████████| 256M/256M [00:07<00:00, 36.1MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 03:59:13,642 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-11 03:59:31 Uploading - Uploading generated training model\n",
      "2021-10-11 04:00:11 Completed - Training job completed\n",
      "ProfilerReport-1633922942: IssuesFound\n",
      "Training seconds: 1715\n",
      "Billable seconds: 1715\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fc28f8c-2dbf-48d8-b3ae-e710536bc2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 02:32:39 Starting - Starting the training job...\n",
      "2021-10-11 02:33:02 Starting - Launching requested ML instancesProfilerReport-1633919582: InProgress\n",
      "......\n",
      "2021-10-11 02:34:13 Starting - Preparing the instances for training.........\n",
      "2021-10-11 02:36:03 Downloading - Downloading input data...\n",
      "2021-10-11 02:36:23 Training - Downloading the training image...........................\n",
      "2021-10-11 02:41:45 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-11 02:41:39,012 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 02:41:39,040 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-11 02:41:39,049 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-11 02:41:39,756 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-10-11-02-33-00-825\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-02-33-00-825/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_aws.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"tied\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_aws.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_aws\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-02-33-00-825/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-10-11-02-33-00-825\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-594409465357/pytorch-training-2021-10-11-02-33-00-825/source/sourcedir.tar.gz\",\"module_name\":\"train_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_aws.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_aws.py --epochs 1 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (4.6.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.51.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.3.0 huggingface-hub-0.0.19 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\u001b[0m\n",
      "\n",
      "2021-10-11 02:42:25 Uploading - Uploading generated training model\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 33.3MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 13.5kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 455k/455k [00:00<00:00, 38.3MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 344kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]#015Downloading:   1%|          | 1.75M/256M [00:00<00:15, 17.1MB/s]#015Downloading:   2%|▏         | 4.50M/256M [00:00<00:13, 19.4MB/s]#015Downloading:   3%|▎         | 7.00M/256M [00:00<00:12, 20.9MB/s]#015Downloading:   4%|▍         | 10.5M/256M [00:00<00:10, 23.9MB/s]#015Downloading:   5%|▌         | 13.5M/256M [00:00<00:09, 25.6MB/s]#015Downloading:   6%|▋         | 16.5M/256M [00:00<00:09, 27.0MB/s]#015Downloading:   7%|▋         | 19.0M/256M [00:00<00:10, 24.1MB/s]#015Downloading:   8%|▊         | 21.3M/256M [00:00<00:10, 24.0MB/s]#015Downloading:   9%|▉         | 23.8M/256M [00:00<00:10, 23.7MB/s]#015Downloading:  10%|█         | 26.0M/256M [00:01<00:10, 22.0MB/s]#015Downloading:  11%|█▏        | 29.3M/256M [00:01<00:09, 24.6MB/s]#015Downloading:  12%|█▏        | 31.7M/256M [00:01<00:10, 21.4MB/s]#015Downloading:  13%|█▎        | 34.0M/256M [00:01<00:10, 21.6MB/s]#015Downloading:  14%|█▍        | 37.0M/256M [00:01<00:09, 23.2MB/s]#015Downloading:  15%|█▌        | 39.5M/256M [00:01<00:09, 24.0MB/s]#015Downloading:  17%|█▋        | 42.2M/256M [00:01<00:08, 25.3MB/s]#015Downloading:  18%|█▊        | 46.0M/256M [00:01<00:07, 27.8MB/s]#015Downloading:  19%|█▉        | 49.0M/256M [00:01<00:07, 28.5MB/s]#015Downloading:  20%|██        | 51.8M/256M [00:02<00:08, 25.9MB/s]#015Downloading:  21%|██▏       | 54.4M/256M [00:02<00:09, 23.4MB/s]#015Downloading:  22%|██▏       | 57.0M/256M [00:02<00:08, 24.0MB/s]#015Downloading:  23%|██▎       | 59.6M/256M [00:02<00:08, 25.0MB/s]#015Downloading:  24%|██▍       | 62.5M/256M [00:02<00:07, 26.2MB/s]#015Downloading:  25%|██▌       | 65.1M/256M [00:02<00:08, 23.1MB/s]#015Downloading:  26%|██▋       | 67.4M/256M [00:02<00:08, 23.2MB/s]#015Downloading:  27%|██▋       | 70.0M/256M [00:02<00:08, 24.0MB/s]#015Downloading:  29%|██▉       | 73.8M/256M [00:03<00:06, 27.3MB/s]#015Downloading:  30%|██▉       | 76.6M/256M [00:03<00:08, 21.3MB/s]#015Downloading:  31%|███▏      | 80.2M/256M [00:03<00:07, 24.6MB/s]#015Downloading:  33%|███▎      | 84.2M/256M [00:03<00:06, 28.0MB/s]#015Downloading:  34%|███▍      | 87.3M/256M [00:03<00:06, 27.8MB/s]#015Downloading:  35%|███▌      | 90.2M/256M [00:03<00:06, 25.9MB/s]#015Downloading:  36%|███▋      | 92.9M/256M [00:03<00:06, 25.7MB/s]#015Downloading:  38%|███▊      | 96.5M/256M [00:03<00:05, 28.3MB/s]#015Downloading:  39%|███▉      | 99.4M/256M [00:04<00:06, 27.2MB/s]#015Downloading:  40%|███▉      | 102M/256M [00:04<00:06, 23.9MB/s] #015Downloading:  42%|████▏     | 106M/256M [00:04<00:05, 27.5MB/s]#015Downloading:  43%|████▎     | 111M/256M [00:04<00:04, 31.7MB/s]#015Downloading:  45%|████▍     | 114M/256M [00:04<00:04, 33.1MB/s]#015Downloading:  46%|████▌     | 118M/256M [00:04<00:04, 31.4MB/s]#015Downloading:  47%|████▋     | 121M/256M [00:04<00:06, 23.4MB/s]#015Downloading:  49%|████▉     | 125M/256M [00:04<00:05, 26.3MB/s]#015Downloading:  50%|█████     | 128M/256M [00:05<00:04, 28.3MB/s]#015Downloading:  51%|█████▏    | 131M/256M [00:05<00:05, 22.7MB/s]#015Downloading:  52%|█████▏    | 134M/256M [00:05<00:06, 21.0MB/s]#015Downloading:  53%|█████▎    | 136M/256M [00:05<00:06, 19.8MB/s]#015Downloading:  54%|█████▍    | 138M/256M [00:05<00:06, 19.4MB/s]#015Downloading:  55%|█████▌    | 141M/256M [00:05<00:05, 21.1MB/s]#015Downloading:  56%|█████▌    | 143M/256M [00:05<00:05, 22.6MB/s]#015Downloading:  57%|█████▋    | 146M/256M [00:06<00:05, 20.9MB/s]#015Downloading:  58%|█████▊    | 148M/256M [00:06<00:05, 20.8MB/s]#015Downloading:  59%|█████▊    | 150M/256M [00:06<00:05, 19.0MB/s]#015Downloading:  60%|█████▉    | 152M/256M [00:06<00:05, 21.0MB/s]#015Downloading:  61%|██████    | 155M/256M [00:06<00:04, 22.1MB/s]#015Downloading:  62%|██████▏   | 158M/256M [00:06<00:04, 22.1MB/s]#015Downloading:  63%|██████▎   | 160M/256M [00:06<00:04, 22.6MB/s]#015Downloading:  64%|██████▎   | 162M/256M [00:06<00:04, 23.6MB/s]#015Downloading:  65%|██████▍   | 165M/256M [00:06<00:03, 24.2MB/s]#015Downloading:  65%|██████▌   | 167M/256M [00:06<00:03, 24.0MB/s]#015Downloading:  67%|██████▋   | 170M/256M [00:07<00:03, 24.1MB/s]#015Downloading:  68%|██████▊   | 173M/256M [00:07<00:03, 25.4MB/s]#015Downloading:  69%|██████▊   | 175M/256M [00:07<00:03, 25.3MB/s]#015Downloading:  70%|██████▉   | 178M/256M [00:07<00:02, 27.2MB/s]#015Downloading:  71%|███████   | 181M/256M [00:07<00:03, 25.3MB/s]#015Downloading:  72%|███████▏  | 184M/256M [00:07<00:02, 26.2MB/s]#015Downloading:  73%|███████▎  | 186M/256M [00:07<00:03, 21.9MB/s]#015Downloading:  74%|███████▎  | 188M/256M [00:07<00:03, 19.6MB/s]#015Downloading:  75%|███████▍  | 190M/256M [00:08<00:03, 19.3MB/s]#015Downloading:  75%|███████▌  | 192M/256M [00:08<00:03, 19.4MB/s]#015Downloading:  76%|███████▌  | 194M/256M [00:08<00:03, 16.9MB/s]#015Downloading:  77%|███████▋  | 196M/256M [00:08<00:03, 17.6MB/s]#015Downloading:  77%|███████▋  | 198M/256M [00:08<00:05, 11.3MB/s]#015Downloading:  78%|███████▊  | 200M/256M [00:08<00:04, 12.8MB/s]#015Downloading:  80%|███████▉  | 204M/256M [00:08<00:03, 15.8MB/s]#015Downloading:  81%|████████  | 206M/256M [00:09<00:02, 17.8MB/s]#015Downloading:  81%|████████▏ | 208M/256M [00:09<00:03, 15.7MB/s]#015Downloading:  82%|████████▏ | 210M/256M [00:09<00:02, 16.4MB/s]#015Downloading:  83%|████████▎ | 212M/256M [00:09<00:02, 17.3MB/s]#015Downloading:  84%|████████▍ | 215M/256M [00:09<00:02, 20.0MB/s]#015Downloading:  85%|████████▌ | 217M/256M [00:09<00:02, 19.8MB/s]#015Downloading:  86%|████████▌ | 220M/256M [00:09<00:01, 22.3MB/s]#015Downloading:  87%|████████▋ | 223M/256M [00:09<00:01, 24.1MB/s]#015Downloading:  88%|████████▊ | 226M/256M [00:09<00:01, 25.7MB/s]#015Downloading:  89%|████████▉ | 229M/256M [00:10<00:01, 25.1MB/s]#015Downloading:  90%|█████████ | 231M/256M [00:10<00:01, 20.8MB/s]#015Downloading:  91%|█████████▏| 233M/256M [00:10<00:01, 19.0MB/s]#015Downloading:  92%|█████████▏| 235M/256M [00:10<00:01, 18.1MB/s]#015Downloading:  93%|█████████▎| 237M/256M [00:10<00:01, 17.9MB/s]#015Downloading:  94%|█████████▎| 239M/256M [00:10<00:00, 18.6MB/s]#015Downloading:  95%|█████████▍| 242M/256M [00:10<00:00, 20.2MB/s]#015Downloading:  95%|█████████▌| 244M/256M [00:10<00:00, 20.5MB/s]#015Downloading:  96%|█████████▋| 246M/256M [00:11<00:00, 21.0MB/s]#015Downloading:  98%|█████████▊| 249M/256M [00:11<00:00, 23.4MB/s]#015Downloading:  99%|█████████▉| 253M/256M [00:11<00:00, 26.4MB/s]#015Downloading: 100%|█████████▉| 256M/256M [00:11<00:00, 26.8MB/s]#015Downloading: 100%|██████████| 256M/256M [00:11<00:00, 23.6MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 02:42:14,966 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-11 02:43:05 Completed - Training job completed\n",
      "Training seconds: 431\n",
      "Billable seconds: 431\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5fe04-b2e1-47a0-a15d-803b94ca102f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
