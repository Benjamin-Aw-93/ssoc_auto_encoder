{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ee8889-feba-4ac8-95b8-1cfaeed27c5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Developing code for Data Processing\n",
    "\n",
    "Last updated: 10th September 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f321eb-14af-4928-b52d-5161d0ed076f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c456c-eb9f-4127-9a42-5aa964574b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e30936-250c-4490-b9a6-0fb077eea649",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf_df = pd.read_csv(\"..\\Data\\Processed\\WGS_Dataset_JobInfo_precleaned.csv\")\n",
    "mcf_df = mcf_df[[\"Job_ID\", \"Title\", \"Description\", \"SSOC_2015\"]].sample(frac=0.1, random_state=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57506707-67cc-429f-a417-dda3ac25ab69",
   "metadata": {},
   "source": [
    "### Writing our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1be86a-0e3f-4405-bfff-a32a5151f25f",
   "metadata": {},
   "source": [
    "To Ben: Can you help me check through the code? If it's not clear or potentially buggy, please flag it to me so I can fix it. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f119e7-3265-46a5-91d5-0e32d01a2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefixes):\n",
    "    \"\"\"\n",
    "    Checks if the first text begins with a certain prefix\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Text to check for\n",
    "        prefixes [str]: List of prefixes to check from\n",
    "        \n",
    "    Returns:\n",
    "        Truncated text, or text if no prefix available\n",
    "    \"\"\"\n",
    "    for prefix in prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "    return text\n",
    "\n",
    "def check_if_first_word_is_verb(string):\n",
    "    \"\"\"\n",
    "    Checks if the first word of the string is a verb\n",
    "    \n",
    "    Parameters:\n",
    "        string (str): Text to check for\n",
    "        \n",
    "    Returns:\n",
    "        Whether the first word is a verb or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define some words that should be False\n",
    "    # regardless of what Spacy says\n",
    "    override_false_list = ['proven', 'possess']\n",
    "    \n",
    "    # Define some words that should be True\n",
    "    # regardless of what Spacy says\n",
    "    override_true_list = ['review' ,'responsible', 'design', 'to', 'able']\n",
    "    \n",
    "    # If the string is a zero length, return False\n",
    "    if len(string) == 0:\n",
    "        return False\n",
    "    \n",
    "    # If the first word is in the override false list, return False\n",
    "    if string.split(' ')[0].lower() in override_false_list:\n",
    "        return False\n",
    "    \n",
    "    # If the first word is in the override True list, return True\n",
    "    if string.split(' ')[0].lower() in override_true_list:\n",
    "        return True\n",
    "    \n",
    "    # If the first two words are \"you are\", we truncate it\n",
    "    string = remove_prefix(string.lower(), ['you are'])\n",
    "        \n",
    "        \n",
    "    # Check if the first word is a verb\n",
    "    return nlp(string)[0].pos_ == 'VERB'\n",
    "\n",
    "def clean_raw_string(string):\n",
    "    \"\"\"\n",
    "    Cleans the raw text from problematic strings or abbreviations\n",
    "    \n",
    "    Parameters:\n",
    "        string (str): Text to clean for\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text without problematic strings or abbreviations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify some common problematic strings to remove\n",
    "    to_remove = ['\\n', '\\xa0', '&nbsp;', '&amp;', '\\t', '&rsquo;']\n",
    "    \n",
    "    # Remove these strings\n",
    "    for item in to_remove:\n",
    "        string = string.replace(item, '')\n",
    "        \n",
    "    # Identify some common abbreviations to replace\n",
    "    to_replace = [('No.', 'Number')]\n",
    "    \n",
    "    # Replace these strings\n",
    "    for item1, item2 in to_replace:\n",
    "        string = string.replace(item1, item2)\n",
    "        \n",
    "    # Remove all non-unicode characters\n",
    "    # Deprecated due to reliance on bullet points\n",
    "    # string = ''.join([i if ord(i) < 128 else ' ' for i in string])\n",
    "    \n",
    "    return string\n",
    "\n",
    "def clean_html_unicode(string):\n",
    "    \"\"\"\n",
    "    Cleans the raw text from html codes\n",
    "    \n",
    "    Parameters:\n",
    "        string (str): Text to clean for\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text without problematic strings or abbreviations\n",
    "    \"\"\"\n",
    "    # Initialise the output string\n",
    "    cleaned_string = string\n",
    "    \n",
    "    # This is run in order, so be careful!\n",
    "    # <.*?>: removes html tages\n",
    "    # ^\\d+\\.{0,1} removes any bullet points for digits \n",
    "    # [^\\w\\s] removes any other symbols\n",
    "    cleaning_regex= ['<.*?>', '^\\d+\\.{0,1}', '[^\\w\\s]']\n",
    "    \n",
    "    # Iteratively apply each regex\n",
    "    for regex in cleaning_regex:\n",
    "        cleaned_string = re.sub(regex, '', cleaned_string).strip()\n",
    "    \n",
    "    return cleaned_string\n",
    "\n",
    "def check_list_for_verbs(list_elements):\n",
    "    \"\"\"\n",
    "    Check list for verbs after extracting text text based on each method below\n",
    "    \n",
    "    Parameters:\n",
    "        list_elements [str]: list of text produced by each function\n",
    "    \n",
    "    Returns:\n",
    "        list of text of maximum verb score, else empty list\n",
    "    \"\"\"\n",
    "    # Initialise a list to store the output\n",
    "    verb_scores = []\n",
    "    \n",
    "    # Iterate through each of the list elements passed in\n",
    "    for list_element in list_elements:\n",
    "        \n",
    "        # Use regex to split up the list into items\n",
    "        # Note this depends on whether the list elements\n",
    "        # passed in are lists (ol/ul) or paragraph lists (p)\n",
    "        if list_element[0:4] in ['<ul>', '<ol>']:\n",
    "            list_items_pattern = re.compile(r'(?=<li>).*?(?<=</li>)')\n",
    "        else:\n",
    "            list_items_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "\n",
    "        # Split each list up into the constituent items\n",
    "        list_items = list_items_pattern.findall(list_element)\n",
    "        \n",
    "        # Initialise a count of number of items beginning with a verb\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate through each item in the list\n",
    "        for list_item in list_items:\n",
    "            \n",
    "            # Remove all the HTML tags and check if the first word is a verb\n",
    "            list_item = clean_html_unicode(list_item)\n",
    "            #list_item = re.sub(\"[^\\w\\s]\", \"\", re.sub('^\\d+\\.{0,1}', '', re.sub('<.*?>', '', list_item.replace('\\t', '')).strip())).strip()\n",
    "            \n",
    "            # Check if the first word is a verb, and add to score if it is\n",
    "            if check_if_first_word_is_verb(list_item):\n",
    "                count += 1\n",
    "        \n",
    "        # Add the list length and verb score to the output\n",
    "        verb_scores.append((len(list_items), count/len(list_items)))\n",
    "        \n",
    "    # Initialise the list to store the new set of \n",
    "    # list elements which we are merging if they\n",
    "    # are short lists with lots of verbs\n",
    "    for_recursive = []\n",
    "\n",
    "    # Iterating over each verb score\n",
    "    for i, verb_score in enumerate(verb_scores):\n",
    "        \n",
    "        # Always append the first list item\n",
    "        if i == 0:\n",
    "            for_recursive.append(list_elements[i])\n",
    "        \n",
    "        # For other items, check if there are less than 6 items\n",
    "        # and the verb score is at least 70%. If so, we merge it\n",
    "        elif (verb_score[0] < 6) and (verb_score[1] >= .7):\n",
    "                        \n",
    "            # Remove the starting list tag if it is included\n",
    "            list_elements_i_cleaned = re.sub(r'(<ul>|<ol>|</ul>|</ol>)', '', list_elements[i])\n",
    "\n",
    "            # If the preceding list has a </ul> or </ol> tag\n",
    "            # then we should remove it before concatenating the\n",
    "            # strings, but otherwise we just concat the strings directly\n",
    "            if for_recursive[-1][:-5] in ['</ul>', '</ol>']:\n",
    "                for_recursive[-1] = for_recursive[-1][:-5] + \" \" + list_elements_i_cleaned + for_recursive[-1][-5:]\n",
    "            else:\n",
    "                for_recursive[-1] += list_elements[i]\n",
    "            \n",
    "        # Otherwise we just append it back to the list\n",
    "        else:\n",
    "            for_recursive.append(list_elements[i])\n",
    "            \n",
    "    # Run the recursive function if we have merged some lists together\n",
    "    if len(for_recursive) != len(list_elements):\n",
    "        return check_list_for_verbs(for_recursive)\n",
    "        \n",
    "    # Otherwise, we output the verb scores\n",
    "    else:\n",
    "\n",
    "        # Append the verb score to the list\n",
    "        # with a exception for very short lists\n",
    "        final_verb_scores = []\n",
    "        for verb_score in verb_scores:\n",
    "            \n",
    "            # If the length is less than 3\n",
    "            if verb_score[0] < 3:\n",
    "                final_verb_scores.append(min(verb_score[1], 0.5)) # cap the score at 50%\n",
    "            else:\n",
    "                final_verb_scores.append(verb_score[1])\n",
    "\n",
    "        # Return the list with maximum verb score, assuming at least\n",
    "        # 50% of the list contains verbs\n",
    "        if max(final_verb_scores) >= 0.5:\n",
    "            return list_elements[final_verb_scores.index(max(final_verb_scores))]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "def process_li_tag(text):\n",
    "    \"\"\"\n",
    "    Process job descriptions using li tags\n",
    "    \n",
    "    Parameters:\n",
    "        text: Job descriptions text\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted text, post-processed by check_list_for_verbs(list_elements)\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all lists in the HTML with a list tag (<ol> or <ul>)\n",
    "    # Regex explanation: \n",
    "    # (?=<ol>|<ul>) is the lookahead for the <ol> or <ul> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</ol>|</ul>) is the lookbehind for the </ol> or </ul> tag\n",
    "    list_pattern = re.compile(r'(?=<ol>|<ul>).*?(?<=</ol>|</ul>)')\n",
    "    list_elements = list_pattern.findall(text)\n",
    "    \n",
    "    if len(list_elements) == 0:\n",
    "        return []\n",
    "    \n",
    "    return check_list_for_verbs(list_elements)\n",
    "    \n",
    "def process_p_list(text):\n",
    "    \"\"\"\n",
    "    Process job descriptions using p tags. Extracting out text preceeded by literal bulletpoints or numeric points. \n",
    "    \n",
    "    Parameters:\n",
    "        text: Job descriptions text\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted text, post-processed by check_list_for_verbs(list_elements)\n",
    "    \"\"\"    \n",
    "    # Extract all lists in the HTML with a paragraph tag (<p>)\n",
    "    # Regex explanation: \n",
    "    # (?=<p>) is the lookahead for the <p> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</p>) is the lookbehind for the </p> tag\n",
    "    para_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "    para_elements = para_pattern.findall(text)\n",
    "            \n",
    "    # Check for specific unicode characters that can be used as bullet points\n",
    "    unicode_to_check = ['\\u2022', '\\u002d', '\\u00b7']\n",
    "    bullet_pt_presence = []\n",
    "    for para_element in para_elements:\n",
    "        \n",
    "        # Remove all the HTML tags\n",
    "        para_element_cleaned = re.sub('<.*?>', '', para_element).strip()\n",
    "        \n",
    "        # Check if the string is non-empty\n",
    "        if len(para_element_cleaned) > 0:\n",
    "            \n",
    "            # Check if the first character has any bullet points\n",
    "            result1 = para_element_cleaned[0] in unicode_to_check\n",
    "            \n",
    "            # Check if the first character is a numbered list\n",
    "            # by checking if the re.match() returns anything\n",
    "            result2 = re.match(r'^\\d+\\.', para_element_cleaned) is not None\n",
    "            \n",
    "            bullet_pt_presence.append(result1 or result2)\n",
    "            \n",
    "        # If it is empty, then it doesn't contain any bullet points\n",
    "        else:\n",
    "            bullet_pt_presence.append(False)\n",
    "    \n",
    "    # Initialise the lists\n",
    "    output = []\n",
    "    p_list = []\n",
    "    \n",
    "    # Build an equivalent list of list items by iterating\n",
    "    # through the boolean list indicating if there is\n",
    "    # a bullet point character at the start of the string\n",
    "    for i, value in enumerate(bullet_pt_presence):\n",
    "        \n",
    "        # If there is a bullet point character\n",
    "        if value:\n",
    "            \n",
    "            # Append the string to the para list\n",
    "            p_list.append(para_elements[i])\n",
    "            \n",
    "        # If there is no bullet point character\n",
    "        else:\n",
    "            \n",
    "            # Append the para list if it is non-empty\n",
    "            if len(p_list) > 0:\n",
    "                output.append(' '.join(p_list))\n",
    "            \n",
    "            # Reset the para list\n",
    "            p_list = []\n",
    "            \n",
    "    if len(output) == 0:\n",
    "        return []\n",
    "    \n",
    "    return check_list_for_verbs(output)\n",
    "\n",
    "def process_p_tag(text):\n",
    "    \"\"\"\n",
    "    Process job descriptions using p tags. Extracting out text preceeded a verb \n",
    "    \n",
    "    Parameters:\n",
    "        text: Job descriptions text\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted text, post-processed by check_list_for_verbs(list_elements)\n",
    "    \"\"\"    \n",
    "    # Extract all lists in the HTML with a paragraph tag (<p>)\n",
    "    # Regex explanation: \n",
    "    # (?=<p>) is the lookahead for the <p> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</p>) is the lookbehind for the </p> tag\n",
    "    para_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "    para_elements = para_pattern.findall(text)\n",
    "    \n",
    "    if len(para_elements) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Iterate through each paragraph element to see which one starts\n",
    "    # with a verb, and we keep that paragraph element\n",
    "    output = []\n",
    "    for para_element in para_elements:\n",
    "\n",
    "        # Remove all the HTML tags and check if the first word is a verb\n",
    "        para_element_cleaned = re.sub(\"[^\\w\\s]\", \"\", re.sub('<.*?>', '', para_element)).strip()\n",
    "        if len(para_element_cleaned) > 0:\n",
    "            if check_if_first_word_is_verb(para_element_cleaned):\n",
    "                output.append(para_element)\n",
    "\n",
    "    return \" \".join(output)\n",
    "            \n",
    "def process_text(raw_text):\n",
    "    \"\"\"\n",
    "    Process job description, put text through each process function, and return results according to precedence. \n",
    "    \n",
    "    Parameters:\n",
    "        text: Job descriptions text\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text\n",
    "    \"\"\"    \n",
    "    # Remove problematic characters\n",
    "    text = clean_raw_string(raw_text)\n",
    "    \n",
    "    li_results = process_li_tag(text)\n",
    "    p_list_results = process_p_list(raw_text)\n",
    "    p_results = process_p_tag(text)\n",
    "    \n",
    "    if len(li_results) > 0:\n",
    "        print('List object detected')\n",
    "        return li_results\n",
    "    elif len(p_list_results) > 0:\n",
    "        print('Paragraph list detected')\n",
    "        return p_list_results\n",
    "    elif len(p_results) > 0:\n",
    "        print('Paragraphs detected')\n",
    "        return p_results\n",
    "    else:\n",
    "        print('None detected, returning all')\n",
    "        return re.sub('<.*?>', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79266a39-f636-42cc-931c-fc864036a4e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Checking the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d8485-925b-4ae1-96b8-aeea96ae255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_text(mcf_df[\"Description\"][835]))\n",
    "print('----------------------------------------')\n",
    "print(mcf_df[\"Description\"][835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f7f6a-66e8-4488-ae1f-ff72514140f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider edge cases\n",
    "# 23021 - starts with manual numbering, need to remove numbers before processing # ok addressed\n",
    "# 18218 - multiple lists (ul list) # ok addressed\n",
    "# 817 - multiple lists (p list) # ok addressed\n",
    "# 12802 - 'possess'\n",
    "# 1657 - combined list\n",
    "# 11007 - 'process', 'monitor', 'update' not recognised as verbs\n",
    "# 5361 - should have picked the first list but didnt\n",
    "# 9458 - should have picked up both lists but didn't\n",
    "# 13850, 21925 - didn't pick the first list, para should have a minimum element\n",
    "# 1566 - should have picked list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7169821-cd14-46c6-8575-1490805ccb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.sample(mcf_df.index.tolist(), 1)[0]\n",
    "print(f\"Index: {idx}\")\n",
    "print(f\"Job title: {mcf_df['Title'][idx]}\")\n",
    "print(f\"SSOC: {mcf_df['SSOC_2015'][idx]}\")\n",
    "print(\"-----------------------\")\n",
    "print(f\"Job tasks: {process_text(mcf_df['Description'][idx])}\")\n",
    "print(\"==============================================\")\n",
    "print(mcf_df['Description'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71559ee-b9c3-4cf1-8382-9dbe4f2a49a2",
   "metadata": {},
   "source": [
    "### Archived Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fd6bc-dbf9-489d-810d-d4ee60d8f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mcf_df[\"Description\"].apply(process_text)\n",
    "print(f\"Coverage: {len(output[output.apply(len) != 0]) / len(output)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b7d44-7ab1-4dc5-93bb-a112756b88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mcf_df[\"Description\"][23545].replace('\\n', '')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2ebf0-e77b-4896-8326-a3fcc354bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?=<ol>|<ul>).*?(?<=<\\/ol>|<\\/ul>)')\n",
    "matches = pattern.findall(text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e37afc-4d36-4246-ac45-d2033648e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_first_word_is_verb(string):\n",
    "    override_false_list = ['proven', 'possess']\n",
    "    override_true_list = ['review' ,'responsible', 'design', 'to', 'able']\n",
    "    if len(string) == 0:\n",
    "        return False\n",
    "    if string.split(' ')[0].lower() in override_false_list:\n",
    "        return False\n",
    "    if string.split(' ')[0].lower() in override_true_list:\n",
    "        return True\n",
    "    if string.lower()[0:8] == 'you are':\n",
    "        string = string.replace(string[0:8], '')\n",
    "    return nlp(string)[0].pos_ == 'VERB'\n",
    "\n",
    "#178\n",
    "#308\n",
    "text = mcf_df[\"Description\"][318]\n",
    "\n",
    "def clean_raw_string(string):\n",
    "    \n",
    "    # Identify some common problematic strings to remove\n",
    "    to_remove = ['\\n', '\\xa0', '&nbsp;', '&amp;', '\\t', '&rsquo;']\n",
    "    for item in to_remove:\n",
    "        string = string.replace(item, '')\n",
    "        \n",
    "    to_replace = [('No.', 'Number')]\n",
    "    for item1, item2 in to_replace:\n",
    "        string = string.replace(item1, item2)\n",
    "        \n",
    "    # Remove all non-unicode characters\n",
    "    #string = ''.join([i if ord(i) < 128 else ' ' for i in string])\n",
    "    \n",
    "    return string\n",
    "\n",
    "def check_list_for_verbs(list_elements):\n",
    "    print(list_elements)\n",
    "    verb_scores = []\n",
    "    for list_element in list_elements:\n",
    "        \n",
    "        # Use regex to split up the list into items\n",
    "\n",
    "        if list_element[0:4] in ['<ul>', '<ol>']:\n",
    "            list_items_pattern = re.compile(r'(?=<li>).*?(?<=</li>)')\n",
    "        else:\n",
    "            list_items_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "\n",
    "        list_items = list_items_pattern.findall(list_element)\n",
    "        \n",
    "        # Initialise a count of number of items beginning with a verb\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate through each item in the list\n",
    "        for list_item in list_items:\n",
    "            \n",
    "            # Remove all the HTML tags and check if the first word is a verb\n",
    "            list_item = re.sub(\"[^\\w\\s]\", \"\", re.sub('^\\d+\\.{0,1}', '', re.sub('<.*?>', '', list_item.replace('\\t', '')).strip())).strip()\n",
    "            if check_if_first_word_is_verb(list_item):\n",
    "                count += 1\n",
    "        \n",
    "        verb_scores.append((len(list_items), count/len(list_items)))\n",
    "        \n",
    "    for_recursive = []\n",
    "    print(verb_scores)\n",
    "    for i, verb_score in enumerate(verb_scores):\n",
    "        \n",
    "        # Always append the first list item\n",
    "        if i == 0:\n",
    "            for_recursive.append(list_elements[i])\n",
    "        \n",
    "        # For other items, check if there are less than 6 items\n",
    "        # and the verb score is 100%. If so, we merge it\n",
    "        elif (verb_score[0] < 6) and (verb_score[1] >= .6):\n",
    "                        \n",
    "            # Remove the starting list tag if it is included\n",
    "            list_elements_i_cleaned = re.sub(r'(<ul>|<ol>|</ul>|</ol>)', '', list_elements[i])\n",
    "            print(list_elements_i_cleaned)\n",
    "            # If the preceding list has a </ul> or </ol> tag\n",
    "            # then we should remove it before concatenating the\n",
    "            # strings, but otherwise we just concat the strings directly\n",
    "            \n",
    "            if for_recursive[-1][:-5] in ['</ul>', '</ol>']:\n",
    "                for_recursive[-1] = for_recursive[-1][:-5] + \" \" + list_elements_i_cleaned + for_recursive[-1][-5:]\n",
    "            else:\n",
    "                for_recursive[-1] += list_elements[i]\n",
    "            \n",
    "        # Otherwise we just append it back to the list\n",
    "        else:\n",
    "            for_recursive.append(list_elements[i])\n",
    "            \n",
    "    # Run the recursive function if we have merged some lists together\n",
    "    if len(for_recursive) != len(list_elements):\n",
    "        return check_list_for_verbs(for_recursive)\n",
    "        \n",
    "    # Otherwise, we output the verb scores\n",
    "    else:\n",
    "\n",
    "        # Append the verb score to the list\n",
    "        # with a exception for very short lists\n",
    "        final_verb_scores = []\n",
    "        for verb_score in verb_scores:\n",
    "            \n",
    "            if verb_score[0] < 3:\n",
    "                final_verb_scores.append(min(verb_score[1], 0.5))\n",
    "            else:\n",
    "                final_verb_scores.append(verb_score[1])\n",
    "\n",
    "        # Return the list with maximum verb score, assuming at least\n",
    "        # 50% of the list contains verbs (to avoid situations of only\n",
    "        # a single list)\n",
    "        if max(final_verb_scores) > 0.5:\n",
    "            return list_elements[final_verb_scores.index(max(final_verb_scores))]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "def process_li_tag(text):\n",
    "\n",
    "    # Extract all lists in the HTML with a list tag (<ol> or <ul>)\n",
    "    # Regex explanation: \n",
    "    # (?=<ol>|<ul>) is the lookahead for the <ol> or <ul> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</ol>|</ul>) is the lookbehind for the </ol> or </ul> tag\n",
    "    list_pattern = re.compile(r'(?=<ol>|<ul>).*?(?<=</ol>|</ul>)')\n",
    "    list_elements = list_pattern.findall(text)\n",
    "    \n",
    "    if len(list_elements) == 0:\n",
    "        return []\n",
    "    \n",
    "    return check_list_for_verbs(list_elements)\n",
    "    \n",
    "#     # Iterate through each list item to see which list has more\n",
    "#     # items with a verb as the first word\n",
    "#     verb_scores = []\n",
    "#     for list_element in list_elements:\n",
    "        \n",
    "#         # Use regex to split up the list into items\n",
    "#         list_items_pattern = re.compile(r'(?=<li>).*?(?<=</li>)')\n",
    "#         list_items = list_items_pattern.findall(list_element)\n",
    "        \n",
    "#         # Initialise a count of number of items beginning with a verb\n",
    "#         count = 0\n",
    "        \n",
    "#         # Iterate through each item in the list\n",
    "#         for list_item in list_items:\n",
    "            \n",
    "#             # Remove all the HTML tags and check if the first word is a verb\n",
    "#             list_item = re.sub('<.*?>', '', list_item)\n",
    "#             if check_if_first_word_is_verb(list_item):\n",
    "#                 count += 1\n",
    "        \n",
    "#         # Append the verb score to the list\n",
    "#         # with a exception for very short lists\n",
    "#         if len(list_items) < 3:\n",
    "#             verb_scores.append(min(count/len(list_items), 0.5))\n",
    "#         else:\n",
    "#             verb_scores.append(count/len(list_items))\n",
    "    \n",
    "def process_p_list(text):\n",
    "    \n",
    "    # Extract all lists in the HTML with a paragraph tag (<p>)\n",
    "    # Regex explanation: \n",
    "    # (?=<p>) is the lookahead for the <p> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</p>) is the lookbehind for the </p> tag\n",
    "    para_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "    para_elements = para_pattern.findall(text)\n",
    "            \n",
    "    # Check for specific unicode characters that can be used as bullet points\n",
    "    unicode_to_check = ['\\u2022', '\\u002d', '\\u00b7']\n",
    "    bullet_pt_presence = []\n",
    "    for para_element in para_elements:\n",
    "        \n",
    "        # Remove all the HTML tags\n",
    "        para_element_cleaned = re.sub('<.*?>', '', para_element).strip()\n",
    "        \n",
    "        # Check if the string is non-empty\n",
    "        if len(para_element_cleaned) > 0:\n",
    "            \n",
    "            # Check if the first character has any bullet points\n",
    "            result1 = para_element_cleaned[0] in unicode_to_check\n",
    "            \n",
    "            # Check if the first character is a numbered list\n",
    "            # by checking if the re.match() returns anything\n",
    "            result2 = re.match(r'^\\d+\\.', para_element_cleaned) is not None\n",
    "            \n",
    "            bullet_pt_presence.append(result1 or result2)\n",
    "            \n",
    "        # If it is empty, then it doesn't contain any bullet points\n",
    "        else:\n",
    "            bullet_pt_presence.append(False)\n",
    "    \n",
    "    # Initialise the lists\n",
    "    output = []\n",
    "    p_list = []\n",
    "    \n",
    "    # Build an equivalent list of list items by iterating\n",
    "    # through the boolean list indicating if there is\n",
    "    # a bullet point character at the start of the string\n",
    "    for i, value in enumerate(bullet_pt_presence):\n",
    "        \n",
    "        # If there is a bullet point character\n",
    "        if value:\n",
    "            \n",
    "            # Append the string to the para list\n",
    "            p_list.append(para_elements[i])\n",
    "            \n",
    "        # If there is no bullet point character\n",
    "        else:\n",
    "            \n",
    "            # Append the para list if it is non-empty\n",
    "            if len(p_list) > 0:\n",
    "                output.append(' '.join(p_list))\n",
    "            \n",
    "            # Reset the para list\n",
    "            p_list = []\n",
    "            \n",
    "    if len(output) == 0:\n",
    "        return []\n",
    "    \n",
    "    return check_list_for_verbs(output)\n",
    "\n",
    "#     # Iterate through each list item to see which list has more\n",
    "#     # items with a verb as the first word\n",
    "#     verb_scores = []\n",
    "#     for list_element in output:\n",
    "        \n",
    "#         # Use regex to split up the list into items\n",
    "#         list_items_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "#         list_items = list_items_pattern.findall(list_element)\n",
    "        \n",
    "#         # Initialise a count of number of items beginning with a verb\n",
    "#         count = 0\n",
    "        \n",
    "#         # Iterate through each item in the list\n",
    "#         for list_item in list_items:\n",
    "            \n",
    "#             # Remove all the HTML tags and check if the first word is a verb\n",
    "#             # !!! this needs to be cleaned up, very messy\n",
    "#             # consider changing \\d to \\w to include a b c lists\n",
    "#             list_item = re.sub(\"[^\\w\\s]\", \"\", re.sub('^\\d+\\.{0,1}', '', re.sub('<.*?>', '', list_item.replace('\\t', '')).strip())).strip()\n",
    "#             if check_if_first_word_is_verb(list_item):\n",
    "#                 count += 1\n",
    "        \n",
    "#         # Append the verb score to the list\n",
    "#         # with a exception for very short lists\n",
    "#         if len(list_items) < 3:\n",
    "#             verb_scores.append(min(count/len(list_items), 0.5))\n",
    "#         else:\n",
    "#             verb_scores.append(count/len(list_items))\n",
    "#     print(verb_scores)\n",
    "    \n",
    "#     # Return the list with maximum verb score, assuming at least\n",
    "#     # 50% of the list contains verbs (to avoid situations of only\n",
    "#     # a single list)\n",
    "#     if max(verb_scores) > 0.5:\n",
    "#         return output[verb_scores.index(max(verb_scores))]\n",
    "#     else:\n",
    "#         return []\n",
    "\n",
    "def process_p_tag(text):\n",
    "    \n",
    "    # Extract all lists in the HTML with a paragraph tag (<p>)\n",
    "    # Regex explanation: \n",
    "    # (?=<p>) is the lookahead for the <p> tag\n",
    "    # .* captures everything between the tags, ? restricts it to capturing one set only\n",
    "    # (?<=</p>) is the lookbehind for the </p> tag\n",
    "    para_pattern = re.compile(r'(?=<p>).*?(?<=</p>)')\n",
    "    para_elements = para_pattern.findall(text)\n",
    "    \n",
    "    if len(para_elements) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Iterate through each paragraph element to see which one starts\n",
    "    # with a verb, and we keep that paragraph element\n",
    "    output = []\n",
    "    for para_element in para_elements:\n",
    "\n",
    "        # Remove all the HTML tags and check if the first word is a verb\n",
    "        para_element_cleaned = re.sub(\"[^\\w\\s]\", \"\", re.sub('<.*?>', '', para_element)).strip()\n",
    "        if len(para_element_cleaned) > 0:\n",
    "            if check_if_first_word_is_verb(para_element_cleaned):\n",
    "                output.append(para_element)\n",
    "\n",
    "    return \" \".join(output)\n",
    "            \n",
    "def process_text(raw_text):\n",
    "    \n",
    "    # Remove problematic characters\n",
    "    text = clean_raw_string(raw_text)\n",
    "    \n",
    "    li_results = process_li_tag(text)\n",
    "    p_list_results = process_p_list(raw_text)\n",
    "    p_results = process_p_tag(text)\n",
    "    \n",
    "    if len(li_results) > 0:\n",
    "        print('List object detected')\n",
    "        return li_results\n",
    "    elif len(p_list_results) > 0:\n",
    "        print('Paragraph list detected')\n",
    "        return p_list_results\n",
    "    elif len(p_results) > 0:\n",
    "        print('Paragraphs detected')\n",
    "        return p_results\n",
    "    else:\n",
    "        print('None detected, returning all')\n",
    "        return re.sub('<.*?>', ' ', text)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Extract all the non-list items in the text\n",
    "    # Iteratively split the text by the match until the last match\n",
    "#     \n",
    "#     non_list_elements = []\n",
    "#     text_placeholder = text\n",
    "#     for list_element in list_elements:\n",
    "#         non_list_elements.append(re.sub('<.*?>', '', text_placeholder.split(list_element)[0]).strip())\n",
    "#         text_placeholder = text_placeholder.split(list_element)[1]\n",
    "#     if len(text_placeholder) != 0: # If last item is non-empty\n",
    "#         non_list_elements.append(text_placeholder)\n",
    "\n",
    "\n",
    "#def clean_p_tag():\n",
    "    \n",
    "        #list_pattern = re.compile(r'(?=<li>|<p>).*?(?:(?<=</li>)|(?<=</p>))')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27d024-7a8a-405e-b6af-9d47ce03d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_text(mcf_df[\"Description\"][18218]))\n",
    "print('----------------------------------------')\n",
    "print(mcf_df[\"Description\"][18218])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ffda8-5472-4877-9609-f2ca6af59d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_text(mcf_df[\"Description\"][817]))\n",
    "print('----------------------------------------')\n",
    "print(mcf_df[\"Description\"][817])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992b340-5139-461e-bcce-bb45aab64f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssoc = pd.read_csv('../Data/Raw/ssoc_v2018.csv', encoding='iso-8859-1')\n",
    "ssoc.dropna(inplace = True)\n",
    "ssoc['ssoc_f'] = ssoc['ssoc_f'].astype('float').astype('int').astype('str')\n",
    "mcf_df = mcf_df[(mcf_df['SSOC_2015'] != 'X5000') & (mcf_df['SSOC_2015'].notnull())]\n",
    "mcf_df['SSOC_2015'] = mcf_df['SSOC_2015'].astype('float').astype('int').astype('str')\n",
    "mcf_data_final = mcf_df.merge(ssoc, left_on = 'SSOC_2015', right_on = 'ssoc_f', how = 'left')\n",
    "mcf_data_final.rename({'ssoc_desc': \"Reported SSOC Desc\"}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccea4f7-ee2b-4eec-9802-c1f2feb09256",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_first_word_is_verb('Supporting delivery of curricular programmes to students as required by the school&rsquo;s academic board.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14554e4f-b100-4e0f-bb4b-19eb773ae26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider edge cases\n",
    "# 23021 - starts with manual numbering, need to remove numbers before processing # ok addressed\n",
    "# 18218 - multiple lists (ul list) # ok addressed\n",
    "# 817 - multiple lists (p list) # ok addressed\n",
    "# 12802 - 'possess'\n",
    "# 1657 - combined list\n",
    "# 11007 - 'process', 'monitor', 'update' not recognised as verbs\n",
    "# 5361 - should have picked the first list but didnt\n",
    "# 9458 - should have picked up both lists but didn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc7d35-895c-4aba-a4da-8b2e7b961820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.sample(mcf_df.index.tolist(), 1)[0]\n",
    "#idx = 23272\n",
    "#idx = 18514\n",
    "#idx = 835\n",
    "print(f\"Index: {idx}\")\n",
    "print(f\"Job title: {mcf_df['Title'][idx]}\")\n",
    "print(f\"SSOC: {mcf_df['SSOC_2015'][idx]}\")\n",
    "print(f\"Job tasks: {process_text(mcf_df['Description'][idx])}\")\n",
    "print(\"-----------------------\")\n",
    "print(\"==============================================\")\n",
    "print(mcf_df['Description'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbeb75-f128-4996-a111-5f2f2d87d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text(mcf_df[\"Description\"][18203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b0b4e-fb99-487c-9b40-f9867d4d24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf_df[\"Description\"][103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0340de-4933-4160-a24a-9e41cd2adf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp(output[0].split(' ')[0]):\n",
    "    print(token.pos_)\n",
    "    print(nlp(output[0].split(' ')[0])[0].pos_ == 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e54b80-89b6-489b-abf0-16aa55b0b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb72b7-d0fd-48a4-917a-b863c92594d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_list_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe5489-96cc-4d71-9d6d-828a22f25e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output.apply(len) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35165d-e489-4743-bd16-a0516d745684",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf_df[mcf_df[\"Description\"].str.contains('<ol>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b4f05-79e5-4e19-b8b3-998c11459188",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf_df[\"Description\"][76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08c467-3fdb-4300-87c3-395dfc2b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# replace <p> tags within <li> tags !\n",
    "# pick <p> tag with closest match instead of preceding <p> tag only? or preceding 2 <p> tags\n",
    "# if <p> tag is empty then hunt for another tag (maybe h1?)\n",
    "# max length of <p> tag?\n",
    "# need to capture all <li> tags instead of just those with <p> preceding\n",
    "# should we just go straight to labelling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7438d-5016-4923-bfc2-e003dcd51b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf_df.iloc[18]['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e639b4b-f91e-45d9-b82a-684a36495c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.sample(output[output.apply(len) != 0].index.tolist(), 1)[0]\n",
    "#idx = 23272\n",
    "#idx = 18514\n",
    "#idx = 835\n",
    "print(f\"Index: {idx}\")\n",
    "for i in output[idx][0]:\n",
    "    print(f\"Job title: {i['job_title']}\")\n",
    "    print(f\"Relevance score: {i['relevance_score']}\")\n",
    "    print(f\"Job description: {i['job_description']}\")\n",
    "    print(f\"Match: {i['match']}\")\n",
    "    print(\"-----------------------\")\n",
    "print(\"==============================================\")\n",
    "print(mcf_df.iloc[idx]['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e422cb-3e89-4d80-b55c-33711b4634c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[320]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviro",
   "language": "python",
   "name": "my_enviro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
